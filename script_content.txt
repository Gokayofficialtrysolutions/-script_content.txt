#!/bin/bash
# OPUS MAGNUM: ULTIMATE TERMINALIS AI ECOSYSTEM
# Size: ~280GB | Models: 25+ | Capabilities: UNLIMITED
set -e

# Determine the directory where the script is located
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"

INSTALL_DIR="$HOME/.terminus-ai"
LOG="$INSTALL_DIR/install.log"
TOTAL=18 # Adjusted for two progress calls in create_agent_orchestration_script
STEP=0

# Function to display progress
progress(){
    STEP=$((STEP+1))
    echo "[$STEP/$TOTAL-$((STEP*100/TOTAL))%] $1" | tee -a "$LOG"
}

# Function for initial directory creation and logging setup
initialize_setup() {
    progress "INITIALIZING SETUP"
    mkdir -p "$INSTALL_DIR"/{core,models,agents,tools,data,logs,cache}
    touch "$LOG"
    echo "TERMINUS AI: THE ULTIMATE LOCAL AI ECOSYSTEM" | tee -a "$LOG"
    echo "Total: ~280GB | Models: 25+ | Agents: Unlimited" | tee -a "$LOG"

    # Copy models.conf from script directory to INSTALL_DIR
    if [ -f "$SCRIPT_DIR/models.conf" ]; then
        cp "$SCRIPT_DIR/models.conf" "$INSTALL_DIR/models.conf"
        echo "Copied models.conf to $INSTALL_DIR" | tee -a "$LOG"
    else
        echo "WARNING: models.conf not found in script directory ($SCRIPT_DIR). Model configuration will rely on fallback or potentially fail if critical." | tee -a "$LOG"
        # The pull_ollama_models function has its own check for $INSTALL_DIR/models.conf and fallback.
    fi
}

# Function to install system dependencies
install_system_dependencies() {
    progress "INSTALLING SYSTEM DEPENDENCIES" # Corresponds to old "INITIALIZING QUANTUM CORE SYSTEMS" (second part)
    if command -v apt &>/dev/null; then
        sudo apt update && sudo apt install -y python3 python3-pip docker.io git curl wget build-essential cmake ninja-build nodejs npm golang rust-all-dev espeak libespeak1
    fi
    if command -v brew &>/dev/null; then
        brew install python docker git curl wget cmake ninja nodejs go rust espeak
    fi
    # Add more error checking here in later plan steps
}

# No longer defining MODELS array globally. It's handled in pull_ollama_models function.

# Function to install base Python packages like torch, transformers, etc.
install_python_core_libraries() {
    progress "INSTALLING PYTHON CORE LIBRARIES"
    python3 -m pip install --upgrade pip setuptools wheel
    if [ $? -ne 0 ]; then
        echo "WARNING: Failed to upgrade pip, setuptools, or wheel. Check $LOG for details." | tee -a "$LOG"
    fi
    echo "Installing Python core libraries from $SCRIPT_DIR/core_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/core_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/core_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
}

# Function for Langchain, Autogen, UI frameworks, etc.
install_python_framework_libraries() {
    progress "INSTALLING PYTHON FRAMEWORK LIBRARIES"
    echo "Installing Python framework libraries from $SCRIPT_DIR/frameworks_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/frameworks_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/frameworks_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
}

# Function for web scraping, data handling, file processing, etc.
install_python_utility_libraries() {
    progress "INSTALLING PYTHON UTILITY LIBRARIES"
    echo "Installing Python utility libraries from $SCRIPT_DIR/utils_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/utils_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/utils_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
}

# Function for installing Ollama
install_ollama_and_dependencies() {
    progress "INSTALLING OLLAMA AND DEPENDENCIES"
    curl -fsSL https://ollama.ai/install.sh | sh
    ollama serve &
    echo "Waiting for Ollama server to start..." | tee -a "$LOG"
    sleep 10 # Increased sleep time for robustness
    if ! ollama list > /dev/null 2>&1 && ! curl -sf --head http://localhost:11434 | grep "HTTP/[12]\.[01] [2].." > /dev/null; then
        echo "ERROR: Ollama server failed to start or is not responding. Aborting. Check $LOG for details." | tee -a "$LOG"
        exit 1
    else
        echo "Ollama server started successfully." | tee -a "$LOG"
    fi
}

# Function for downloading AI models
pull_ollama_models() {
    progress "SELECTING AND PULLING OLLAMA MODELS"

    ALL_AVAILABLE_MODELS=()
    CORE_MODELS=()
    CURRENT_SECTION=""
    CONFIG_FILE="$INSTALL_DIR/models.conf" # Assuming models.conf is in INSTALL_DIR

    if [ ! -f "$CONFIG_FILE" ]; then
        echo "ERROR: Configuration file '$CONFIG_FILE' not found." | tee -a "$LOG"
        echo "Please ensure 'models.conf' exists in $INSTALL_DIR." | tee -a "$LOG"
        echo "Proceeding with no models available for selection. You can only skip model download." | tee -a "$LOG"
        # Fallback: Define minimal core models if config is missing, to prevent errors later if user tries to select core
        CORE_MODELS=("llama3.1:8b" "mistral:7b") # Minimal fallback
    else
        echo "Reading model lists from $CONFIG_FILE..." | tee -a "$LOG"
        while IFS= read -r line || [ -n "$line" ]; do
            # Remove leading/trailing whitespace (optional, but good for robustness)
            line=$(echo "$line" | awk '{$1=$1};1')

            # Skip empty lines and comments
            [[ "$line" =~ ^\s*# ]] && continue
            [[ "$line" =~ ^\s*$ ]] && continue

            if [[ "$line" =~ ^\[(.*)\]$ ]]; then
                CURRENT_SECTION="${BASH_REMATCH[1]}"
            else
                # Remove potential carriage returns for cross-platform compatibility
                line=$(echo "$line" | tr -d '\r')
                if [ -n "$line" ]; then # Ensure line is not empty after stripping CR
                    case "$CURRENT_SECTION" in
                        ALL_AVAILABLE_MODELS)
                            ALL_AVAILABLE_MODELS+=("$line")
                            ;;
                        CORE_MODELS)
                            CORE_MODELS+=("$line")
                            ;;
                    esac
                fi
            fi
        done < "$CONFIG_FILE"
        echo "Finished reading model lists. Found ${#ALL_AVAILABLE_MODELS[@]} available models and ${#CORE_MODELS[@]} core models." | tee -a "$LOG"
    fi

    if [ ${#ALL_AVAILABLE_MODELS[@]} -eq 0 ] && [ -f "$CONFIG_FILE" ]; then
        echo "WARNING: No models were loaded from $CONFIG_FILE. It might be empty or incorrectly formatted." | tee -a "$LOG"
        echo "Model download options will be limited. You may only be able to skip." | tee -a "$LOG"
    elif [ ${#ALL_AVAILABLE_MODELS[@]} -eq 0 ] && [ ! -f "$CONFIG_FILE" ]; then
        # Error already printed, this is just to ensure the flow is logical
        echo "Continuing with no models defined due to missing models.conf." | tee -a "$LOG"
    fi

    # Ensure CORE_MODELS is not empty if user might select it, even if ALL_AVAILABLE_MODELS is empty.
    # This is a safety net, though the user prompt should guide them.
    if [ ${#CORE_MODELS[@]} -eq 0 ] && [ ${#ALL_AVAILABLE_MODELS[@]} -gt 0 ]; then
        echo "WARNING: CORE_MODELS list is empty in models.conf. Selecting 'CORE' will result in no models being downloaded unless ALL models are also empty." | tee -a "$LOG"
    elif [ ${#CORE_MODELS[@]} -eq 0 ] && [ ${#ALL_AVAILABLE_MODELS[@]} -eq 0 ]; then
         # If both are empty (e.g. models.conf missing and no fallback for CORE_MODELS or ALL_AVAILABLE_MODELS)
         CORE_MODELS=("llama3.1:8b" "mistral:7b") # Re-apply minimal fallback for safety if somehow cleared
         echo "Re-applying minimal fallback for CORE_MODELS as both lists were empty." | tee -a "$LOG"
    fi


    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "Ollama Model Installation Options:" | tee -a "$LOG"
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "Available models for installation:" | tee -a "$LOG"
    for i in "${!ALL_AVAILABLE_MODELS[@]}"; do
        printf "  %2d. %s\n" "$((i+1))" "${ALL_AVAILABLE_MODELS[$i]}" | tee -a "$LOG"
    done
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "You can choose to:" | tee -a "$LOG"
    echo "  1. Download ALL available models (${#ALL_AVAILABLE_MODELS[@]} models, ~180GB+)." | tee -a "$LOG"
    echo "  2. Download a CORE set of essential models (${#CORE_MODELS[@]} models, ~20-50GB)." | tee -a "$LOG"
    echo "  3. Select specific models to download." | tee -a "$LOG"
    echo "  4. Skip Ollama model downloads for now." | tee -a "$LOG"
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    read -r -p "Enter your choice (1, 2, 3, or 4): " user_choice

    MODELS_TO_PULL=()
    case "$user_choice" in
        1)
            echo "Preparing to download ALL ${#ALL_AVAILABLE_MODELS[@]} models." | tee -a "$LOG"
            MODELS_TO_PULL=("${ALL_AVAILABLE_MODELS[@]}")
            ;;
        2)
            echo "Preparing to download CORE set of ${#CORE_MODELS[@]} models." | tee -a "$LOG"
            MODELS_TO_PULL=("${CORE_MODELS[@]}")
            ;;
        3)
            echo "Enter the names of the models you wish to download, separated by spaces." | tee -a "$LOG"
            echo "Example: llama3.1:8b mistral:7b deepseek-coder-v2:16b" | tee -a "$LOG"
            echo "Available models listed above. Please type or copy-paste exact names." | tee -a "$LOG"
            read -r -p "Selected models: " selected_models_str
            # Convert the space-separated string to an array
            read -r -a USER_SELECTED_MODELS <<< "$selected_models_str"

            # Validate user selections
            for model_name in "${USER_SELECTED_MODELS[@]}"; do
                is_valid=false
                for available_model in "${ALL_AVAILABLE_MODELS[@]}"; do
                    if [[ "$model_name" == "$available_model" ]]; then
                        MODELS_TO_PULL+=("$model_name")
                        is_valid=true
                        break
                    fi
                done
                if ! $is_valid; then
                    echo "WARNING: Model '$model_name' is not in the list of available models and will be skipped." | tee -a "$LOG"
                fi
            done

            if [ ${#MODELS_TO_PULL[@]} -eq 0 ] && [ ${#USER_SELECTED_MODELS[@]} -ne 0 ]; then
                 echo "No valid models selected from your input. Defaulting to CORE models." | tee -a "$LOG"
                 MODELS_TO_PULL=("${CORE_MODELS[@]}")
            elif [ ${#MODELS_TO_PULL[@]} -eq 0 ]; then
                 echo "No models selected. Defaulting to CORE models." | tee -a "$LOG"
                 MODELS_TO_PULL=("${CORE_MODELS[@]}")
            fi
            ;;
        4)
            echo "Skipping Ollama model downloads as per user choice." | tee -a "$LOG"
            # MODELS_TO_PULL will remain empty
            ;;
        *)
            echo "Invalid choice. Defaulting to CORE set of models." | tee -a "$LOG"
            MODELS_TO_PULL=("${CORE_MODELS[@]}")
            ;;
    esac

    if [ ${#MODELS_TO_PULL[@]} -eq 0 ]; then
        echo "No models selected for download. Skipping Ollama model pulling phase." | tee -a "$LOG"
        return
    fi

    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "The following ${#MODELS_TO_PULL[@]} models will be downloaded:" | tee -a "$LOG"
    for model_to_pull in "${MODELS_TO_PULL[@]}"; do
        echo "- $model_to_pull" | tee -a "$LOG"
    done
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    sleep 3 # Give user time to read

    FAILED_MODELS=()
    SUCCESSFUL_MODELS=0
    TOTAL_MODELS_TO_PULL=${#MODELS_TO_PULL[@]}

    for model in "${MODELS_TO_PULL[@]}";do # Iterate over MODELS_TO_PULL
        echo "Pulling $model ($((SUCCESSFUL_MODELS + ${#FAILED_MODELS[@]} + 1))/$TOTAL_MODELS_TO_PULL)..." | tee -a "$LOG"
        ollama pull "$model"
        if [ $? -ne 0 ]; then
            echo "WARNING: Failed to pull model $model. It will be skipped. Check $LOG for details." | tee -a "$LOG"
            FAILED_MODELS+=("$model")
        else
            echo "Successfully pulled $model." | tee -a "$LOG"
            SUCCESSFUL_MODELS=$((SUCCESSFUL_MODELS + 1))
        fi
    done

    echo "Ollama model pulling complete. $SUCCESSFUL_MODELS/$TOTAL_MODELS models downloaded successfully." | tee -a "$LOG"

    if [ ${#FAILED_MODELS[@]} -ne 0 ]; then
        echo "Summary of failed model downloads (${#FAILED_MODELS[@]}):" | tee -a "$LOG"
        for failed_model in "${FAILED_MODELS[@]}"; do
            echo "- $failed_model" | tee -a "$LOG"
        done
    fi
}

# Function for generating master_orchestrator.py
create_agent_orchestration_script() {
    progress "CREATING AGENT CONFIGURATION FILE (agents.json)"
    cat>"$INSTALL_DIR/agents.json"<<'AGENTS_EOF'
[
  {
    "name": "DeepThink",
    "model": "deepseek-r1:32b",
    "specialty": "Advanced Reasoning & Logic",
    "active": true
  },
  {
    "name": "MasterPlanner",
    "model": "mixtral:8x22b",
    "specialty": "Complex Task Planning and Decomposition into Agent Steps. Output ONLY JSON plans.",
    "active": true
  },
  {
    "name": "CodeMaster",
    "model": "deepseek-coder-v2:16b",
    "specialty": "Programming & Development",
    "active": true
  },
  {
    "name": "DataWizard",
    "model": "qwen2.5:72b",
    "specialty": "Data Analysis & Processing",
    "active": true
  },
  {
    "name": "WebCrawler",
    "model": "dolphin-mixtral:8x7b",
    "specialty": "Web Research & Intelligence",
    "active": true
  },
  {
    "name": "DocProcessor",
    "model": "llama3.1:70b",
    "specialty": "Document Analysis & Generation",
    "active": true
  },
  {
    "name": "VisionAI",
    "model": "llava:34b",
    "specialty": "Image & Visual Processing",
    "active": true
  },
  {
    "name": "MathGenius",
    "model": "deepseek-math:7b",
    "specialty": "Mathematical Computations",
    "active": true
  },
  {
    "name": "CreativeWriter",
    "model": "nous-hermes2:34b",
    "specialty": "Creative Content Generation",
    "active": true
  },
  {
    "name": "SystemAdmin",
    "model": "codellama:34b",
    "specialty": "System Administration",
    "active": true
  },
  {
    "name": "SecurityExpert",
    "model": "mixtral:8x22b",
    "specialty": "Cybersecurity Analysis",
    "active": true
  },
  {
    "name": "ResearchBot",
    "model": "yi:34b",
    "specialty": "Scientific Research",
    "active": true
  },
  {
    "name": "MultiLang",
    "model": "qwen2.5-coder:32b",
    "specialty": "Multilingual Processing",
    "active": true
  },
  {
    "name": "ImageForge",
    "model": "diffusers/stable-diffusion-xl-base-1.0",
    "specialty": "Image Generation",
    "active": true
  },
  {
    "name": "AudioMaestro",
    "model": "pydub/pyttsx3",
    "specialty": "Audio Processing & TTS",
    "active": true
  }
]
AGENTS_EOF
    echo "Created agents.json in $INSTALL_DIR" | tee -a "$LOG"

    progress "CREATING AGENT ORCHESTRATION SCRIPT (master_orchestrator.py)" # Clarified progress message
    cat>"$INSTALL_DIR/agents/master_orchestrator.py"<<'EOF'
import asyncio, json, requests, subprocess, threading, queue, time, datetime
import torch
import aiohttp
from diffusers import DiffusionPipeline
from moviepy.editor import VideoFileClip
from pydub import AudioSegment
import pyttsx3
import shutil # Added for file backup operations
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
from dataclasses import dataclass
from typing import List,Dict,Any,Optional
from pathlib import Path
from transformers import pipeline as hf_pipeline
import sys # For platform detection
import re # For parsing 'top N' processes
import platform # For OS info
# subprocess, shutil, asyncio, Path are already effectively imported or used within methods
from tools.auto_dev import auto_dev # Added for project scaffolding
# streamlit and pandas are not directly used here but in the UI script that imports this.

# ChromaDB for Knowledge Base
import chromadb
from chromadb.utils import embedding_functions # For default embedding function
import uuid # For generating unique IDs for knowledge entries

@dataclass
class Agent:
   name:str;model:str;specialty:str;active:bool=True

class TerminusOrchestrator:
   def __init__(self):
       self.agents = []
       base_install_dir = Path(__file__).parent.parent # Assuming master_orchestrator.py is in $INSTALL_DIR/agents/
       agents_file_path = base_install_dir / "agents.json"
       try:
           with open(agents_file_path, 'r') as f:
               agents_data = json.load(f)
           for agent_config in agents_data:
               self.agents.append(Agent(
                   name=agent_config.get('name'),
                   model=agent_config.get('model'),
                   specialty=agent_config.get('specialty'),
                   active=agent_config.get('active', True)
               ))
       except FileNotFoundError:
           print(f"ERROR: agents.json not found at {agents_file_path}. No agents loaded.")
       except json.JSONDecodeError:
           print(f"ERROR: Could not decode agents.json. Invalid JSON format. No agents loaded.")
       except Exception as e:
           print(f"ERROR: An unexpected error occurred while loading agents from agents.json: {e}. No agents loaded.")

       self.ollama_url="http://localhost:11434/api/generate"
       self.active_tasks={}

       # Image Generation Setup
       self.image_gen_pipeline = None
       self.device = "cuda" if torch.cuda.is_available() else "cpu"
       self.image_gen_model_id = "stabilityai/stable-diffusion-xl-base-1.0" # Default, can be made configurable
       self.generated_images_dir = Path(__file__).parent.parent / "data" / "generated_images"
       self.generated_images_dir.mkdir(parents=True, exist_ok=True)

       # Video Processing Setup
       self.video_processing_dir = Path(__file__).parent.parent / "data" / "video_outputs"
       self.video_processing_dir.mkdir(parents=True, exist_ok=True)

       # Audio Processing Setup
       self.audio_processing_dir = Path(__file__).parent.parent / "data" / "audio_outputs"
       self.audio_processing_dir.mkdir(parents=True, exist_ok=True)
       try:
           self.tts_engine = pyttsx3.init()
       except Exception as e:
           print(f"WARNING: Failed to initialize TTS engine (pyttsx3): {e}. TTS functionality will be unavailable.")
           self.tts_engine = None

       # Zero-Shot Intent Classifier Setup
       self.intent_classifier = None
       self.intent_classifier_model = "facebook/bart-large-mnli"
       self.candidate_intent_labels = [
           "image_generation", "code_generation", "code_modification", "code_explanation",
           "project_scaffolding", "video_info", "video_frame_extraction", "video_to_gif",
           "audio_info", "audio_format_conversion", "text_to_speech",
           "data_analysis", "web_search", "document_processing", "general_question_answering",
           "complex_task_planning"
       ]
       try:
           print(f"Initializing Zero-Shot Intent Classifier ({self.intent_classifier_model})...")
           self.intent_classifier = hf_pipeline("zero-shot-classification", model=self.intent_classifier_model, device=self.device)
           print("Intent Classifier initialized.")
       except Exception as e:
           print(f"WARNING: Failed to initialize Zero-Shot Intent Classifier: {e}. Intent classification will be unavailable.")
           self.intent_classifier = None

       self.conversation_history = []
       self.max_history_items = 6 # e.g., 3 user + 3 assistant turns

       # Knowledge Base Setup (ChromaDB)
       self.knowledge_collection = None
       try:
           chroma_db_path = str(base_install_dir / "data" / "vector_store")
           # Ensure the parent directory exists for ChromaDB, as PersistentClient might not create it.
           Path(chroma_db_path).parent.mkdir(parents=True, exist_ok=True)

           self.chroma_client = chromadb.PersistentClient(path=chroma_db_path)

           # Using a default SentenceTransformer embedding function from chromadb.utils
           # This requires sentence-transformers to be installed.
           default_ef = embedding_functions.SentenceTransformerEmbeddingFunction()

           self.knowledge_collection_name = "terminus_knowledge_v1"
           self.knowledge_collection = self.chroma_client.get_or_create_collection(
               name=self.knowledge_collection_name,
               embedding_function=default_ef # Assign the embedding function here
           )
           print(f"Knowledge base initialized. Collection '{self.knowledge_collection_name}' loaded/created at {chroma_db_path}.")
           # Test add - TODO remove later or make conditional
           # self.knowledge_collection.add(documents=["Test document"], metadatas=[{"source":"init"}], ids=["init_test_id_123"])
           # print(f"Knowledge base count: {self.knowledge_collection.count()}")

       except Exception as e:
           print(f"CRITICAL ERROR: Failed to initialize ChromaDB knowledge base: {e}")
           print("Knowledge base functionalities will be unavailable.")
           # self.knowledge_collection will remain None, methods should check for this.

   async def _execute_single_plan_step(self, step_definition: Dict, full_plan_list: List[Dict], current_step_outputs: Dict) -> Dict:
       """
       Executes a single, non-parallel plan step, including its own retry logic.
       Args:
           step_definition: The dictionary defining the step to execute.
           full_plan_list: The entire list of plan steps (used for resolving dependencies' output_variable_name).
           current_step_outputs: Dictionary of outputs from already executed steps in the current plan attempt.
       Returns:
           A dictionary representing the result of the step execution.
       """
       step_id = step_definition.get("step_id")
       agent_name = step_definition.get("agent_name")
       task_prompt = step_definition.get("task_prompt", "")
       dependencies = step_definition.get("dependencies", [])
       output_var_name = step_definition.get("output_variable_name")

       # Retrieve retry parameters for this specific step
       max_retries = step_definition.get("max_retries", 0)
       retry_delay_seconds = step_definition.get("retry_delay_seconds", 5)
       retry_on_statuses = step_definition.get("retry_on_statuses", ["error"])

       current_execution_retries = 0 # Renamed to avoid conflict with group retries

       target_agent = next((a for a in self.agents if a.name == agent_name), None)
       if not target_agent:
           return {"status": "error", "agent": agent_name, "step_id": step_id, "response": f"Agent '{agent_name}' not found for step {step_id}."}

       step_result = {} # Ensure step_result is defined

       while True: # Inner loop for step's own execution and retries
           current_task_prompt = task_prompt
           for dep_id in dependencies:
               dep_output_key_to_find = None
               for prev_step_def in full_plan_list:
                   if prev_step_def.get("step_id") == dep_id:
                       # Try to get output from the main step_outputs (if dep was sequential)
                       # or from a parallel group's aggregated output
                       # This part might need refinement if prev_step_def was a parallel group
                       # For now, assume direct output_variable_name is in current_step_outputs
                       dep_output_key_to_find = prev_step_def.get("output_variable_name", f"step_{dep_id}_output")
                       break

               if dep_output_key_to_find and dep_output_key_to_find in current_step_outputs:
                   # Check if the dependency output is a dictionary (e.g. from a parallel group)
                   # and if the task_prompt is trying to access a sub-key
                   # Example: {{{{agg_output.sub_output_A}}}}
                   match = re.match(r"{{{{(\w+)\.(.+)}}}}", current_task_prompt)
                   if match and match.group(1) == dep_output_key_to_find and isinstance(current_step_outputs[dep_output_key_to_find], dict):
                       # This is a simplistic way to handle one level of sub-key access
                       # A more robust template engine might be needed for complex cases
                       sub_key = match.group(2)
                       if sub_key in current_step_outputs[dep_output_key_to_find]:
                           replacement_value = str(current_step_outputs[dep_output_key_to_find][sub_key])
                           current_task_prompt = current_task_prompt.replace(f"{{{{{dep_output_key_to_find}.{sub_key}}}}}", replacement_value)
                       else:
                           print(f"Warning: Sub-key '{sub_key}' not found in output of dependency step {dep_id} (variable {dep_output_key_to_find}) for step {step_id}.")
                   else: # Standard replacement
                        current_task_prompt = current_task_prompt.replace(f"{{{{{dep_output_key_to_find}}}}}", str(current_step_outputs[dep_output_key_to_find]))
               elif dep_output_key_to_find: # Key was found in plan but not in outputs
                   print(f"Warning: Output for dependency step {dep_id} (variable {dep_output_key_to_find}) not found in current_step_outputs for step {step_id}. Prompt may be incomplete.")
               # If dep_output_key_to_find is None, the dependency wasn't in full_plan_list (should not happen if plan is valid)

           log_prompt = f"Executing single step {step_id}"
           if current_execution_retries > 0:
               log_prompt += f" (Retry {current_execution_retries}/{max_retries})"
           log_prompt += f": Agent='{agent_name}', Prompt='{current_task_prompt[:100]}...'"
           print(log_prompt)

           step_result = await self.execute_agent(target_agent, current_task_prompt)

           if step_result.get("status") == "success":
               key_to_store = output_var_name if output_var_name else f"step_{step_id}_output"
               current_step_outputs[key_to_store] = step_result.get("response") # Modify current_step_outputs directly
               for media_key in ["image_path", "frame_path", "gif_path", "speech_path", "modified_file"]:
                   if media_key in step_result:
                       current_step_outputs[f"{key_to_store}_{media_key}"] = step_result[media_key]
               return step_result # Return success result

           current_execution_retries += 1
           if current_execution_retries <= max_retries and step_result.get("status") in retry_on_statuses:
               print(f"Single step {step_id} failed with status '{step_result.get('status')}'. Retrying in {retry_delay_seconds}s... ({current_execution_retries}/{max_retries})")
               await asyncio.sleep(retry_delay_seconds)
           else:
               print(f"Single step {step_id} (Agent: {agent_name}) failed permanently after {current_execution_retries-1} retries or due to non-retryable status '{step_result.get('status')}'.")
               return step_result # Return final failure result

       return step_result # Should be unreachable if loop logic is correct, but as a fallback

   async def store_knowledge(self, content: str, metadata: Optional[Dict] = None, content_id: Optional[str] = None) -> Dict:
       if self.knowledge_collection is None:
           return {"status": "error", "message": "Knowledge base not initialized."}

       if not content or not isinstance(content, str):
           return {"status": "error", "message": "Content must be a non-empty string."}

       try:
           # Ensure content_id is a string; generate if not provided or not a string
           final_content_id = content_id
           if not final_content_id or not isinstance(final_content_id, str):
               final_content_id = str(uuid.uuid4())

           # Clean metadata: ChromaDB expects values to be str, int, float, or bool.
           cleaned_metadata = {}
           if metadata:
               if not isinstance(metadata, dict):
                   print(f"Warning: Metadata provided was not a dict, ignoring. Type: {type(metadata)}")
               else:
                   for k, v in metadata.items():
                       if isinstance(v, (str, int, float, bool)):
                           cleaned_metadata[k] = v
                       else:
                           # Attempt to convert other types to string, or log/skip
                           cleaned_metadata[k] = str(v)
                           print(f"Warning: Metadata value for key '{k}' was converted to string. Original type: {type(v)}")

           # ChromaDB's add expects lists for documents, metadatas, and ids.
           self.knowledge_collection.add(
               documents=[content],
               metadatas=[cleaned_metadata] if cleaned_metadata else [None], # Pass [None] if metadata is empty after cleaning
               ids=[final_content_id]
           )
           # print(f"Stored knowledge with ID: {final_content_id}, metadata: {cleaned_metadata}, count: {self.knowledge_collection.count()}")
           return {"status": "success", "id": final_content_id, "message": f"Content stored with ID: {final_content_id}."}

       except chromadb.errors.IDAlreadyExistsError:
           # Option 1: Update existing entry (if desired, requires .update method and careful handling)
           # Option 2: Return an error indicating ID conflict
           print(f"Error: Knowledge item with ID '{final_content_id}' already exists.")
           return {"status": "error", "id": final_content_id, "message": f"Knowledge item with ID '{final_content_id}' already exists."}
       except Exception as e:
           print(f"ERROR: Failed to store knowledge: {e}")
           # Consider more specific error types if ChromaDB raises them
           return {"status": "error", "message": f"Failed to store knowledge: {str(e)}"}

   async def retrieve_knowledge(self, query_text: str, n_results: int = 5, filter_metadata: Optional[Dict] = None) -> Dict:
       if self.knowledge_collection is None:
           return {"status": "error", "message": "Knowledge base not initialized.", "results": []}

       if not query_text or not isinstance(query_text, str):
           return {"status": "error", "message": "Query text must be a non-empty string.", "results": []}

       if not isinstance(n_results, int) or n_results <= 0:
           print(f"Warning: Invalid n_results value ({n_results}), defaulting to 5.")
           n_results = 5

       try:
           # Metadata filtering (where clause)
           # ChromaDB's where clause can be complex. For now, we'll assume filter_metadata is a simple dict for exact matches.
           # More complex filters like {"$and": [...], "$or": [...]} or operators like {"$gt": value} would require
           # more sophisticated parsing or direct passing if the user forms it correctly.
           # For this initial implementation, we'll keep it simple. If filter_metadata is provided,
           # we'll assume it's for direct equality checks.

           # Ensure filter_metadata keys are strings and values are simple types if provided.
           cleaned_filter_metadata = None
           if filter_metadata:
               if not isinstance(filter_metadata, dict):
                   print(f"Warning: filter_metadata provided was not a dict, ignoring. Type: {type(filter_metadata)}")
               else:
                   cleaned_filter_metadata = {}
                   for k, v in filter_metadata.items():
                       if isinstance(v, (str, int, float, bool)):
                           cleaned_filter_metadata[k] = v
                       else:
                           print(f"Warning: Value for filter_metadata key '{k}' is not a simple type, skipping this filter condition. Type: {type(v)}")
                   if not cleaned_filter_metadata: # If all items were invalid
                       cleaned_filter_metadata = None

           query_results = self.knowledge_collection.query(
               query_texts=[query_text],
               n_results=n_results,
               where=cleaned_filter_metadata # Pass None if no valid filter
           )

           # Process results into a more usable format
           # query_results is a dict with keys like 'ids', 'documents', 'metadatas', 'distances'
           # Each value is a list of lists (one inner list per query_text, here only one)
           results_list = []
           if query_results and query_results.get('ids') and query_results.get('ids')[0]:
               ids = query_results['ids'][0]
               documents = query_results.get('documents', [[]])[0]
               metadatas = query_results.get('metadatas', [[]])[0]
               distances = query_results.get('distances', [[]])[0] # Lower distance is better

               for i in range(len(ids)):
                   results_list.append({
                       "id": ids[i],
                       "document": documents[i] if i < len(documents) else None,
                       "metadata": metadatas[i] if i < len(metadatas) else None,
                       "distance": distances[i] if i < len(distances) else None
                   })

           return {"status": "success", "results": results_list, "message": f"Retrieved {len(results_list)} results."}

       except Exception as e:
           print(f"ERROR: Failed to retrieve knowledge: {e}")
           return {"status": "error", "message": f"Failed to retrieve knowledge: {str(e)}", "results": []}

   def get_conversation_history_for_display(self) -> List[Dict]:
       # Returns a copy of the conversation history.
       # Each item is expected to be a dict like {'role': 'user'/'assistant', 'content': '...'}
       return list(self.conversation_history)

   async def scaffold_new_project(self, project_name: str, project_type: str) -> Dict:
       if not project_name or not project_type:
           return {"status": "error", "message": "Project name and type are required."}

       # Sanitize project_name to prevent directory traversal or invalid characters
       # A simple alphanumeric check, allowing underscores and hyphens
       safe_project_name = "".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in project_name)
       if not safe_project_name: # Handle case where sanitization results in an empty string
           safe_project_name = "default_project_name"

       print(f"Attempting to scaffold project: Name='{safe_project_name}', Type='{project_type}'")
       try:
           # auto_dev is imported at the top of the file now
           message = auto_dev.create_project(name=safe_project_name, project_type=project_type)

           if "successfully" in message:
               return {"status": "success", "message": message, "project_name": safe_project_name, "project_type": project_type}
           else:
               return {"status": "error", "message": message}
       except Exception as e:
           error_message = f"Failed to scaffold project '{safe_project_name}' of type '{project_type}': {str(e)}"
           print(f"ERROR: {error_message}")
           return {"status": "error", "message": error_message}

   async def get_video_metadata(self, video_path: str) -> Dict:
       try:
           target_video_path = Path(video_path)
           if not target_video_path.is_file():
               return {"status": "error", "message": f"Video file not found: {video_path}"}

           clip = VideoFileClip(str(target_video_path))
           metadata = {
               "filename": target_video_path.name,
               "duration_seconds": clip.duration,
               "fps": clip.fps,
               "width": clip.w,
               "height": clip.h,
           }
           if hasattr(clip, 'aspect_ratio'):
               metadata["aspect_ratio"] = clip.aspect_ratio
           clip.close()
           return {"status": "success", "message": "Video metadata extracted.", "metadata": metadata}
       except Exception as e:
           return {"status": "error", "message": f"Failed to get video metadata for '{video_path}': {str(e)}"}

   async def extract_video_frame(self, video_path: str, timestamp_str: str) -> Dict:
       clip = None # Initialize clip to None for error handling
       try:
           target_video_path = Path(video_path)
           if not target_video_path.is_file():
               return {"status": "error", "message": f"Video file not found: {video_path}"}

           timestamp_sec_for_filename = timestamp_str.replace(':','-').replace('.', '_')

           clip = VideoFileClip(str(target_video_path))

           try:
               numeric_ts = float(timestamp_str) if ':' not in timestamp_str else None
               if numeric_ts is not None and numeric_ts > clip.duration:
                   clip.close()
                   return {"status": "error", "message": f"Timestamp {timestamp_str} is beyond video duration ({clip.duration}s)."}
           except ValueError:
               pass # Let moviepy handle HH:MM:SS format

           frame_filename = f"frame_{target_video_path.stem}_at_{timestamp_sec_for_filename}.png"
           frame_path = self.video_processing_dir / frame_filename

           clip.save_frame(str(frame_path), t=timestamp_str)
           clip.close()

           return {"status": "success", "message": f"Frame extracted to {frame_path}", "frame_path": str(frame_path)}
       except Exception as e:
           if clip and hasattr(clip, 'close'):
               clip.close()
           return {"status": "error", "message": f"Failed to extract frame from '{video_path}' at '{timestamp_str}': {str(e)}"}

   async def convert_video_to_gif(self, video_path: str, start_str: str, end_str: str, resolution_scale: float = 0.5, fps: int = 10) -> Dict:
       clip = None
       subclip = None
       subclip_resized = None
       try:
           target_video_path = Path(video_path)
           if not target_video_path.is_file():
               return {"status": "error", "message": f"Video file not found: {video_path}"}

           clip = VideoFileClip(str(target_video_path))

           subclip = clip.subclip(start_str, end_str)

           if resolution_scale != 1.0 and resolution_scale > 0:
               subclip_resized = subclip.resize(resolution_scale)
           else:
               subclip_resized = subclip

           start_fn = start_str.replace(':','-').replace('.', '_')
           end_fn = end_str.replace(':','-').replace('.', '_')
           gif_filename = f"gif_{target_video_path.stem}_{start_fn}_to_{end_fn}.gif"
           gif_path = self.video_processing_dir / gif_filename

           subclip_resized.write_gif(str(gif_path), fps=fps)

           if subclip_resized is not subclip : # if resize happened and created a new object
                if hasattr(subclip_resized, 'close'): subclip_resized.close()
           if hasattr(subclip, 'close'): subclip.close() # Always close original subclip
           if hasattr(clip, 'close'): clip.close() # Always close original clip

           return {"status": "success", "message": f"GIF created: {gif_path}", "gif_path": str(gif_path)}
       except Exception as e:
           if hasattr(subclip_resized, 'close') and subclip_resized is not None: subclip_resized.close()
           if hasattr(subclip, 'close') and subclip is not None and (subclip_resized is None or subclip_resized is not subclip) : subclip.close()
           if hasattr(clip, 'close') and clip is not None : clip.close()
           return {"status": "error", "message": f"Failed to convert video '{video_path}' to GIF: {str(e)}"}

   async def modify_code_in_project(self, project_name: str, relative_file_path: str, modification_instruction: str) -> Dict:
       if not project_name or not relative_file_path or not modification_instruction:
           return {"status": "error", "message": "Project name, file path, and modification instruction are required."}

       project_base_path = Path(__file__).parent.parent

       safe_project_name = "".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in project_name)

       safe_relative_file_path_parts = []
       for part in Path(relative_file_path).parts:
           if part == '..': # Disallow parent directory traversal
               continue
           safe_part = "".join(c if c.isalnum() or c in ['_', '-', '.'] else '_' for c in part)
           if safe_part:
                safe_relative_file_path_parts.append(safe_part)

       if not safe_project_name or not safe_relative_file_path_parts:
            return {"status": "error", "message": "Invalid project name or file path after sanitization."}

       safe_relative_file_path = Path(*safe_relative_file_path_parts)
       target_file_path = (project_base_path / safe_project_name / safe_relative_file_path).resolve()

       expected_project_dir = (project_base_path / safe_project_name).resolve()
       # Check if target_file_path is within expected_project_dir
       # This check means target_file_path must be equal to expected_project_dir OR one of its children
       if not (target_file_path == expected_project_dir or expected_project_dir in target_file_path.parents):
           return {"status": "error", "message": "File path manipulation detected or invalid path structure."}

       print(f"Attempting to modify file: {target_file_path} in project: {safe_project_name}")

       if not target_file_path.is_file():
           return {"status": "error", "message": f"File not found: {target_file_path}"}

       try:
           original_code = target_file_path.read_text(encoding='utf-8')

           backup_file_path = target_file_path.with_suffix(target_file_path.suffix + f".bak_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}")
           shutil.copy2(target_file_path, backup_file_path)
           print(f"Backup of original file created at: {backup_file_path}")

           llm_prompt = (
               f"You are an expert programmer. Your task is to modify the given code based on a user request.\n"
               f"Original code from file '{safe_relative_file_path}':\n"
               f"```\n{original_code}\n```\n\n"
               f"User request for modification: {modification_instruction}\n\n"
               f"Follow these instructions carefully:\n"
               f"1. Apply the requested modification to the original code.\n"
               f"2. If possible, make only the necessary changes to fulfill the request and keep the rest of the code intact. Avoid reformatting or changing unrelated parts of the code.\n"
               f"3. If adding new functions or complex logic, include brief comments or docstrings for clarity, following the style of the original code if possible.\n"
               f"4. If the request involves adding new functionality that might fail (e.g., file operations, network requests), include basic error handling (e.g., try-except blocks) where appropriate and idiomatic for the language.\n"
               f"5. Ensure the modified code is syntactically correct and adheres to common best practices for the language of the original code.\n"
               f"6. VERY IMPORTANT: Output *only* the complete, raw, modified code for the entire file. Do not include any surrounding text, explanations, apologies, or markdown formatting such as ```python ... ``` or similar, around the code block. Just the pure, new code content for the file.\n"
           )

           codemaster_agent = next((agent for agent in self.agents if agent.name == "CodeMaster"), None)
           if not codemaster_agent:
               return {"status": "error", "message": "CodeMaster agent not found."}

           print(f"Sending modification task to CodeMaster ({codemaster_agent.model})...")
           modification_result = await self.execute_agent(codemaster_agent, llm_prompt) # context can be None

           if modification_result.get("status") == "success":
               modified_code = modification_result.get("response", "").strip()
               if not modified_code:
                   return {"status": "error", "message": "CodeMaster returned empty code. No changes applied. Original file restored from backup if possible, or backup retained."}

               target_file_path.write_text(modified_code, encoding='utf-8')
               success_msg = (
                   f"File '{target_file_path}' modified successfully by CodeMaster. "
                   f"Original backed up to '{backup_file_path}'. "
                   f"Please review the changes carefully."
               )
               print(success_msg)
               return {"status": "success", "message": success_msg, "modified_file": str(target_file_path)}
           else:
               err_msg = modification_result.get('response', 'CodeMaster failed to process the modification.')
               print(f"CodeMaster execution failed: {err_msg}")
               return {"status": "error", "message": f"CodeMaster failed: {err_msg}. Backup of original file is at {backup_file_path}."}

       except Exception as e:
           error_message = f"Failed to modify code in '{target_file_path}': {str(e)}"
           print(f"ERROR: {error_message}")
           backup_msg = f"Backup might be available at {backup_file_path}" if 'backup_file_path' in locals() else "No backup was made before error."
           return {"status": "error", "message": f"{error_message}. {backup_msg}"}

   async def generate_code_module(self, requirements: str, language: str = "python") -> Dict:
       if not requirements.strip():
           return {"status": "error", "message": "Code generation requirements cannot be empty."}

       print(f"Attempting to generate code module (language: {language}) for requirements: '{requirements[:100]}...'")
       try:
           llm_prompt = (
               f"You are an expert programmer. Your task is to generate a complete code module or class structure in {language} "
               f"based on the following requirements.\n\n"
               f"Requirements:\n{requirements}\n\n"
               f"Please adhere to these instructions:\n"
               f"1. Generate well-structured, clean, and idiomatic code for the specified language.\n"
               f"2. If the requirements imply a class, generate the class structure. If they imply a module with functions, generate that.\n"
               f"3. Include necessary imports if they are common and obvious for the tasks described.\n"
               f"4. Add brief comments or docstrings to explain major components (classes, functions, complex logic), following typical conventions for {language}.\n"
               f"5. Ensure the generated code is syntactically correct.\n"
               f"6. VERY IMPORTANT: Output *only* the complete, raw code for the module/class. Do not include any surrounding text, explanations, apologies, or markdown formatting such as ```python ... ``` or similar, around the code block. Just the pure code.\n"
           )

           codemaster_agent = next((agent for agent in self.agents if agent.name == "CodeMaster"), None)
           if not codemaster_agent:
               return {"status": "error", "message": "CodeMaster agent not found."}

           print(f"Sending code module generation task to CodeMaster ({codemaster_agent.model})...")
           generation_result = await self.execute_agent(codemaster_agent, llm_prompt)

           if generation_result.get("status") == "success":
               generated_code = generation_result.get("response", "").strip()
               if not generated_code:
                    return {"status": "error", "message": "CodeMaster returned empty code for the module."}

               print("Code module generated successfully.")
               return {"status": "success", "message": "Code module generated successfully.", "generated_code": generated_code}
           else:
               err_msg = generation_result.get('response', 'CodeMaster failed to generate the code module.')
               print(f"CodeMaster execution for module generation failed: {err_msg}")
               return {"status": "error", "message": f"CodeMaster failed: {err_msg}"}

       except Exception as e:
           error_message = f"Failed to generate code module: {str(e)}"
           print(f"ERROR: {error_message}")
           return {"status": "error", "message": error_message}

   async def explain_code_snippet(self, code_snippet: str, language: str = "python") -> Dict:
       if not code_snippet.strip():
           return {"status": "error", "message": "Code snippet cannot be empty."}

       print(f"Attempting to explain code snippet (language: {language}): '{code_snippet[:100]}...'")
       try:
           llm_prompt = (
               f"You are an expert programmer. Your task is to explain the following code snippet written in {language}.\n\n"
               f"Code Snippet:\n```\n{code_snippet}\n```\n\n"
               f"Please provide a clear, concise, and informative explanation of what this code does, its purpose, and how it works. "
               f"If there are complex parts, break them down. If there are potential improvements or issues, you can mention them briefly. "
               f"Format your explanation clearly, perhaps using markdown for readability (e.g., headings, bullet points if appropriate)."
           )

           explainer_agent = next((agent for agent in self.agents if agent.name == "CodeMaster"), None)
           if not explainer_agent:
               explainer_agent = next((agent for agent in self.agents if agent.name == "DeepThink"), None)

           if not explainer_agent:
               return {"status": "error", "message": "Suitable agent for code explanation not found."}

           print(f"Sending code explanation task to {explainer_agent.name} ({explainer_agent.model})...")
           explanation_result = await self.execute_agent(explainer_agent, llm_prompt)

           if explanation_result.get("status") == "success":
               explanation_text = explanation_result.get("response", "").strip()
               if not explanation_text:
                    return {"status": "error", "message": f"{explainer_agent.name} returned an empty explanation."}

               print("Code explanation generated successfully.")
               return {"status": "success", "message": "Code explained successfully.", "explanation": explanation_text}
           else:
               err_msg = explanation_result.get('response', f'{explainer_agent.name} failed to explain the code.')
               print(f"{explainer_agent.name} execution for code explanation failed: {err_msg}")
               return {"status": "error", "message": f"{explainer_agent.name} failed: {err_msg}"}

       except Exception as e:
           error_message = f"Failed to explain code snippet: {str(e)}"
           print(f"ERROR: {error_message}")
           return {"status": "error", "message": error_message}

   async def get_audio_info(self, audio_path: str) -> Dict:
       try:
           target_audio_path = Path(audio_path)
           if not target_audio_path.is_file():
               return {"status": "error", "message": f"Audio file not found: {audio_path}"}

           audio = AudioSegment.from_file(str(target_audio_path))
           info = {
               "filename": target_audio_path.name,
               "duration_seconds": len(audio) / 1000.0,
               "channels": audio.channels,
               "frame_rate_hz": audio.frame_rate,
               "sample_width_bytes": audio.sample_width,
               "max_amplitude": audio.max,
           }
           return {"status": "success", "message": "Audio information extracted.", "info": info}
       except Exception as e:
           return {"status": "error", "message": f"Failed to get audio info for '{audio_path}': {str(e)}"}

   async def convert_audio_format(self, audio_path: str, target_format: str = "mp3") -> Dict:
       try:
           target_audio_path = Path(audio_path)
           if not target_audio_path.is_file():
               return {"status": "error", "message": f"Audio file not found: {audio_path}"}

           target_format = target_format.lower().strip(".")
           if not target_format:
               return {"status": "error", "message": "Target format cannot be empty."}

           audio = AudioSegment.from_file(str(target_audio_path))

           output_filename = f"{target_audio_path.stem}_converted.{target_format}"
           output_path = self.audio_processing_dir / output_filename

           audio.export(str(output_path), format=target_format)

           return {"status": "success", "message": f"Audio converted to {target_format}: {output_path}", "output_path": str(output_path)}
       except Exception as e:
           return {"status": "error", "message": f"Failed to convert audio '{audio_path}' to '{target_format}': {str(e)}"}

   async def text_to_speech(self, text_to_speak: str, output_filename_stem: str = "tts_output") -> Dict:
       if not self.tts_engine:
           return {"status": "error", "message": "TTS engine not initialized. Cannot perform text-to-speech."}
       if not text_to_speak.strip():
           return {"status": "error", "message": "Text for TTS cannot be empty."}
       if not output_filename_stem.strip():
           output_filename_stem = "tts_output" # Default if empty after strip

       # Sanitize filename stem
       safe_filename_stem = "".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in output_filename_stem)
       if not safe_filename_stem: safe_filename_stem = "tts_output"

       output_filename = f"{safe_filename_stem}.mp3" # pyttsx3 often saves as mp3 by default or can do wav
       output_path = self.audio_processing_dir / output_filename

       try:
           print(f"Generating speech for: '{text_to_speak[:50]}...' -> {output_path}")
           self.tts_engine.save_to_file(text_to_speak, str(output_path))
           self.tts_engine.runAndWait() # Crucial for pyttsx3 to actually save the file

           # Verify file creation as runAndWait might not throw error for all engine issues
           if not output_path.is_file() or output_path.stat().st_size == 0:
               # Attempt with a .wav extension as a fallback for some engines/OS if mp3 failed
               output_filename = f"{safe_filename_stem}.wav"
               output_path = self.audio_processing_dir / output_filename
               self.tts_engine.save_to_file(text_to_speak, str(output_path))
               self.tts_engine.runAndWait()
               if not output_path.is_file() or output_path.stat().st_size == 0:
                   return {"status": "error", "message": "TTS file generation failed or produced empty file, even after trying .wav."}

           return {"status": "success", "message": f"Speech saved to {output_path}", "speech_path": str(output_path)}
       except Exception as e:
           return {"status": "error", "message": f"Failed to generate speech: {str(e)}"}

   async def generate_image_with_diffusion(self, prompt: str) -> Dict:
       if self.image_gen_pipeline is None:
           print(f"Loading image generation model ({self.image_gen_model_id})... This may take a while.")
           try:
               self.image_gen_pipeline = DiffusionPipeline.from_pretrained(
                   self.image_gen_model_id,
                   torch_dtype=torch.float16, # Use float16 for efficiency
                   use_safetensors=True
               )
               self.image_gen_pipeline.to(self.device)
               # Optional: if low VRAM, enable CPU offloading
               # if self.device == "cuda": # Only for CUDA, check VRAM if possible
               #     try:
               #         if torch.cuda.get_device_properties(0).total_memory < 8 * 1024**3: # Example: Less than 8GB VRAM
               #             print("Low VRAM detected, enabling model CPU offload for image generation.")
               #             self.image_gen_pipeline.enable_model_cpu_offload()
               #     except Exception as e:
               #         print(f"Could not check VRAM properties: {e}. Proceeding without CPU offload check.")
               print("Image generation model loaded.")
           except Exception as e:
               print(f"ERROR: Could not load image generation model: {str(e)}")
               return {"agent": "ImageForge", "response": f"Error loading model: {str(e)}", "status": "error", "image_path": None}

       print(f"Generating image for prompt: '{prompt}' on device: {self.device}")
       try:
           image = self.image_gen_pipeline(prompt).images[0]

           timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
           image_filename = f"image_{timestamp}.png"
           image_path = self.generated_images_dir / image_filename
           image.save(image_path)
           print(f"Image saved to {image_path}")

           return {
               "agent": "ImageForge",
               "response": f"Image generated successfully: {image_filename}",
               "status": "success",
               "image_path": str(image_path)
           }
       except Exception as e:
           print(f"ERROR: Failed to generate image: {str(e)}")
           return {"agent": "ImageForge", "response": f"Error generating image: {str(e)}", "status": "error", "image_path": None}

   async def get_disk_space(self) -> Dict:
       command = ["df", "-h"]
       try:
           if not shutil.which(command[0]):
               return {"status": "error", "data": f"Command '{command[0]}' not found.", "message": f"Command '{command[0]}' not found."}
           process = await asyncio.create_subprocess_exec(
               *command,
               stdout=asyncio.subprocess.PIPE,
               stderr=asyncio.subprocess.PIPE
           )
           stdout, stderr = await process.communicate()
           if process.returncode == 0:
               return {"status": "success", "data": stdout.decode(), "message": "Disk space information retrieved."}
           else:
               return {"status": "error", "data": stderr.decode(), "message": f"Error getting disk space: {stderr.decode()}"}
       except Exception as e:
           return {"status": "error", "data": str(e), "message": f"Failed to get disk space: {str(e)}"}

   async def get_memory_usage(self) -> Dict:
       command = []
       os_type = sys.platform
       if os_type == "linux" or os_type == "linux2":
           if shutil.which("free"):
               command = ["free", "-h"]
           else:
                return {"status": "error", "data": "Command 'free' not found on Linux.", "message": "Command 'free' not found on Linux."}
       elif os_type == "darwin": # macOS
           if shutil.which("vm_stat"):
               command = ["vm_stat"]
           else:
               return {"status": "error", "data": "Command 'vm_stat' not found on macOS.", "message": "Command 'vm_stat' not found on macOS."}
       else:
           return {"status": "error", "data": f"Memory usage command not supported on this OS: {os_type}", "message": f"Memory usage command not supported on this OS: {os_type}"}

       try:
           process = await asyncio.create_subprocess_exec(
               *command,
               stdout=asyncio.subprocess.PIPE,
               stderr=asyncio.subprocess.PIPE
           )
           stdout, stderr = await process.communicate()
           if process.returncode == 0:
               return {"status": "success", "data": stdout.decode(), "message": "Memory usage information retrieved."}
           else:
               return {"status": "error", "data": stderr.decode(), "message": f"Error getting memory usage: {stderr.decode()}"}
       except Exception as e:
           return {"status": "error", "data": str(e), "message": f"Failed to get memory usage: {str(e)}"}

   async def get_top_processes(self, n: int = 10) -> Dict:
       if not isinstance(n, int) or n <= 0:
           n = 10

       os_type = sys.platform
       cmd_ps_list = []
       if os_type == "linux" or os_type == "linux2":
           cmd_ps_list = ["ps", "aux", "--sort=-%cpu"]
       elif os_type == "darwin":
           cmd_ps_list = ["ps", "aux", "-r"]
       else:
           return {"status": "error", "data": f"Top processes command not supported on this OS: {os_type}", "message": f"Top processes command not supported on this OS: {os_type}"}

       cmd_head_list = ["head", f"-n{n+1}"]

       if not shutil.which(cmd_ps_list[0]) or not shutil.which(cmd_head_list[0]):
            return {"status": "error", "data": f"Required command '{cmd_ps_list[0]}' or '{cmd_head_list[0]}' not found.", "message": f"Required command '{cmd_ps_list[0]}' or '{cmd_head_list[0]}' not found."}

       try:
           ps_process = await asyncio.create_subprocess_exec(
               *cmd_ps_list,
               stdout=asyncio.subprocess.PIPE,
               stderr=asyncio.subprocess.PIPE
           )

           head_process = await asyncio.create_subprocess_exec(
               *cmd_head_list,
               stdin=ps_process.stdout,
               stdout=asyncio.subprocess.PIPE,
               stderr=asyncio.subprocess.PIPE
           )

           if ps_process.stdout:
               ps_process.stdout.close()

           stdout, stderr_head = await head_process.communicate()
           _, stderr_ps = await ps_process.communicate()

           if head_process.returncode == 0 and ps_process.returncode == 0 :
               return {"status": "success", "data": stdout.decode(), "message": f"Top {n} processes retrieved."}
           else:
               error_output = f"ps error: {stderr_ps.decode().strip() if stderr_ps else 'N/A'}. head error: {stderr_head.decode().strip() if stderr_head else 'N/A'}."
               return {"status": "error", "data": error_output, "message": f"Error getting top processes: {error_output}"}
       except Exception as e:
           return {"status": "error", "data": str(e), "message": f"Failed to get top processes: {str(e)}"}

    async def get_os_info(self) -> Dict:
        try:
            uname_info = platform.uname()
            data = {
                "system": uname_info.system,
                "node_name": uname_info.node,
                "release": uname_info.release,
                "version": uname_info.version,
                "machine": uname_info.machine,
                "processor": uname_info.processor,
                "platform_string": platform.platform()
            }
            return {"status": "success", "data": data, "message": "OS information retrieved."}
        except Exception as e:
            return {"status": "error", "data": str(e), "message": f"Failed to get OS information: {str(e)}"}

    async def get_cpu_info(self) -> Dict:
        os_type = sys.platform
        command = []
        data = {}

        try:
            if os_type == "linux" or os_type == "linux2":
                if shutil.which("lscpu"):
                    command = ["lscpu"]
                    process = await asyncio.create_subprocess_exec(*command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    stdout, stderr = await process.communicate()
                    if process.returncode == 0:
                        # Basic parsing for lscpu - more detailed parsing could be added
                        raw_data = stdout.decode()
                        data_lines = [line.split(":", 1) for line in raw_data.splitlines() if ":" in line]
                        data = {item[0].strip(): item[1].strip() for item in data_lines if len(item) == 2}
                        return {"status": "success", "data": data, "message": "CPU information retrieved using lscpu."}
                    else:
                        return {"status": "error", "data": stderr.decode(), "message": f"lscpu command failed: {stderr.decode()}"}
                elif Path("/proc/cpuinfo").is_file(): # Fallback for Linux
                    cpu_info_raw = Path("/proc/cpuinfo").read_text()
                    # Basic parsing of /proc/cpuinfo
                    data = {}
                    for line in cpu_info_raw.splitlines():
                        if ":" in line:
                            key, value = line.split(":", 1)
                            data[key.strip()] = value.strip()
                    return {"status": "success", "data": data, "message": "CPU information retrieved from /proc/cpuinfo."}
                else:
                    return {"status": "error", "data": "No suitable command/file for CPU info on Linux.", "message": "No suitable command/file for CPU info on Linux."}

            elif os_type == "darwin": # macOS
                if shutil.which("sysctl"):
                    command_brand = ["sysctl", "-n", "machdep.cpu.brand_string"]
                    command_cores = ["sysctl", "-n", "hw.physicalcpu"]
                    command_logical_cores = ["sysctl", "-n", "hw.logicalcpu"]

                    process_brand = await asyncio.create_subprocess_exec(*command_brand, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    stdout_brand, _ = await process_brand.communicate()
                    if process_brand.returncode == 0: data["brand_string"] = stdout_brand.decode().strip()

                    process_cores = await asyncio.create_subprocess_exec(*command_cores, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    stdout_cores, _ = await process_cores.communicate()
                    if process_cores.returncode == 0: data["physical_cores"] = stdout_cores.decode().strip()

                    process_logical = await asyncio.create_subprocess_exec(*command_logical_cores, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    stdout_logical, _ = await process_logical.communicate()
                    if process_logical.returncode == 0: data["logical_cores"] = stdout_logical.decode().strip()

                    return {"status": "success", "data": data, "message": "CPU information retrieved using sysctl."}
                else:
                    return {"status": "error", "data": "Command 'sysctl' not found on macOS.", "message": "Command 'sysctl' not found on macOS."}
            else:
                return {"status": "error", "data": f"CPU info command not supported on this OS: {os_type}", "message": f"CPU info command not supported on this OS: {os_type}"}
        except Exception as e:
            return {"status": "error", "data": str(e), "message": f"Failed to get CPU info: {str(e)}"}

    async def get_network_config(self) -> Dict:
        os_type = sys.platform
        command = []
        if os_type == "linux" or os_type == "linux2":
            if shutil.which("ip"):
                command = ["ip", "addr"]
            elif shutil.which("ifconfig"): # Fallback for older Linux
                command = ["ifconfig"]
            else:
                return {"status": "error", "data": "Commands 'ip' or 'ifconfig' not found on Linux.", "message": "Commands 'ip' or 'ifconfig' not found on Linux."}
        elif os_type == "darwin": # macOS
            if shutil.which("ifconfig"):
                command = ["ifconfig"]
            else:
                return {"status": "error", "data": "Command 'ifconfig' not found on macOS.", "message": "Command 'ifconfig' not found on macOS."}
        else:
            return {"status": "error", "data": f"Network config command not supported on this OS: {os_type}", "message": f"Network config command not supported on this OS: {os_type}"}

        try:
            process = await asyncio.create_subprocess_exec(*command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
            stdout, stderr = await process.communicate()
            if process.returncode == 0:
                return {"status": "success", "data": stdout.decode(), "message": "Network configuration retrieved."}
            else:
                return {"status": "error", "data": stderr.decode(), "message": f"Error getting network config: {stderr.decode()}"}
        except Exception as e:
            return {"status": "error", "data": str(e), "message": f"Failed to get network config: {str(e)}"}

   async def execute_agent(self, agent: Agent, prompt: str, context: Dict = None) -> Dict:
       if agent.name == "ImageForge": # Or use agent.specialty == "Image Generation"
           return await self.generate_image_with_diffusion(prompt)
       elif agent.name == "SystemAdmin":
           prompt_lower = prompt.lower().strip()
           if "disk space" in prompt_lower or "disk usage" in prompt_lower:
               return await self.get_disk_space()
           elif "memory usage" in prompt_lower or "ram usage" in prompt_lower or "free memory" in prompt_lower:
               return await self.get_memory_usage()
           elif "top processes" in prompt_lower or "running processes" in prompt_lower:
               num_processes = 10 # default
               try:
                   match = re.search(r"top\s*(\d+)\s*processes", prompt_lower)
                   if match and match.group(1):
                       num_processes = int(match.group(1))
               except Exception:
                   pass
               return await self.get_top_processes(n=num_processes) # Existing method
        elif "cpu info" in prompt_lower or "processor details" in prompt_lower:
            return await self.get_cpu_info()
        elif "os info" in prompt_lower or "operating system details" in prompt_lower:
            return await self.get_os_info()
        elif "network config" in prompt_lower or "ip address" in prompt_lower or "network interfaces" in prompt_lower:
            return await self.get_network_config()
        else:
            return {
                "status": "info",
                "agent": "SystemAdmin",
                "response": f"SystemAdmin received: '{prompt}'. Command not recognized for direct execution. Supported commands: 'disk space', 'memory usage', 'top N processes', 'cpu info', 'os info', 'network config'."
            }
    else:
        try:
               payload = {"model": agent.model, "prompt": f"[{agent.specialty}] {prompt}", "stream": False, "options": {"temperature": 0.7}}
               if context:
                   payload["prompt"] += f"\nContext: {json.dumps(context)}"

               async with aiohttp.ClientSession() as session:
                   async with session.post(self.ollama_url, json=payload) as resp:
                       if resp.status != 200:
                           error_text = await resp.text()
                           print(f"Ollama API Error for agent {agent.name} with model {agent.model}: {resp.status} - {error_text}")
                           return {"agent": agent.name, "model": agent.model, "response": f"Error from Ollama: {resp.status} - {error_text}", "status": "error"}
                       result = await resp.json()
                       return {"agent": agent.name, "model": agent.model, "response": result.get("response", "Error: No response field"), "status": "success"}
           except aiohttp.ClientConnectorError as e:
               print(f"Connection Error for agent {agent.name} (model {agent.model}) targeting {self.ollama_url}: {str(e)}")
               return {"agent": agent.name, "model": agent.model, "response": f"Connection Error: Could not connect to Ollama server at {self.ollama_url}. Details: {str(e)}", "status": "error"}
           except Exception as e:
               print(f"Generic Error executing agent {agent.name} (model {agent.model}): {str(e)}")
               return {"agent": agent.name, "model": agent.model, "response": f"Error executing agent: {str(e)}", "status": "error"}

   async def parallel_execution(self, prompt: str, selected_agents: List[str] = None, context: Dict = None) -> List[Dict]:
       prompt_lower = prompt.lower()
       active_agents_list = []
       agent_selection_reason = "User selected" # Default reason

       # Use a set to keep track of names to avoid duplicate Agent objects
       determined_agent_names = set()
       determined_agent_objects = []

       if not selected_agents:
           # 1. Primary Context-Based Selection
           if context and 'current_mode' in context:
               mode_to_primary_agent = {
                   "Image Generation": "ImageForge",
                   "Video Processing": "VideoCrafter", # Assuming this agent name exists
                   "Audio Processing": "AudioMaestro",
                   "Code Generation": "CodeMaster",
                   "Document Processing": "DocProcessor",
                   "Web Intelligence": "WebCrawler",
                   # Add other relevant modes if they have a clear primary agent
               }
               current_mode = context.get("current_mode")
               if current_mode and current_mode in mode_to_primary_agent:
                   primary_agent_name = mode_to_primary_agent[current_mode]
                   agent_obj = next((a for a in self.agents if a.name == primary_agent_name and a.active), None)
                   if agent_obj:
                       determined_agent_objects.append(agent_obj)
                       determined_agent_names.add(agent_obj.name)
                       agent_selection_reason = f"Context: Mode '{current_mode}'"

           # 2. Secondary Keyword-Based Selection
           agent_keywords = {
               "ImageForge": ["image of", "picture of", "draw a", "generate art", "create a photo", "generate image"],
               "CodeMaster": ["python code for", "write a script to", "generate a function that", "develop a program to", "code snippet for", "explain code", "generate module"],
               "DataWizard": ["analyze data", "statistics for csv", "excel report on", "plot data from", "database insights"],
               "WebCrawler": ["search the web for", "find information online about", "what's the latest on", "look up on internet"],
               "DocProcessor": ["summarize document", "analyze this text", "process pdf content", "read file content"],
               "MathGenius": ["calculate", "solve math", "what is the result of", "compute"],
               "AudioMaestro": ["audio", "sound", "music", "speech", "tts", "text to speech", "voice"],
               # Add other agent keywords as needed
           }

           keyword_selected_agents = []
           for agent_name, keywords in agent_keywords.items():
               if any(keyword in prompt_lower for keyword in keywords):
                   agent_obj = next((a for a in self.agents if a.name == agent_name and a.active), None)
                   if agent_obj and agent_obj.name not in determined_agent_names: # Add if not already added by context
                       keyword_selected_agents.append(agent_obj)
                       determined_agent_names.add(agent_obj.name)

           if keyword_selected_agents:
               determined_agent_objects.extend(keyword_selected_agents)
               if "Context: Mode" in agent_selection_reason : # If context already set a reason
                   agent_selection_reason += " & Keywords"
               else:
                   agent_selection_reason = "Keywords recognized"

           # Assign to active_agents_list if any were determined
           if determined_agent_objects:
               active_agents_list = determined_agent_objects
           else: # Fallback if neither context nor keywords selected agents
               default_general_agents = ["DeepThink", "CreativeWriter"]
               active_agents_list = [a for a in self.agents if a.name in default_general_agents and a.active]
               agent_selection_reason = "Default general agents"

               if not active_agents_list:
                   active_agents_list = [a for a in self.agents if a.active]
                   agent_selection_reason = "Fallback to all active"

       else: # User explicitly selected agents
           active_agents_list = [a for a in self.agents if a.name in selected_agents and a.active]
           # agent_selection_reason remains "User selected"

       if not active_agents_list: # Final check if any active agents are available
           print("No active agents determined for the prompt. Cannot execute.")
           return [{"agent": "System", "model": "N/A", "response": "No suitable active agents found for your request.", "status": "error"}]

       # Refined logging for agent selection
       selected_agent_names = [a.name for a in active_agents_list]
       print(f"Executing with agents: {selected_agent_names} for prompt '{prompt[:50]}...'. Reason: {agent_selection_reason}.")

       tasks = [self.execute_agent(agent, prompt, context) for agent in active_agents_list]
       results = await asyncio.gather(*tasks, return_exceptions=True)

       processed_results = []
       for i, r_or_e in enumerate(results):
           agent_name = active_agents_list[i].name if i < len(active_agents_list) else "UnknownAgent"
           agent_model = active_agents_list[i].model if i < len(active_agents_list) else "N/A"
           if isinstance(r_or_e, Exception):
               print(f"Error during execution for agent {agent_name}: {r_or_e}")
               processed_results.append({"agent": agent_name, "model": agent_model, "response": f"An unexpected error occurred: {str(r_or_e)}", "status": "error"})
           elif isinstance(r_or_e, dict):
               processed_results.append(r_or_e)
           else:
               processed_results.append({"agent": agent_name, "model": agent_model, "response": f"Unexpected result type: {type(r_or_e)}", "status": "error"})

       return processed_results

   def consensus_analysis(self,results:List[Dict])->Dict:
       responses=[r["response"] for r in results if r["status"]=="success"]
       # Basic consensus: return the most common response or the longest one if all unique
       if not responses:
           return {"consensus_score":0, "best_response":"No valid responses from agents.", "summary":"No successful agent responses."}

       # Example: Could count frequencies or find longest/most detailed if diverse.
       # For now, a simplified approach:
       best_response = max(responses, key=len) if responses else "No valid responses"
       consensus_score = len(responses) / len(results) if results else 0 # results could be empty if all selected_agents were inactive

       return {"consensus_score":consensus_score,"best_response":best_response,"summary":f"Processed by {len(responses)} of {len(results)} initially tasked agents."}

   async def classify_user_intent(self, user_prompt: str) -> Dict:
       if not self.intent_classifier:
           return {"status": "error", "message": "Intent classifier not available.", "intent": None, "scores": None}
       if not user_prompt.strip():
           return {"status": "error", "message": "Prompt is empty, cannot classify intent.", "intent": None, "scores": None}

       try:
           print(f"Classifying intent for prompt: '{user_prompt[:100]}...'")
           classification_result = self.intent_classifier(user_prompt, self.candidate_intent_labels, multi_label=True)

           top_intent = classification_result['labels'][0]
           scores_dict = {label: score for label, score in zip(classification_result['labels'], classification_result['scores'])}

           print(f"Top identified intent: {top_intent} (Score: {scores_dict[top_intent]:.4f})")
           return {"status": "success", "intent": top_intent, "all_scores": scores_dict, "raw_result": classification_result}

       except Exception as e:
           print(f"ERROR: Intent classification failed: {str(e)}")
           return {"status": "error", "message": f"Intent classification failed: {str(e)}", "intent": None, "scores": None}

   async def execute_master_plan(self, user_prompt: str) -> List[Dict]:
       print(f"MasterPlanner received user prompt: '{user_prompt[:100]}...'")

       self.conversation_history.append({"role": "user", "content": user_prompt})
       if len(self.conversation_history) > self.max_history_items:
           self.conversation_history = self.conversation_history[-self.max_history_items:]

       max_revision_attempts = 1
       current_attempt = 0
       original_plan_json_str = ""
       final_execution_results = []
       # Store classified intent from the first attempt to reuse in revisions
       first_attempt_intent_info = "No specific intent pre-classified or low confidence."
       retrieved_kb_context_for_plan = "" # Initialize variable to store KB context

       while current_attempt <= max_revision_attempts:
           current_attempt += 1
           print(f"MasterPlanner Attempt: {current_attempt}/{max_revision_attempts + 1}")

           current_planner_prompt = ""
           dynamic_agents_description = self.get_agent_capabilities_description()

           if current_attempt == 1: # Only attempt KB query and intent classification on the first attempt
               # --- Step 1: Preliminary KB Query (if KB is available) ---
               if self.knowledge_collection is not None:
                   print("MasterPlanner: Attempting preliminary Knowledge Base query.")
                   # Construct history string for KB query prompt (same as for main prompt)
                   kb_query_history_to_consider = self.conversation_history[:-1] # Exclude current user_prompt
                   kb_query_scored_history = []
                   kb_query_relevant_history_turns = []
                   current_prompt_lower_for_kb_keywords = user_prompt.lower()
                   stopwords_for_kb_keywords = set(["a", "an", "the", "is", "are", "was", "were", "to", "of", "for", "on", "in", "and", "what", "who", "how", "why", "tell", "me", "about"])
                   current_kb_keywords = {word for word in current_prompt_lower_for_kb_keywords.split() if word.isalnum() and word not in stopwords_for_kb_keywords and len(word) > 2}

                   if current_kb_keywords and kb_query_history_to_consider:
                       for i_kb, turn_kb in enumerate(kb_query_history_to_consider):
                           turn_content_lower_kb = str(turn_kb.get('content', '')).lower()
                           score_kb = sum(1 for keyword_kb in current_kb_keywords if keyword_kb in turn_content_lower_kb)
                           if score_kb > 0:
                               kb_query_scored_history.append({"turn": turn_kb, "score": score_kb, "original_index": i_kb})
                       kb_query_scored_history.sort(key=lambda x_kb: (x_kb["score"], x_kb["original_index"]), reverse=True)
                       kb_query_relevant_history_turns = [item_kb["turn"] for item_kb in kb_query_scored_history[:2]]
                   if not kb_query_relevant_history_turns and kb_query_history_to_consider:
                       kb_query_relevant_history_turns = kb_query_history_to_consider[-2:]

                   kb_query_history_for_prompt_list = [f"{t['role'].capitalize()}: {str(t.get('content', ''))}" for t in kb_query_relevant_history_turns]
                   kb_query_history_string_for_prompt = "\n".join(kb_query_history_for_prompt_list)
                   kb_query_context_for_prompt = f"Relevant Conversation History:\n{kb_query_history_string_for_prompt}\n\n" if kb_query_history_string_for_prompt else ""

                   kb_query_generation_prompt = (
                       f"{kb_query_context_for_prompt}"
                       f"Current User Request: '{user_prompt}'\n\n"
                       f"Your task is to determine if retrieving information from a knowledge base would be beneficial for planning a response to the current user request. "
                       f"The knowledge base contains summaries, processed documents, and key facts from previous interactions.\n"
                       f"If retrieving information would be helpful, formulate a concise and effective search query string (max 5-7 words if possible) that can be used to search the knowledge base. "
                       f"The query should aim to find the most relevant context or previously discussed details.\n"
                       f"If you believe no specific information retrieval from the knowledge base is necessary for this particular request, output the exact string: NO_QUERY_NEEDED\n\n"
                       f"Output ONLY the search query string OR the 'NO_QUERY_NEEDED' marker. Do not include any other explanations or text.\n\n"
                       f"Search Query or Marker:"
                   )

                   planner_agent_for_kb_query = next((agent for agent in self.agents if agent.name == "MasterPlanner"), None) # Or a dedicated query agent
                   if planner_agent_for_kb_query:
                       print(f"MasterPlanner: Generating KB search query using {planner_agent_for_kb_query.name}...")
                       kb_query_llm_response = await self.execute_agent(planner_agent_for_kb_query, kb_query_generation_prompt)

                       generated_kb_query = ""
                       if kb_query_llm_response.get("status") == "success":
                           generated_kb_query = kb_query_llm_response.get("response", "").strip()
                           print(f"MasterPlanner: LLM suggested KB query/marker: '{generated_kb_query}'")
                       else:
                           print(f"MasterPlanner: Failed to get KB query from LLM: {kb_query_llm_response.get('response')}")

                       if generated_kb_query and generated_kb_query.upper() != "NO_QUERY_NEEDED":
                           print(f"MasterPlanner: Retrieving knowledge with query: '{generated_kb_query}'")
                           kb_results = await self.retrieve_knowledge(query_text=generated_kb_query, n_results=3)
                           if kb_results.get("status") == "success" and kb_results.get("results"):
                               formatted_kb_entries = []
                               for i_res, res_item in enumerate(kb_results["results"]):
                                   doc_preview = res_item.get('document', 'N/A')[:150] # Preview of content
                                   meta_preview = str(res_item.get('metadata', '{}'))[:100] # Preview of metadata
                                   formatted_kb_entries.append(f"  Result {i_res+1}: \"{doc_preview}...\" (Metadata: {meta_preview}...)")
                               if formatted_kb_entries:
                                   retrieved_kb_context_for_plan = "Previously retrieved relevant information from Knowledge Base:\n" + "\n".join(formatted_kb_entries)
                                   print(f"MasterPlanner: Context built from KB:\n{retrieved_kb_context_for_plan}")
                           else:
                               print(f"MasterPlanner: Knowledge Base retrieval failed or found no results for query '{generated_kb_query}'. Message: {kb_results.get('message')}")
                       elif generated_kb_query.upper() == "NO_QUERY_NEEDED":
                           print("MasterPlanner: LLM indicated no KB query needed.")
                   else:
                       print("MasterPlanner: MasterPlanner agent not found, cannot generate KB query.")
               else:
                   print("MasterPlanner: Knowledge Base not available, skipping preliminary query.")

               # --- Step 2: Intent Classification (existing logic) ---
               classified_intent_result = await self.classify_user_intent(user_prompt)
               if classified_intent_result and classified_intent_result.get("status") == "success":
                   top_intent = classified_intent_result.get("intent")
                   top_score = classified_intent_result.get("all_scores", {}).get(top_intent, 0)
                   if top_score > 0.7: # Confidence threshold
                       first_attempt_intent_info = f"Pre-classified user intent: '{top_intent}' (Confidence: {top_score:.2f}). Please consider this primary intent."

               # Keyword-based history selection for the first attempt's prompt
               current_prompt_lower = user_prompt.lower()
               stopwords = set(["a", "an", "the", "is", "are", "was", "were", "to", "of", "for", "on", "in", "and", "what", "who", "how", "why", "tell", "me", "about"])
               current_keywords = {word for word in current_prompt_lower.split() if word.isalnum() and word not in stopwords and len(word) > 2}

               history_to_consider = self.conversation_history[:-1] # Exclude current prompt
               scored_history = []
               relevant_history_turns = []

               if current_keywords and history_to_consider: # Only score if there are keywords and history
                   for i, turn in enumerate(history_to_consider):
                       turn_content_lower = str(turn.get('content', '')).lower()
                       score = sum(1 for keyword in current_keywords if keyword in turn_content_lower)
                       if score > 0: # Only consider turns with some relevance
                           scored_history.append({"turn": turn, "score": score, "original_index": i})

                   # Sort by score (descending), then by original_index (descending, for recency among same scores)
                   scored_history.sort(key=lambda x: (x["score"], x["original_index"]), reverse=True)

                   # Select top N relevant turns (e.g., top 2)
                   relevant_history_turns = [item["turn"] for item in scored_history[:2]] # Take top 2 relevant

               # If no relevant turns found by keywords (or no keywords/history), take the last 1-2 turns as fallback
               if not relevant_history_turns and history_to_consider:
                   relevant_history_turns = history_to_consider[-2:] # Last two turns or fewer if not enough history

               history_for_prompt_list = []
               for turn in relevant_history_turns: # Format the selected relevant turns
                   history_for_prompt_list.append(f"{turn['role'].capitalize()}: {str(turn.get('content', ''))}")
               history_string_for_prompt = "\n".join(history_for_prompt_list)
               history_context_for_prompt = f"Recent relevant conversation history (for context, most recent of selected is last):\n{history_string_for_prompt}\n\n" if history_string_for_prompt else ""

               # Prepare KB context string for the main prompt
               kb_context_section_for_main_prompt = ""
               if retrieved_kb_context_for_plan: # This was populated in the KB query step
                   kb_context_section_for_main_prompt = f"Consider the following potentially relevant information retrieved from the knowledge base:\n{retrieved_kb_context_for_plan}\n\n"

               current_planner_prompt = (
                   f"You are the MasterPlanner. Your role is to decompose a complex user request into a sequence of tasks for other specialized AI agents.\n\n"
                   f"{history_context_for_prompt}" # Use keyword-selected history
                   f"{kb_context_section_for_main_prompt}" # Add KB context here
                   f"Analyze the following CURRENT user request: '{user_prompt}'\n\n"
                   f"{first_attempt_intent_info}\n\n" # Intent info also from first attempt
                   f"Available specialized agents and their capabilities are:\n{dynamic_agents_description}\n\n"
                   f"Based on the user request, conversation history, and any provided knowledge base information, create a JSON plan as a list of steps. Each step must be a JSON object with the following fields:\n"
                   f"  - 'step_id': (Integer or String) A unique sequential identifier for the step (e.g., 1, 2, '2a').\n"
                   f"  - 'agent_name': (String) The name of the agent to execute this step (must be one of the available agents listed above), OR the special value 'parallel_group'.\n"
                   f"  - 'task_prompt': (String) The specific, self-contained prompt for the target agent. (Not used if agent_name is 'parallel_group').\n"
                   f"  - 'dependencies': (Optional, List[Integer/String]) A list of 'step_id's that this step depends on. Outputs are referenced using {{{{output_variable_name_of_dependency_step}}}}.\n"
                   f"  - 'output_variable_name': (Optional, String) A descriptive variable name for the step's primary output.\n"
                   f"  - 'max_retries': (Integer, Optional, Default: 0) Max retries for this step/group.\n"
                   f"  - 'retry_delay_seconds': (Integer, Optional, Default: 5) Delay before retry.\n"
                   f"  - 'retry_on_statuses': (List[String], Optional, Default: [\"error\"]) Statuses triggering retry.\n\n"
                   f"Parallel Execution Block (using 'parallel_group'):\n"
                   f"To define a set of tasks that can run in parallel, create a step with 'agent_name': 'parallel_group'. This step must contain a 'sub_steps' key, which is a list of standard step objects.\n"
                   f"  - 'sub_steps': (List[Object]) Each object is a normal step definition. Sub-steps are executed concurrently.\n"
                   f"    - CRITICAL: Sub-steps within the same 'parallel_group' must be input-independent of each other. Their 'dependencies' should only refer to steps *outside* the group (i.e., steps the parent group depends on, or earlier sequential steps).\n"
                   f"    - The 'output_variable_name' of the 'parallel_group' step will collect outputs from its sub-steps (keyed by sub-step's 'output_variable_name' or 'step_id').\n\n"
                   f"Example of a sequential step with retry: {{'step_id': 1, 'agent_name': 'WebCrawler', 'task_prompt': 'Fetch primary_data_url', 'output_variable_name': 'primary_data', 'max_retries': 2, 'retry_on_statuses': ['error', 'timeout']}}\n"
                   f"Example of a parallel group: \n"
                   f"    {{'step_id': 2, 'agent_name': 'parallel_group', 'dependencies': [1], 'output_variable_name': 'gathered_info', 'sub_steps': [\n"
                   f"        {{'step_id': '2a', 'agent_name': 'WebCrawler', 'task_prompt': 'Fetch detail_A from {{{{primary_data.urlA}}}}', 'output_variable_name': 'detail_A_content'}},\n"
                   f"        {{'step_id': '2b', 'agent_name': 'DocProcessor', 'task_prompt': 'Summarize text {{{{primary_data.text_field}}}}', 'output_variable_name': 'summary_of_text'}}\n"
                   f"    ]}}\n"
                   f"Example of a subsequent sequential step: {{'step_id': 3, 'agent_name': 'CreativeWriter', 'task_prompt': 'Combine: {{{{gathered_info.detail_A_content}}}} and {{{{gathered_info.summary_of_text}}}}.', 'dependencies': [2]}}\n\n"
                   f"IMPORTANT: Output ONLY the JSON plan as a raw string. Do not include any other text. Ensure sub-steps in a parallel group are truly independent. Use retry parameters judiciously."
               )
           else: # Revision attempt
               if not original_plan_json_str:
                   final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": "Cannot revise plan: original plan not found."}]
                   break

               failed_step_details_json = json.dumps(final_execution_results[-1]) if final_execution_results and final_execution_results[-1].get("status") == "error" else json.dumps({"error": "Unknown failure or no error details from previous plan attempt."})

               # Construct history string again (could be refactored)
               history_for_prompt_list = []
               if self.conversation_history:
                   history_to_consider = self.conversation_history[:-1]
                   for turn in history_to_consider: # Using all available history for revision prompt
                       history_for_prompt_list.append(f"{turn['role'].capitalize()}: {turn['content']}")
               history_string_for_prompt = "\n".join(history_for_prompt_list)
               history_context_for_prompt = f"Full conversation history (for context, most recent is last):\n{history_string_for_prompt}\n\n" if history_string_for_prompt else ""

               current_planner_prompt = (
                   f"You are the MasterPlanner. Your previous attempt to create a plan for the user request resulted in an error or an unexecutable plan. Review the original plan, the user's request, the failed step details, and provide a revised JSON plan.
"
                   f"{history_context_for_prompt}"
                   f"Original User Request: '{user_prompt}'\n\n"
                   f"{first_attempt_intent_info}\n\n" # Use intent from first attempt
                   f"Original Plan (that failed or was problematic):\n{original_plan_json_str}\n\n"
                   f"Details of the failed step from the last execution attempt:\n{failed_step_details_json}\n\n"
                   f"Available specialized agents and their capabilities are:\n{dynamic_agents_description}\n\n"
                   f"Based on all this information, provide a revised JSON plan. Ensure the new plan addresses the previous failure. "
                   f"Use the same JSON structure as before (list of steps with 'step_id', 'agent_name', 'task_prompt', 'dependencies', 'output_variable_name').
"
                   f"IMPORTANT: Output ONLY the revised JSON plan as a raw string. Do not include any other text. If you determine the original plan cannot be improved or the task is unplannable with the given information and agents, return an empty JSON list []."
               )

           master_planner_agent = next((agent for agent in self.agents if agent.name == "MasterPlanner"), None)
           if not master_planner_agent:
               final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": "MasterPlanner agent not found or configured."}]
               break

           print(f"Prompting MasterPlanner (Attempt {current_attempt})...")
           plan_generation_result = await self.execute_agent(master_planner_agent, current_planner_prompt)

           if plan_generation_result.get("status") != "success":
               final_execution_results = [plan_generation_result] # Store this attempt's error
               break # If planner fails, no more revisions

           current_plan_json_str = plan_generation_result.get("response", "").strip()
           if current_attempt == 1:
               original_plan_json_str = current_plan_json_str # Store the first plan

           if not current_plan_json_str:
               final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": f"MasterPlanner returned empty plan string on attempt {current_attempt}"}]
               if current_attempt >= max_revision_attempts: break
               else: continue # Try revision if allowed

           try:
               plan_list = json.loads(current_plan_json_str)
               if not isinstance(plan_list, list):
                   if isinstance(plan_list, dict) and 'plan' in plan_list and isinstance(plan_list['plan'], list):
                       plan_list = plan_list['plan']
                   else:
                       raise ValueError("Plan is not a JSON list.")
           except json.JSONDecodeError as e:
               print(f"JSONDecodeError parsing plan on attempt {current_attempt}: {e}. Raw response: {current_plan_json_str}")
               final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": f"Failed to parse plan (JSONDecodeError) on attempt {current_attempt}: {e}. Response: {current_plan_json_str[:200]}"}]
               if current_attempt >= max_revision_attempts: break
               else: continue # Try revision
           except ValueError as e:
               print(f"ValueError parsing plan on attempt {current_attempt}: {e}. Raw response: {current_plan_json_str}")
               final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": f"Plan format error on attempt {current_attempt}: {e}. Response: {current_plan_json_str[:200]}"}]
               if current_attempt >= max_revision_attempts: break
               else: continue # Try revision

           if not plan_list:
               print(f"MasterPlanner returned an empty plan list on attempt {current_attempt}.")
               # If it's the first attempt, it's info. If it's a revision that results in empty, it might mean it gave up.
               final_execution_results = [{"status": "info", "agent": "MasterPlanner", "response": "MasterPlanner returned an empty plan list (may indicate task is unplannable or too simple for multi-step)."}] if current_attempt == 1 else final_execution_results
               break

           step_outputs = {}
           current_attempt_results = []
           plan_succeeded_this_attempt = True
           print(f"Executing plan with {len(plan_list)} steps (Attempt {current_attempt})...")

           # Validate plan structure before execution
           is_valid_plan = True
           for i, step_def in enumerate(plan_list):
               if not isinstance(step_def, dict):
                   final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": f"Plan validation failed: Step {i+1} is not a dictionary. Plan: {current_plan_json_str[:500]}"}]
                   is_valid_plan = False; break
               required_keys = {"step_id": int, "agent_name": str, "task_prompt": str}
               for key, expected_type in required_keys.items():
                   if key not in step_def:
                       final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": f"Plan validation failed: Step {step_def.get('step_id', i+1)} is missing required key '{key}'. Plan: {current_plan_json_str[:500]}"}]
                       is_valid_plan = False; break
                   if not isinstance(step_def[key], expected_type):
                       final_execution_results = [{"status": "error", "agent": "MasterPlanner", "response": f"Plan validation failed: Step {step_def.get('step_id', i+1)} key '{key}' has incorrect type (expected {expected_type.__name__}, got {type(step_def[key]).__name__}). Plan: {current_plan_json_str[:500]}"}]
                       is_valid_plan = False; break
               if not is_valid_plan: break

           if not is_valid_plan:
               if current_attempt >= max_revision_attempts: break # Break while loop if validation fails on last attempt
               else: continue # Try revision if validation fails and more attempts are allowed

           # Enhanced Plan Step Schema (for documentation purposes within the code):
           # Each step in the 'plan_list' is a dictionary. Beyond the validated keys
           # ('step_id', 'agent_name', 'task_prompt'), it can optionally include:
           #   - 'dependencies': (List[Integer/String]) IDs of steps this one depends on.
           #   - 'output_variable_name': (String) Variable name for this step's output.
           #   - 'max_retries': (Integer, Default: 0) Max retry attempts for this step.
           #   - 'retry_delay_seconds': (Integer, Default: 5) Delay before a retry.
           #   - 'retry_on_statuses': (List[String], Default: ["error"]) Statuses that trigger retry.
           # For 'parallel_group' agent_name:
           #   - 'sub_steps': (List[Object]) List of standard step definitions.

           # Proceed with execution if plan is valid
           for step_definition in sorted(plan_list, key=lambda x: x.get('step_id', 0)):

               # This is the main loop for sequential steps or parallel groups
               # Retry logic for the *overall step/group* will be handled here.

               group_max_retries = step_definition.get("max_retries", 0)
               group_retry_delay = step_definition.get("retry_delay_seconds", 5)
               group_retry_statuses = step_definition.get("retry_on_statuses", ["error"])
               group_current_retries = 0

               while True: # Loop for group-level retries
                   step_id = step_definition.get("step_id")
                   agent_name = step_definition.get("agent_name")

                   group_log_prompt = f"Processing step/group {step_id} (Agent: {agent_name})"
                   if group_current_retries > 0:
                       group_log_prompt += f" (Overall Retry {group_current_retries}/{group_max_retries})"
                   print(group_log_prompt)

                   group_step_result = None # Result for the whole group or single step

                   if agent_name == "parallel_group":
                       sub_steps = step_definition.get("sub_steps", [])
                       if not sub_steps:
                           print(f"Warning: Parallel group {step_id} has no sub_steps. Skipping.")
                           group_step_result = {"status": "success", "response": "No sub-steps to execute.", "sub_step_results": []}
                           # Or treat as error? For now, success with empty results.
                       else:
                           # Validate sub_steps (basic validation)
                           valid_sub_steps = True
                           for i, sub_step_def in enumerate(sub_steps):
                               if not (isinstance(sub_step_def, dict) and
                                       sub_step_def.get("step_id") and
                                       sub_step_def.get("agent_name") and
                                       sub_step_def.get("task_prompt")):
                                   group_step_result = {"status": "error", "agent": agent_name, "step_id": step_id,
                                                        "response": f"Invalid sub_step at index {i} in group {step_id}."}
                                   valid_sub_steps = False; break
                           if not valid_sub_steps:
                               current_attempt_results.append(group_step_result)
                               plan_succeeded_this_attempt = False; break # Break from group retry loop

                           sub_step_tasks = []
                           for sub_step_def in sub_steps:
                               # Pass the main plan_list for dependency resolution from outside the group
                               # Pass current step_outputs for resolving dependencies
                               sub_step_tasks.append(self._execute_single_plan_step(sub_step_def, plan_list, step_outputs))

                           # Execute sub-steps concurrently
                           print(f"Executing {len(sub_step_tasks)} sub-steps in parallel for group {step_id}...")
                           gathered_results = await asyncio.gather(*sub_step_tasks, return_exceptions=True)

                           # Process results of parallel execution
                           all_sub_steps_succeeded = True
                           aggregated_outputs = {}
                           processed_gathered_results = []

                           for i, res_or_exc in enumerate(gathered_results):
                               sub_step_id_for_log = sub_steps[i].get('step_id', f"sub_step_{i}")
                               if isinstance(res_or_exc, Exception):
                                   print(f"Sub-step {sub_step_id_for_log} in group {step_id} raised an exception: {res_or_exc}")
                                   all_sub_steps_succeeded = False
                                   # Store the exception as a result
                                   processed_gathered_results.append({
                                       "status": "error", "step_id": sub_step_id_for_log,
                                       "agent": sub_steps[i].get('agent_name'),
                                       "response": f"Exception: {str(res_or_exc)}"
                                   })
                                   # No break here, let all parallel tasks finish or fail
                               elif res_or_exc.get("status") != "success":
                                   print(f"Sub-step {sub_step_id_for_log} in group {step_id} failed with status: {res_or_exc.get('status')}")
                                   all_sub_steps_succeeded = False
                                   processed_gathered_results.append(res_or_exc)
                                   # No break here
                               else: # Sub-step succeeded
                                   processed_gathered_results.append(res_or_exc)
                                   sub_output_var = sub_steps[i].get("output_variable_name", sub_step_id_for_log)
                                   aggregated_outputs[sub_output_var] = res_or_exc.get("response")
                                   # Also store rich media from sub-steps if any, prefixing with sub_output_var
                                   for media_key in ["image_path", "frame_path", "gif_path", "speech_path", "modified_file"]:
                                       if media_key in res_or_exc:
                                            aggregated_outputs[f"{sub_output_var}_{media_key}"] = res_or_exc[media_key]

                           if all_sub_steps_succeeded:
                               group_output_var = step_definition.get("output_variable_name", f"step_{step_id}_output")
                               step_outputs[group_output_var] = aggregated_outputs
                               group_step_result = {"status": "success", "step_id": step_id, "agent": agent_name,
                                                    "response": "All sub-steps completed successfully.",
                                                    "sub_step_results": processed_gathered_results,
                                                    "aggregated_outputs": aggregated_outputs}
                           else:
                               group_step_result = {"status": "error", "step_id": step_id, "agent": agent_name,
                                                    "response": "One or more sub-steps failed in parallel group.",
                                                    "sub_step_results": processed_gathered_results}
                   else: # Regular sequential step
                       # Call the refactored single step execution logic
                       # step_outputs is passed to be modified by _execute_single_plan_step on success
                       group_step_result = await self._execute_single_plan_step(step_definition, plan_list, step_outputs)

                   # --- End of processing for this attempt of step/group ---

                   if group_step_result.get("status") == "success":
                       current_attempt_results.append(group_step_result)
                       # If it was a single step, its output would have been added to step_outputs by _execute_single_plan_step
                       # If it was a parallel group, its aggregated_outputs were added to step_outputs if successful
                       break # Break from group-level retry loop
                   else: # Group/step failed
                       group_current_retries += 1
                       if group_current_retries <= group_max_retries and group_step_result.get("status") in group_retry_statuses:
                           print(f"Step/Group {step_id} failed with status '{group_step_result.get('status')}'. Retrying overall group/step in {group_retry_delay}s... ({group_current_retries}/{group_max_retries})")
                           await asyncio.sleep(group_retry_delay)
                           # Loop continues for group retry
                       else:
                           print(f"Step/Group {step_id} (Agent: {agent_name}) failed permanently for this plan attempt.")
                           current_attempt_results.append(group_step_result) # Add final failure of group/step
                           plan_succeeded_this_attempt = False
                           break # Break from group-level retry loop

               if not plan_succeeded_this_attempt: # If a step/group failed permanently, break the outer plan execution loop
                   break

           final_execution_results = current_attempt_results
           if plan_succeeded_this_attempt:
               print(f"Plan attempt {current_attempt} succeeded.")
               break # Exit while loop
           # If loop finished due to plan failure (plan_succeeded_this_attempt is False)
           # and there are more overall plan revision attempts, the outer 'while current_attempt <= max_revision_attempts' will continue.
           # If revisions are exhausted, it will also break from the outer while.

       # Summarization and history update for the assistant's response
       summarizer_agent = next((agent for agent in self.agents if agent.name == "CreativeWriter"), None)
       if not summarizer_agent: summarizer_agent = next((agent for agent in self.agents if agent.name == "DeepThink"), None)

       if summarizer_agent:
           plan_execution_summary_for_llm = f"Original User Request: '{user_prompt}'\nPlan Execution Summary (Attempt {current_attempt}):\n"
           if final_execution_results and isinstance(final_execution_results, list):
               for i, res in enumerate(final_execution_results):
                   agent_name = res.get('agent', 'N/A')
                   status = res.get('status', 'unknown')
                   response_snippet = str(res.get('response', 'No response'))[:100]
                   plan_execution_summary_for_llm += f"  Step {i+1} (Agent: {agent_name}): Status='{status}', Output Snippet='{response_snippet}'\n"
               if not final_execution_results:
                    plan_execution_summary_for_llm += "  No steps were executed or the plan was empty.\n"
           else: # Should be a list, but if it's an error dict from planner directly
                master_planner_error_response = final_execution_results[0].get("response") if isinstance(final_execution_results, list) and final_execution_results and isinstance(final_execution_results[0], dict) else "MasterPlanner did not produce a valid plan or an error occurred."
                plan_execution_summary_for_llm += f"  MasterPlanner phase outcome: {master_planner_error_response}\n"

           summarizer_prompt = (
               f"You are an AI assistant. Based on the user's original request and a summary of the multi-step plan execution that followed, "
               f"provide a concise, natural language summary of what actions were taken by the system and the overall outcome. "
               f"Focus on what would be most useful for the user to understand what just happened. "
               f"Do not refer to yourself as '{summarizer_agent.name}', just act as the main AI assistant.\n\n"
               f"{plan_execution_summary_for_llm}\n\n"
               f"Please provide only the summary text, suitable for conversation history."
           )
           print(f"Requesting summary from {summarizer_agent.name} for plan execution...")
           summary_result = await self.execute_agent(summarizer_agent, summarizer_prompt)

           if summary_result.get("status") == "success" and summary_result.get("response", "").strip():
               assistant_response_summary = summary_result.get("response").strip()
               print(f"LLM Summary for history: {assistant_response_summary}")
           else:
               print(f"LLM summarization failed or returned empty. Using static summary. Error: {summary_result.get('response')}")
               num_steps = len(final_execution_results) if isinstance(final_execution_results, list) else 0
               successful_steps = sum(1 for res in final_execution_results if isinstance(res, dict) and res.get("status") == "success") if isinstance(final_execution_results, list) else 0
               assistant_response_summary = f"Plan executed: {successful_steps}/{num_steps} steps successful."
       else:
           num_steps = len(final_execution_results) if isinstance(final_execution_results, list) else 0
           successful_steps = sum(1 for res in final_execution_results if isinstance(res, dict) and res.get("status") == "success") if isinstance(final_execution_results, list) else 0
           assistant_response_summary = f"Plan executed: {successful_steps}/{num_steps} steps successful (Summarizer agent unavailable)."

       self.conversation_history.append({"role": "assistant", "content": assistant_response_summary})
       if len(self.conversation_history) > self.max_history_items:
           self.conversation_history = self.conversation_history[-self.max_history_items:]

       return final_execution_results

class DocumentUniverse:
   def __init__(self):
       self.processors={"pdf":self.pdf_proc,"docx":self.docx_proc,"xlsx":self.xlsx_proc,"html":self.html_proc,"json":self.json_proc,"csv":self.csv_proc,"txt":self.txt_proc}
   def pdf_proc(self,file):import fitz;return fitz.open(file).get_text()
   def docx_proc(self,file):import docx;return '\n'.join([p.text for p in docx.Document(file).paragraphs])
   def xlsx_proc(self,file):import openpyxl;return str(list(openpyxl.load_workbook(file).active.values))
   def html_proc(self,file):from bs4 import BeautifulSoup;return BeautifulSoup(open(file),'html.parser').get_text()
   def json_proc(self,file):return json.load(open(file))
   def csv_proc(self,file):import csv;return list(csv.reader(open(file)))
   def txt_proc(self,file):return open(file).read()
   def process_file(self,file_path):
       # This method expects a file path string or a file-like object for Streamlit's UploadedFile
       # For now, assuming file_path is an UploadedFile object from Streamlit, which has a 'name' attribute
       # and can be passed directly to functions expecting a file-like object.
       # If it's a path string, it needs to be opened.
       # This might need adjustment based on how Streamlit passes the file.
       # Assuming the UploadedFile object itself can be passed to the processors if they handle file-like objects.
       # For paths, they'd need open(file_path, 'rb') or similar.
       # The current implementation seems to mix this. For simplicity, let's assume it receives a path for now.
       # This part needs careful review if Streamlit UploadedFile objects are passed.
       # For now, the logic is kept as is from the original script.
       ext=Path(file_path.name).suffix.lower()[1:] if hasattr(file_path, 'name') else Path(file_path).suffix.lower()[1:]
       processor=self.processors.get(ext)
       return processor(file_path) if processor else "Unsupported format"


class WebIntelligence:
   def __init__(self):
       self.session=requests.Session()
       self.session.headers.update({"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
   def search_web(self,query):
       from duckduckgo_search import DDGS
       return [{"title":r["title"],"url":r["href"],"snippet":r["body"]} for r in DDGS().text(query,max_results=10)]
   def scrape_page(self,url):
       try:
           from bs4 import BeautifulSoup
           resp=self.session.get(url,timeout=10)
           return BeautifulSoup(resp.content,'html.parser').get_text()[:5000]
       except:return "Scraping failed"

orchestrator=TerminusOrchestrator()
doc_processor=DocumentUniverse()
web_intel=WebIntelligence()
EOF
}

# Function for generating terminus_ui.py
create_terminus_ui_script() {
    progress "CREATING TERMINUS UI SCRIPT"
    cat>"$INSTALL_DIR/terminus_ui.py"<<'EOF'
import streamlit as st,asyncio,json,os,subprocess,time,datetime # Ensure time and datetime are imported
from pathlib import Path # Path is used for temp_video_path
import pandas as pd,plotly.express as px,plotly.graph_objects as go
from agents.master_orchestrator import orchestrator,doc_processor,web_intel

st.set_page_config(page_title="TERMINUS AI",layout="wide",initial_sidebar_state="expanded")

def main():
   st.markdown("""<div style='text-align:center;background:linear-gradient(90deg,#FF6B6B,#4ECDC4,#45B7D1,#96CEB4);padding:20px;border-radius:10px;margin-bottom:20px'>
   <h1 style='color:white;text-shadow:2px 2px 4px rgba(0,0,0,0.5)'>TERMINUS AI NEXUS</h1>
   <p style='color:white;font-size:18px'>ULTIMATE LOCAL AI ECOSYSTEM | 25+ MODELS | UNLIMITED POWER</p></div>""",unsafe_allow_html=True)

   # Sidebar Controls
   with st.sidebar:
       st.header("COMMAND CENTER")
       operation_mode=st.selectbox("Operation Mode",["Multi-Agent Chat","Document Processing","Web Intelligence","Image Generation", "Video Processing", "Audio Processing", "Code Generation","Data Analysis","Creative Suite", "Knowledge Base Explorer"])

       st.subheader("Agent Selection")
       agent_names=[a.name for a in orchestrator.agents]
       selected_agents=st.multiselect("Active Agents",agent_names,default=agent_names[:6])

       st.subheader("Parameters")
       temperature=st.slider("Temperature",0.0,2.0,0.7)
       max_tokens=st.slider("Max Tokens",500,8000,2000)
       # parallel_mode and consensus_mode are specific to Multi-Agent Chat, moved there or implicitly handled.

   # Main Interface
   if operation_mode=="Multi-Agent Chat":
       col1,col2=st.columns([2,1])
       with col1:
           st.subheader("UNIVERSAL AI COMMAND")
           user_prompt=st.text_area("Enter your command:",height=200,placeholder="Ask anything - the entire AI constellation will respond...")
           use_master_planner = st.checkbox(" Use MasterPlanner for complex requests (experimental)", key="use_master_planner_checkbox")

           if st.button("EXECUTE ALL AGENTS",type="primary"):
               if user_prompt:
                   if use_master_planner:
                       with st.spinner("MasterPlanner is thinking and coordinating... This may take a while."):
                           planned_results = asyncio.run(orchestrator.execute_master_plan(user_prompt))
                       st.subheader("MASTER PLAN EXECUTION RESULTS:")
                       if not planned_results:
                           st.info("The MasterPlanner did not generate any steps or the plan execution yielded no results.")
                       for i, step_result in enumerate(planned_results):
                           st.markdown(f"---
**Step {i+1} Output (Agent: {step_result.get('agent', 'N/A')})**")
                           status_icon = "" if step_result.get("status") == "success" else "" if step_result.get("status") == "info" else ""

                           if "response" in step_result: # Check if 'response' key exists
                               # Ensure expander is created even if response is None or empty, to show other details
                               expanded_default = True if step_result.get("response") else False # Expand if there's a text response
                               with st.expander(f"{status_icon} Agent: {step_result.get('agent', 'N/A')} ({step_result.get('model', 'N/A')}) - Step {i+1}", expanded=expanded_default):
                                   st.markdown(str(step_result.get("response", ""))) # Convert None to empty string for markdown

                           # Display rich media outputs if available
                           if step_result.get("image_path"):
                               st.image(step_result["image_path"], caption=f"Image from Step {i+1} ({step_result.get('agent')})")
                           if step_result.get("frame_path"):
                               st.image(step_result["frame_path"], caption=f"Video Frame from Step {i+1} ({step_result.get('agent')})")
                           if step_result.get("gif_path"):
                               st.image(step_result["gif_path"], caption=f"GIF from Step {i+1} ({step_result.get('agent')})")
                               st.markdown(f"Path to GIF: `{step_result['gif_path']}`")
                           if step_result.get("speech_path"):
                               st.audio(step_result["speech_path"]) # Auto-detect format
                               st.markdown(f"Path to audio: `{step_result['speech_path']}`")
                           if step_result.get("modified_file"):
                               st.info(f"File modified by {step_result.get('agent')}: {step_result.get('modified_file')}")

                           # Raw JSON for debugging
                           with st.expander(f"Raw JSON for Step {i+1} result", expanded=False):
                               st.json(step_result)
                   else:
                       with st.spinner("Processing across AI constellation..."):
                           current_context = {"current_mode": operation_mode}
                           results=asyncio.run(orchestrator.parallel_execution(prompt=user_prompt, selected_agents=selected_agents, context=current_context))

                           if consensus_mode and results: # ensure results is not empty
                               # Filter out potential error or info messages from MasterPlanner if it was incorrectly part of results
                               valid_agent_responses = [r for r in results if r.get("agent") != "MasterPlanner" or r.get("status") == "success"]
                               if valid_agent_responses:
                                   consensus = orchestrator.consensus_analysis(valid_agent_responses)
                                   st.success(f"Consensus Score: {consensus['consensus_score']:.2%}")
                               else:
                                   st.info("No valid agent responses for consensus analysis.")


                           st.subheader("AGENT RESPONSES")
                           if not results:
                               st.info("No results from agents.")
                           for result in results:
                               status_icon="" if result.get("status")=="success" else "" if result.get("status") == "info" else ""
                               # Handle cases where MasterPlanner might have returned an info/error message if not using the planner path
                               if result.get("agent") == "MasterPlanner" and result.get("status") != "success":
                                   st.info(f"MasterPlanner Info: {result.get('response')}")
                                   continue
                               # Default expander for regular agent responses
                               expanded_default_regular = True if result.get("response") else False
                               with st.expander(f"{status_icon} {result.get('agent', 'N/A')} ({result.get('model', 'N/A')})", expanded=expanded_default_regular):
                                   st.write(str(result.get("response",""))) # Ensure response is a string
               else:
                   st.warning("Please enter a prompt.")

           # Display conversation history after results
           st.markdown("---")
           if hasattr(orchestrator, 'get_conversation_history_for_display'):
               history_to_display = orchestrator.get_conversation_history_for_display()
               if history_to_display:
                   with st.expander(" Conversation History (Recent Turns)", expanded=False):
                       for turn in reversed(history_to_display):
                           role = turn.get('role', 'unknown').capitalize()
                           content = str(turn.get('content', '')) # Ensure content is string

                           # Create a more unique key using a combination of role, timestamp, and content snippet
                           timestamp_for_key = int(time.time() * 1000) # milliseconds for more uniqueness
                           key_content_snippet = "".join(filter(str.isalnum, content[:20])) # alphanumeric snippet
                           unique_key = f"history_{role.lower()}_{timestamp_for_key}_{key_content_snippet}"

                           if role == "User":
                               st.markdown(f"** User:**")
                               st.text_area("", value=content, height=100, disabled=True, key=unique_key)
                           elif role == "Assistant":
                               st.markdown(f"** Assistant (Summary/Response):**") # Clarified role
                               st.text_area("", value=content, height=100, disabled=True, key=unique_key)
                           else: # Should not happen with current history structure but good for robustness
                               st.markdown(f"**{role}:**")
                               st.text_area("", value=content, height=100, disabled=True, key=unique_key)
                           st.markdown("---")
               # else: # This case means no history items, not that method is missing.
               #    st.caption("No history yet for this session.") # Covered by if history_to_display
           # else: # This means the method itself is missing.
           #    st.caption("Conversation history feature not available in orchestrator.")


       with col2:
           st.subheader("SYSTEM STATUS")
           st.metric("Total Agents",len(orchestrator.agents))
           st.metric("Active Agents",len(selected_agents))
           st.metric("Total Models",len(set(a.model for a in orchestrator.agents)))

           # Agent Status
           st.subheader("AGENT STATUS")
           agent_df=pd.DataFrame([{"Agent":a.name,"Model":a.model,"Status":"Active" if a.active else "Inactive"} for a in orchestrator.agents])
           st.dataframe(agent_df,use_container_width=True)

       # System Information moved to its own operation_mode block below
       # st.subheader("System Information")
       # if st.button("Get Disk Space", key="sysinfo_disk_space"):
       # ... existing code ...
       # if st.button("Get Memory Usage", key="sysinfo_mem_usage"):
       # ... existing code ...
       # top_n_processes = st.number_input("Number of processes:", min_value=1, max_value=50, value=10, key="sysinfo_top_n")
       # if st.button("Get Top Processes", key="sysinfo_top_procs"):
       # ... existing code ...


   elif operation_mode=="Document Processing":
       st.subheader("UNIVERSAL DOCUMENT PROCESSOR")
       uploaded_files=st.file_uploader("Upload documents",accept_multiple_files=True,type=['pdf','docx','xlsx','txt','csv','json','html'])

       if uploaded_files:
           for file in uploaded_files:
               with st.expander(f"{file.name}"):
                   content=doc_processor.process_file(file)
                   st.text_area("Content Preview",content[:1000]+"..." if len(content)>1000 else content,height=200)

                   if st.button(f"Analyze with AI",key=f"analyze_{file.name}"):
                       with st.spinner("Storing document excerpt and analyzing..."):
                           # 1. Store an excerpt in the knowledge base
                           excerpt_to_store = content[:2000] # Using the same length as analysis prompt for consistency
                           doc_metadata = {
                               "source": "document_upload",
                               "filename": file.name,
                               "filetype": file.type if file.type else "unknown",
                               "processed_timestamp": datetime.datetime.now().isoformat()
                           }
                           # Generate a unique ID for this document content to avoid collision if re-uploaded/re-analyzed
                           # A simple approach: combine filename and a timestamp of processing.
                           # More robust: hash the content, but that's more involved for this step.
                           # For now, let uuid handle uniqueness if not specified.
                           # content_id_for_kb = f"doc_{file.name}_{int(time.time())}"

                           kb_store_result = asyncio.run(orchestrator.store_knowledge(
                               content=excerpt_to_store,
                               metadata=doc_metadata
                               # content_id=content_id_for_kb # Let store_knowledge generate ID
                           ))
                           if kb_store_result.get("status") == "success":
                               st.success(f"Document excerpt stored in knowledge base (ID: {kb_store_result.get('id')}).")
                           else:
                               st.error(f"Failed to store document excerpt in knowledge base: {kb_store_result.get('message')}")

                           # 2. Proceed with AI analysis (existing logic)
                           prompt=f"Analyze this document content: {content[:2000]}" # content is already defined
                           results=asyncio.run(orchestrator.parallel_execution(prompt,selected_agents[:3])) # Use selected_agents from sidebar
                           for result in results:
                               st.info(f"**{result['agent']}**: {result['response'][:500]}...")

   elif operation_mode=="Web Intelligence":
       st.subheader("WEB INTELLIGENCE NEXUS")
       search_query=st.text_input("Search Query:")

       if st.button("SEARCH & ANALYZE"):
           if search_query:
               with st.spinner("Searching and analyzing..."):
                   search_results=web_intel.search_web(search_query)
                   st.json(search_results[:3])

                   analysis_prompt=f"Analyze these search results: {json.dumps(search_results[:3])}"
                   results=asyncio.run(orchestrator.parallel_execution(analysis_prompt,selected_agents[:3]))

                   for result in results:
                       with st.expander(f"{result['agent']} Analysis"):
                           st.write(result["response"])

   elif operation_mode == "Image Generation":
       st.subheader("IMAGE GENERATION STUDIO")
       image_prompt = st.text_area("Enter your image description:", height=100, placeholder="E.g., 'A photorealistic cat astronaut on the moon'")
       if st.button("Generate Image", type="primary"):
           if image_prompt:
               with st.spinner("Generating image... This may take a while, especially on first run or CPU."):
                   # Ensure 'ImageForge' is available and selected
                   imageforge_agent = next((agent for agent in orchestrator.agents if agent.name == "ImageForge"), None)
                   if imageforge_agent and imageforge_agent.active:
                       results = asyncio.run(orchestrator.parallel_execution(prompt=image_prompt, selected_agents=["ImageForge"]))
                       if results and isinstance(results, list) and len(results) > 0:
                           generation_result = results[0] # Expect one result for ImageForge
                           if generation_result.get("status") == "success" and generation_result.get("image_path"):
                               st.image(generation_result["image_path"], caption=f"Generated image for: {image_prompt}")
                               st.success(f"Image successfully generated and saved to: {generation_result['image_path']}")
                           else:
                               st.error(f"Image generation failed: {generation_result.get('response', 'Unknown error')}")
                       else:
                           st.error("Image generation failed to produce a result.")
                   else:
                       st.error("ImageForge agent not found or is not active. Please check agent configuration.")
           else:
               st.warning("Please enter an image description.")

   elif operation_mode == "Video Processing":
       st.subheader(" VIDEO PROCESSING UTILITIES")
       uploaded_video = st.file_uploader("Upload a video file", type=['mp4', 'mov', 'avi', 'mkv'])

       if uploaded_video is not None:
           temp_video_dir = Path(__file__).parent / "temp_uploads"
           temp_video_dir.mkdir(parents=True, exist_ok=True)
           temp_video_path = temp_video_dir / uploaded_video.name
           with open(temp_video_path, "wb") as f:
               f.write(uploaded_video.getbuffer())

           st.video(str(temp_video_path))
           st.markdown("---")

           video_task = st.selectbox("Select Video Task:", ["Get Video Info", "Extract Frame", "Convert to GIF"])

           if video_task == "Get Video Info":
               if st.button("Get Information"):
                   with st.spinner("Fetching video information..."):
                       result = asyncio.run(orchestrator.get_video_metadata(str(temp_video_path)))
                       if result and result.get("status") == "success":
                           st.success(result.get("message"))
                           st.json(result.get("metadata"))
                       else:
                           st.error(result.get("message", "Failed to get video info."))

           elif video_task == "Extract Frame":
               timestamp = st.text_input("Enter Timestamp (e.g., 00:01:30 or 90 for seconds):", "00:00:05")
               if st.button("Extract Frame"):
                   with st.spinner(f"Extracting frame at {timestamp}..."):
                       result = asyncio.run(orchestrator.extract_video_frame(str(temp_video_path), timestamp))
                       if result and result.get("status") == "success":
                           st.success(result.get("message"))
                           st.image(result.get("frame_path"), caption=f"Frame at {timestamp}")
                       else:
                           st.error(result.get("message", "Failed to extract frame."))

           elif video_task == "Convert to GIF":
               col1, col2 = st.columns(2)
               start_time = col1.text_input("Start Time (e.g., 00:00:00 or 0):", "0")
               end_time = col2.text_input("End Time (e.g., 00:00:05 or 5):", "5")
               gif_fps = st.slider("GIF FPS:", 5, 30, 10)
               gif_scale = st.slider("Resolution Scale:", 0.1, 1.0, 0.5, 0.1)
               if st.button("Convert to GIF"):
                   with st.spinner("Converting to GIF... This may take time."):
                       result = asyncio.run(orchestrator.convert_video_to_gif(str(temp_video_path), start_time, end_time, resolution_scale=gif_scale, fps=gif_fps))
                       if result and result.get("status") == "success":
                           st.success(result.get("message"))
                           st.markdown(f"Download your GIF: `{result.get('gif_path')}` (Note: Direct display of local GIFs can be tricky in Streamlit, path provided).")
                       else:
                           st.error(result.get("message", "Failed to convert to GIF."))
           # Consider cleaning up temp_video_path

   elif operation_mode == "Code Generation":
       st.subheader("PROJECT SCAFFOLDING & CODE GENERATION")

       st.markdown("###  Scaffold New Project")

       project_name = st.text_input("Enter Project Name:", placeholder="e.g., my_new_cli_app", key="scaffold_project_name_v2")

       project_type_options = {
           "Python CLI (Typer)": "python_cli",
           "Python FastAPI Backend": "python_fastapi",
           "Python Streamlit Dashboard": "python_streamlit",
           "Basic Python Project": "python_basic"
       }
       display_project_type = st.selectbox(
           "Select Project Type:",
           options=list(project_type_options.keys()),
           key="scaffold_project_type_v2"
       )
       selected_project_type_value = project_type_options[display_project_type]

       if st.button("Scaffold Project", type="primary", key="scaffold_button_v2"):
           if project_name and selected_project_type_value:
               with st.spinner(f"Scaffolding '{project_name}' as {display_project_type}..."):
                   result = asyncio.run(orchestrator.scaffold_new_project(
                       project_name=project_name,
                       project_type=selected_project_type_value
                   ))

                   if result and result.get("status") == "success":
                       st.success(result.get("message", "Project scaffolded successfully!"))
                       # Potentially show the project path if available in message
                   else:
                       st.error(result.get("message", "Failed to scaffold project."))
           else:
               st.warning("Please enter a project name and select a project type.")

       st.markdown("---") # Separator from project scaffolding
       st.markdown("###  AI-Assisted Code Modification (Experimental)")

       st.warning(
           "**Experimental Feature:** AI code modification can be unpredictable. "
           "Always review changes carefully. Backups of original files (with a .bak_timestamp suffix) "
           "are created in the same directory."
       )

       mod_project_name = st.text_input(
           "Project Name (must exist in Terminus AI root):",
           placeholder="e.g., my_fastapi_app"
       )
       mod_relative_file_path = st.text_input(
           "File Path within Project (e.g., main.py or app/main.py):",
           placeholder="e.g., main.py"
       )
       mod_instruction = st.text_area(
           "Modification Instruction (be specific):",
           height=150,
           placeholder="e.g., Add a new FastAPI GET endpoint to '/status' that returns {'status': 'ok'}"
       )

       if st.button("Attempt Code Modification", key="attempt_code_mod_button"):
           if mod_project_name and mod_relative_file_path and mod_instruction:
               with st.spinner(f"Attempting to modify '{mod_relative_file_path}' in project '{mod_project_name}'..."):
                   result = asyncio.run(orchestrator.modify_code_in_project(
                       project_name=mod_project_name,
                       relative_file_path=mod_relative_file_path,
                       modification_instruction=mod_instruction
                   ))

                   if result and result.get("status") == "success":
                       st.success(result.get("message", "Code modification attempted successfully!"))
                       if result.get("modified_file"):
                           st.info(f"Modified file: {result.get('modified_file')}")
                           # Optionally, try to read and display the modified code if it's not too large
                           # try:
                           #     with open(result.get("modified_file"), "r", encoding="utf-8") as f:
                           #         modified_code_content = f.read()
                           #     with st.expander("View Modified Code", expanded=False):
                           #         st.code(modified_code_content, language="python")
                           # except Exception as e_read:
                           #     st.warning(f"Could not display modified code: {e_read}")
                   else:
                       st.error(result.get("message", "Code modification failed."))
           else:
               st.warning("Please provide Project Name, File Path, and Modification Instruction.")

       st.markdown("---")
       st.markdown("###  Explain Code Snippet")
       explain_code_input = st.text_area("Enter code snippet to explain:", height=200, key="explain_code_input_v2") # v2 key
       explain_lang_input = st.text_input("Language (e.g., python, javascript):", value="python", key="explain_lang_input_v2") # v2 key

       if st.button("Explain Code", key="explain_code_button_v2"): # v2 key
           if explain_code_input and explain_lang_input:
               with st.spinner(f"Thinking... explaining snippet in {explain_lang_input}"):
                   result = asyncio.run(orchestrator.explain_code_snippet(explain_code_input, explain_lang_input))
                   if result and result.get("status") == "success":
                       st.success("Explanation received:")
                       st.markdown(result.get("explanation"))
                   else:
                       st.error(result.get("message", "Failed to get explanation."))
           else:
               st.warning("Please enter a code snippet and specify the language.")

       st.markdown("---")
       st.markdown("###  Generate Code Module/Class")
       gen_requirements_input = st.text_area("Describe the module/class requirements:", height=200, key="gen_requirements_input_v2") # v2 key
       gen_lang_input = st.text_input("Language (e.g., python):", value="python", key="gen_lang_input_v2") # v2 key

       if st.button("Generate Code Module", key="gen_module_button_v2"): # v2 key
           if gen_requirements_input and gen_lang_input:
               with st.spinner(f"Generating {gen_lang_input} module..."):
                   result = asyncio.run(orchestrator.generate_code_module(gen_requirements_input, gen_lang_input))
                   if result and result.get("status") == "success":
                       st.success("Code module generated:")
                       st.code(result.get("generated_code"), language=gen_lang_input.lower() if gen_lang_input else "python")
                   else:
                       st.error(result.get("message", "Failed to generate code module."))
           else:
               st.warning("Please describe the requirements and specify the language for the module.")

   elif operation_mode == "Audio Processing":
       st.subheader(" AUDIO PROCESSING SUITE")
       audio_task = st.selectbox("Select Audio Task:", ["Get Audio Info", "Convert Audio Format", "Text-to-Speech (TTS)"], key="audio_task_select")

       if audio_task == "Get Audio Info" or audio_task == "Convert Audio Format":
           uploaded_audio = st.file_uploader("Upload an audio file", type=['mp3', 'wav', 'ogg', 'flac', 'aac', 'm4a'], key="audio_upload_inf_conv")
           if uploaded_audio is not None:
               temp_audio_dir = Path(__file__).parent / "temp_uploads"
               temp_audio_dir.mkdir(parents=True, exist_ok=True)
               temp_audio_path = temp_audio_dir / uploaded_audio.name
               with open(temp_audio_path, "wb") as f:
                   f.write(uploaded_audio.getbuffer())
               st.audio(str(temp_audio_path))

               if audio_task == "Get Audio Info":
                   if st.button("Get Information", key="get_audio_info_button"):
                       with st.spinner("Fetching audio information..."):
                           result = asyncio.run(orchestrator.get_audio_info(str(temp_audio_path)))
                           if result and result.get("status") == "success":
                               st.success(result.get("message"))
                               st.json(result.get("info"))
                           else:
                               st.error(result.get("message", "Failed to get audio info."))

               elif audio_task == "Convert Audio Format":
                   target_format_options = ["mp3", "wav", "ogg", "flac"]
                   convert_target_format = st.selectbox("Select Target Format:", target_format_options, key="convert_audio_format_select")
                   if st.button("Convert Format", key="convert_audio_button"):
                       with st.spinner(f"Converting to {convert_target_format}..."):
                           result = asyncio.run(orchestrator.convert_audio_format(str(temp_audio_path), convert_target_format))
                           if result and result.get("status") == "success":
                               st.success(result.get("message"))
                               st.audio(result.get("output_path"))
                               # try:
                               #     with open(result.get("output_path"), "rb") as fp:
                               #         st.download_button(
                               #             label="Download Converted File",
                               #             data=fp,
                               #             file_name=Path(result.get("output_path")).name,
                               #             mime=f"audio/{convert_target_format}"
                               #         )
                               # except FileNotFoundError:
                               #     st.error(f"Converted file not found at {result.get('output_path')}. Cannot offer download.")
                               # except Exception as e:
                               #     st.error(f"Error preparing download: {e}")
                           else:
                               st.error(result.get("message", "Failed to convert audio."))

       elif audio_task == "Text-to-Speech (TTS)":
           tts_text = st.text_area("Enter text to convert to speech:", height=150, key="tts_text_input")
           tts_output_filename_stem = st.text_input("Output filename (without extension):", value="speech_output", key="tts_filename_stem")
           if st.button("Generate Speech", key="tts_generate_button"):
               if tts_text.strip() and tts_output_filename_stem.strip():
                   with st.spinner("Generating speech..."):
                       result = asyncio.run(orchestrator.text_to_speech(tts_text, tts_output_filename_stem))
                       if result and result.get("status") == "success":
                           st.success(result.get("message"))
                           st.audio(result.get("speech_path"))
                           # try:
                           #     with open(result.get("speech_path"), "rb") as fp:
                           #         st.download_button(
                           #             label="Download Speech",
                           #             data=fp,
                           #             file_name=Path(result.get("speech_path")).name,
                           #             mime="audio/mpeg" # Assuming mp3, adjust if wav fallback is common
                           #         )
                           # except FileNotFoundError:
                           #     st.error(f"Speech file not found at {result.get('speech_path')}. Cannot offer download.")
                           # except Exception as e:
                           #     st.error(f"Error preparing download: {e}")
                       else:
                           st.error(result.get("message", "Failed to generate speech."))
               else:
                   st.warning("Please enter text and a filename stem.")

   elif operation_mode == "System Information": # New dedicated mode
        st.subheader(" SYSTEM INFORMATION DASHBOARD")
        system_admin_agent_name = "SystemAdmin" # Standardized agent name

        # Row 1 for OS and CPU
        col_os, col_cpu = st.columns(2)
        with col_os:
            if st.button("Get OS Details", key="sysinfo_os_details_v2"):
                with st.spinner("Fetching OS details..."):
                    result_list = asyncio.run(orchestrator.parallel_execution(
                        prompt="os info",
                        selected_agents=[system_admin_agent_name]
                    ))
                    if result_list and result_list[0].get("status") == "success":
                        st.success("OS Details Retrieved:")
                        st.json(result_list[0].get("data", {"error": "No data"}))
                    else:
                        st.error(f"Failed: {result_list[0].get('message', 'Unknown error')}" if result_list else "Execution failed")

        with col_cpu:
            if st.button("Get CPU Details", key="sysinfo_cpu_details_v2"):
                with st.spinner("Fetching CPU details..."):
                    result_list = asyncio.run(orchestrator.parallel_execution(
                        prompt="cpu info",
                        selected_agents=[system_admin_agent_name]
                    ))
                    if result_list and result_list[0].get("status") == "success":
                        st.success("CPU Details Retrieved:")
                        st.json(result_list[0].get("data", {"error": "No data"}))
                    else:
                        st.error(f"Failed: {result_list[0].get('message', 'Unknown error')}" if result_list else "Execution failed")

        st.markdown("---")

        # Row 2 for Disk and Memory
        col_disk, col_mem = st.columns(2)
        with col_disk:
            if st.button("Get Disk Space", key="sysinfo_disk_space_v3"):
                with st.spinner("Fetching disk space..."):
                    result_list = asyncio.run(orchestrator.parallel_execution(
                        prompt="show disk space",
                        selected_agents=[system_admin_agent_name]
                    ))
                    if result_list and result_list[0].get("status") == "success":
                        st.text_area("Disk Space:", value=result_list[0].get("data", "No data"), height=250, disabled=True, key="disk_space_output_v3")
                    else:
                        st.error(f"Failed: {result_list[0].get('message', 'Unknown error')}" if result_list else "Execution failed")

        with col_mem:
            if st.button("Get Memory Usage", key="sysinfo_mem_usage_v3"):
                with st.spinner("Fetching memory usage..."):
                    result_list = asyncio.run(orchestrator.parallel_execution(
                        prompt="show memory usage",
                        selected_agents=[system_admin_agent_name]
                    ))
                    if result_list and result_list[0].get("status") == "success":
                        st.text_area("Memory Usage:", value=result_list[0].get("data", "No data"), height=250, disabled=True, key="memory_usage_output_v3")
                    else:
                        st.error(f"Failed: {result_list[0].get('message', 'Unknown error')}" if result_list else "Execution failed")

        st.markdown("---")

        # Row 3 for Network and Processes
        col_net, col_proc = st.columns(2)
        with col_net:
            if st.button("Get Network Configuration", key="sysinfo_net_config_v2"):
                with st.spinner("Fetching network configuration..."):
                    result_list = asyncio.run(orchestrator.parallel_execution(
                        prompt="network config",
                        selected_agents=[system_admin_agent_name]
                    ))
                    if result_list and result_list[0].get("status") == "success":
                        st.success("Network Configuration Retrieved:")
                        st.text_area("Network Config:", value=result_list[0].get("data", "No data"), height=300, disabled=True, key="net_config_output_v2")
                    else:
                        st.error(f"Failed: {result_list[0].get('message', 'Unknown error')}" if result_list else "Execution failed")

        with col_proc:
            top_n_processes = st.number_input("Number of processes for 'Top Processes':", min_value=1, max_value=50, value=10, key="sysinfo_top_n_v3")
            if st.button("Get Top Processes", key="sysinfo_top_procs_v3"):
                with st.spinner(f"Fetching top {top_n_processes} processes..."):
                    result_list = asyncio.run(orchestrator.parallel_execution(
                        prompt=f"show top {top_n_processes} processes",
                        selected_agents=[system_admin_agent_name]
                    ))
                    if result_list and result_list[0].get("status") == "success":
                        st.text_area(f"Top {top_n_processes} Processes:", value=result_list[0].get("data", "No data"), height=300, disabled=True, key="top_processes_output_v3")
                    else:
                        st.error(f"Failed: {result_list[0].get('message', 'Unknown error')}" if result_list else "Execution failed")

   elif operation_mode == "Knowledge Base Explorer": # New Block
       st.subheader(" KNOWLEDGE BASE EXPLORER")

       query_text = st.text_input("Enter your search query for the Knowledge Base:", key="kb_query_text")

       col1, col2, col3 = st.columns([2,1,1])
       with col1:
           n_results = st.number_input("Number of results to retrieve:", min_value=1, max_value=50, value=5, key="kb_n_results") # Increased max to 50
       with col2:
           filter_key = st.text_input("Metadata filter key (optional):", placeholder="e.g., filename", key="kb_filter_key")
       with col3:
           filter_value = st.text_input("Metadata filter value (optional):", placeholder="e.g., report.pdf", key="kb_filter_value")

       if st.button("Search Knowledge Base", key="kb_search_button"):
           if not query_text:
               st.warning("Please enter a search query.")
           else:
               filter_metadata_dict = None
               if filter_key and filter_value:
                   filter_metadata_dict = {filter_key.strip(): filter_value.strip()}
               elif filter_key and not filter_value:
                   st.warning(f"Filter key '{filter_key}' provided, but no filter value. Ignoring filter.")
               elif not filter_key and filter_value:
                   st.warning(f"Filter value '{filter_value}' provided, but no filter key. Ignoring filter.")

               with st.spinner("Searching knowledge base..."):
                   kb_response = asyncio.run(orchestrator.retrieve_knowledge(
                       query_text=query_text,
                       n_results=n_results,
                       filter_metadata=filter_metadata_dict
                   ))

               if kb_response and kb_response.get("status") == "success":
                   results = kb_response.get("results", [])
                   if results:
                       st.success(f"Found {len(results)} results for '{query_text}':")
                       for i, item in enumerate(results):
                           expander_title = f"Result {i+1}: ID `{item.get('id', 'N/A')}` (Distance: {item.get('distance', float('nan')):.4f})"
                           with st.expander(expander_title):
                               st.text_area(
                                   "Document Content:",
                                   value=str(item.get('document', 'N/A')),
                                   height=150,
                                   disabled=True,
                                   key=f"kb_doc_{item.get('id', i)}" # Ensure unique key
                               )
                               metadata_content = item.get('metadata')
                               if metadata_content:
                                   st.markdown("**Metadata:**")
                                   st.json(metadata_content)
                               else:
                                   st.caption("No metadata for this item.")
                           st.divider()
                   else:
                       st.info(f"No results found in the Knowledge Base for your query: '{query_text}'.")
               elif kb_response: # Error from orchestrator
                   st.error(f"Failed to search knowledge base: {kb_response.get('message', 'Unknown error')}")
               else: # Should not happen if orchestrator always returns a dict
                   st.error("An unexpected issue occurred while searching the knowledge base.")


if __name__=="__main__":
   main()
EOF
}

# Function for generating auto_dev.py
create_auto_dev_script() {
    progress "CREATING AUTO DEV SCRIPT"
    echo "Installing auto_dev dependencies from $SCRIPT_DIR/dev_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/dev_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/dev_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
    cat>"$INSTALL_DIR/tools/auto_dev.py"<<'EOF'
import subprocess,os,ast,json,shlex
from pathlib import Path

class AutoDev:
    def __init__(self):
        self.tools = {"format": "black", "lint": "flake8", "type": "mypy", "test": "pytest", "security": "bandit"}
        self.gitignore_content = """# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# PEP 582; __pypackages__
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
"""

    def create_project(self, name, project_type="python_cli"):
        project_path = Path(name)
        project_path.mkdir(exist_ok=True)

        if project_type == "python_cli":
            main_py_content = """#!/usr/bin/env python3
import typer
from typing_extensions import Annotated

app = typer.Typer()

@app.command()
def hello(name: Annotated[str, typer.Option(prompt="Enter your name")] = "World"):
    print(f"Hello {name}")

@app.command()
def goodbye(name: str = "World", formal: bool = False):
    if formal:
        print(f"Goodbye Ms. {name}. Have a good day.")
    else:
        print(f"Bye {name}!")

if __name__ == "__main__":
    app()
"""
            requirements_txt_content = "typer[all]==0.12.3"
            readme_md_content = f"""# {name}

A Command Line Interface (CLI) application built with Typer.

## Created by Terminus AI

## Installation
```bash
pip install -r requirements.txt
```

## Usage
```bash
python main.py --help
python main.py hello
python main.py goodbye --name Gokay
```
"""
            (project_path / "main.py").write_text(main_py_content)
            (project_path / "requirements.txt").write_text(requirements_txt_content)
            (project_path / "README.md").write_text(readme_md_content)
            (project_path / ".gitignore").write_text(self.gitignore_content)
            return f"Python CLI project '{name}' created successfully at {project_path}"

        elif project_type == "python_fastapi":
            app_dir = project_path / "app"
            app_dir.mkdir(exist_ok=True)

            main_py_content_fastapi = f"""from fastapi import FastAPI
from typing import Dict

app = FastAPI(title="{name} API", version="0.1.0")

@app.get("/")
async def read_root() -> Dict[str, str]:
    return {{"message": "Welcome to {name}!"}}

@app.get("/items/{{item_id}}")
async def read_item(item_id: int, q: str | None = None) -> Dict[str, any]:
    return {{"item_id": item_id, "q": q}}

# To run this application:
# 1. Install uvicorn: pip install "uvicorn[standard]"
# 2. Navigate to the project directory in your terminal.
# 3. Run: uvicorn app.main:app --reload
"""
            (app_dir / "main.py").write_text(main_py_content_fastapi)

            requirements_txt_content_fastapi = """fastapi==0.111.0
uvicorn[standard]==0.29.0
"""
            (project_path / "requirements.txt").write_text(requirements_txt_content_fastapi)

            readme_md_content_fastapi = f"""# {name} - FastAPI Backend

A FastAPI backend application.

## Created by Terminus AI

## Project Structure
```
{name}/
 app/
    main.py     # Main application logic
 requirements.txt  # Project dependencies
 README.md         # This file
```

## Setup and Installation
1.  Create a virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use \`venv\Scripts\\activate\`
    ```
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Running the Application
Navigate to the project's root directory (`{name}/`) in your terminal and run:
```bash
uvicorn app.main:app --reload
```
The application will typically be available at `http://127.0.0.1:8000`.
"""
            (project_path / "README.md").write_text(readme_md_content_fastapi)
            (project_path / ".gitignore").write_text(self.gitignore_content) # Reuse
            return f"Python FastAPI project '{name}' created successfully at {project_path}"

        elif project_type == "python_streamlit":
            app_py_content_streamlit = f"""import streamlit as st
import pandas as pd
import numpy as np

st.set_page_config(layout="wide")

st.title(" {name} - Streamlit Dashboard")

st.header("Sample Data Visualization")
st.write("This is a simple example of a Streamlit dashboard.")

# Sample Data
chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c']
)

st.subheader("Line Chart")
st.line_chart(chart_data)

st.subheader("Area Chart")
st.area_chart(chart_data)

st.sidebar.header("Controls")
option = st.sidebar.selectbox(
    "Choose a chart type:",
    ("Line Chart", "Area Chart", "Bar Chart (Random)")
)

if option == "Bar Chart (Random)":
    st.subheader("Bar Chart")
    bar_data = pd.DataFrame(
        np.random.randint(0, 100, 50),
        columns=['Data']
    )
    st.bar_chart(bar_data)

st.write("---")
st.write("Generated by Terminus AI AutoDev.")
"""
            (project_path / "app.py").write_text(app_py_content_streamlit)

            requirements_txt_content_streamlit = """streamlit==1.36.0
pandas
numpy
"""
            (project_path / "requirements.txt").write_text(requirements_txt_content_streamlit)

            readme_md_content_streamlit = f"""# {name} - Streamlit Dashboard

A Streamlit dashboard application.

## Created by Terminus AI

## Project Structure
```
{name}/
 app.py            # Main Streamlit application
 requirements.txt  # Project dependencies
 README.md         # This file
```

## Setup and Installation
1.  Create a virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use \`venv\Scripts\\activate\`
    ```
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Running the Application
Navigate to the project's root directory (`{name}/`) in your terminal and run:
```bash
streamlit run app.py
```
The application will typically open in your web browser automatically.
"""
            (project_path / "README.md").write_text(readme_md_content_streamlit)
            (project_path / ".gitignore").write_text(self.gitignore_content) # Reuse
            return f"Python Streamlit project '{name}' created successfully at {project_path}"
        else:
            # Fallback to basic Python project
            (project_path/"main.py").write_text("""#!/usr/bin/env python3

def main():
    print('Hello World')

if __name__=='__main__':
    main()""")
            (project_path/"requirements.txt").write_text("")
            (project_path/"README.md").write_text(f"# {name}\n\nProject created by Terminus AI")
            return f"Basic Python project '{name}' created successfully at {project_path}"

   def analyze_code(self,file_path):
       try:
           with open(file_path) as f:
               tree=ast.parse(f.read())
           return {"functions":[n.name for n in ast.walk(tree) if isinstance(n,ast.FunctionDef)],"classes":[n.name for n in ast.walk(tree) if isinstance(n,ast.ClassDef)],"lines":len(open(file_path).readlines())}
       except:return {"error":"Analysis failed"}

   def run_command(self,cmd_string): # Renamed cmd to cmd_string for clarity
       try:
           cmd_list = shlex.split(cmd_string)
           # Basic validation: Ensure the command is one of the known tools if possible,
           # or ensure it does not contain metacharacters if not splitting.
           # For now, we rely on shlex.split() to handle arguments safely.
           # A more robust solution might involve checking cmd_list[0] against self.tools.values()
           return subprocess.run(cmd_list, capture_output=True, text=True, check=True).stdout
       except subprocess.CalledProcessError as e:
           # Log error or return more specific error info
           return f"Command failed with error: {e.stderr}"
       except Exception as e:
           return f"Command execution failed: {str(e)}"

auto_dev=AutoDev()
EOF
}

# Function for generating data_engine.py
create_data_engine_script() {
    progress "CREATING DATA ENGINE SCRIPT"
    # Dependencies (dask, distributed, polars, etc.) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/tools/data_engine.py"<<'EOF'
import pandas as pd,numpy as np,json,sqlite3,redis,pymongo
from pathlib import Path
import plotly.express as px,plotly.graph_objects as go

class DataUniverse:
   def __init__(self):
       self.connections={}
       self.cache={}

   def load_data(self,source,format_type="auto"):
       if format_type=="auto":format_type=Path(source).suffix[1:]
       loaders={"csv":pd.read_csv,"json":pd.read_json,"xlsx":pd.read_excel,"parquet":pd.read_parquet,"sql":self.load_sql}
       loader=loaders.get(format_type,pd.read_csv)
       return loader(source) if format_type!="sql" else loader(source)

   def analyze_dataframe(self,df):
       return {"shape":df.shape,"columns":list(df.columns),"dtypes":df.dtypes.to_dict(),"missing":df.isnull().sum().to_dict(),"stats":df.describe().to_dict()}

   def create_visualization(self,df,chart_type="scatter",x=None,y=None):
       if chart_type=="scatter":return px.scatter(df,x=x,y=y)
       elif chart_type=="line":return px.line(df,x=x,y=y)
       elif chart_type=="bar":return px.bar(df,x=x,y=y)
       else:return px.histogram(df,x=x)

   def load_sql(self,query,db_path="data.db"):
       conn=sqlite3.connect(db_path)
       return pd.read_sql_query(query,conn)

data_engine=DataUniverse()
EOF
}

# Function for generating quantum_engine.py
create_quantum_engine_script() {
    progress "CREATING QUANTUM ENGINE SCRIPT"
    # Dependencies (qiskit, pennylane, etc.) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/core/quantum_engine.py"<<'EOF'
import numpy as np
from qiskit import QuantumCircuit,execute,Aer
from qiskit.circuit.library import RealAmplitudes,ZZFeatureMap

class QuantumProcessor:
   def __init__(self):
       self.backend=Aer.get_backend('qasm_simulator')
       self.circuits={}

   def create_circuit(self,qubits=4):
       qc=QuantumCircuit(qubits,qubits)
       return qc

   def quantum_transform(self,data):
       # Quantum data transformation
       qc=self.create_circuit(len(data))
       for i,val in enumerate(data):
           qc.ry(val*np.pi,i)
       qc.measure_all()
       job=execute(qc,self.backend,shots=1024)
       return job.result().get_counts()

   def quantum_optimization(self,objective_function,params):
       # Quantum optimization algorithm
       return {"optimized_params":params,"cost":objective_function(params)}

quantum_proc=QuantumProcessor()
EOF
}

# Function for generating nas_engine.py
create_nas_engine_script() {
    progress "CREATING NAS ENGINE SCRIPT"
    cat>"$INSTALL_DIR/core/nas_engine.py"<<'EOF'
import torch,torch.nn as nn,random

class NASEngine:
   def __init__(self):
       self.architectures=[]
       self.performance_history={}

   def generate_architecture(self,layers=5):
       layer_types=['conv','linear','attention','residual']
       activations=['relu','gelu','swish','leaky_relu']
       arch={'layers':[],'optimizer':'adam','lr':0.001}
       for i in range(layers):
           layer={'type':random.choice(layer_types),'activation':random.choice(activations),'size':random.choice([64,128,256,512])}
           arch['layers'].append(layer)
       return arch

   def evolve_architecture(self,base_arch,mutation_rate=0.1):
       new_arch=base_arch.copy()
       if random.random()<mutation_rate:
           layer_idx=random.randint(0,len(new_arch['layers'])-1)
           new_arch['layers'][layer_idx]['size']*=random.choice([0.5,2])
       return new_arch

   def search_optimal_architecture(self,dataset,epochs=10):
       best_arch=None
       best_performance=0
       for _ in range(epochs):
           arch=self.generate_architecture()
           performance=self.evaluate_architecture(arch,dataset)
           if performance>best_performance:
               best_arch,best_performance=arch,performance
       return best_arch,best_performance

   def evaluate_architecture(self,arch,dataset):
       # Simplified evaluation
       return random.random()

nas_engine=NASEngine()
EOF
}

# Function for generating distributed_engine.py
create_distributed_engine_script() {
    progress "CREATING DISTRIBUTED ENGINE SCRIPT"
    # Dependencies (ray, dask, distributed, celery) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/core/distributed_engine.py"<<'EOF'
import ray,asyncio,threading,multiprocessing
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor

@ray.remote
class DistributedWorker:
   def __init__(self,worker_id):
       self.worker_id=worker_id
       self.tasks_completed=0

   def process_task(self,task_data):
       # Distributed task processing
       self.tasks_completed+=1
       return f"Worker {self.worker_id} processed task: {task_data}"

class DistributedMesh:
   def __init__(self,num_workers=4):
       if not ray.is_initialized():ray.init()
       self.workers=[DistributedWorker.remote(i) for i in range(num_workers)]
       self.task_queue=[]

   def distribute_tasks(self,tasks):
       futures=[]
       for i,task in enumerate(tasks):
           worker=self.workers[i%len(self.workers)]
           future=worker.process_task.remote(task)
           futures.append(future)
       return ray.get(futures)

   def scale_workers(self,new_count):
       current=len(self.workers)
       if new_count>current:
           self.workers.extend([DistributedWorker.remote(i) for i in range(current,new_count)])
       return f"Scaled to {new_count} workers"

distributed_mesh=DistributedMesh()
EOF
}

# Function for generating security_engine.py
create_security_engine_script() {
    progress "CREATING SECURITY ENGINE SCRIPT"
    # Dependencies (cryptography, keyring, etc.) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/core/security_engine.py"<<'EOF'
import hashlib,secrets,base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

class SecurityEngine:
   def __init__(self):
       self.master_key=Fernet.generate_key()
       self.cipher=Fernet(self.master_key)
       self.sessions={}

   def encrypt_data(self,data):
       return self.cipher.encrypt(data.encode()).decode()

   def decrypt_data(self,encrypted_data):
       return self.cipher.decrypt(encrypted_data.encode()).decode()

   def generate_session_token(self):
       return secrets.token_urlsafe(32)

   def hash_password(self,password,salt=None):
       if not salt:salt=secrets.token_hex(16)
       return hashlib.pbkdf2_hmac('sha256',password.encode(),salt.encode(),100000).hex(),salt

   def verify_password(self,password,hash_value,salt):
       return self.hash_password(password,salt)[0]==hash_value

   def secure_communication(self,message):
       encrypted=self.encrypt_data(message)
       signature=hashlib.sha256(encrypted.encode()).hexdigest()
       return {"encrypted_message":encrypted,"signature":signature}

security_engine=SecurityEngine()
EOF
}

# Function for generating launch_terminus.py
create_launcher_script() {
    progress "CREATING LAUNCHER SCRIPT"
    cat>"$INSTALL_DIR/launch_terminus.py"<<'EOF'
#!/usr/bin/env python3
import subprocess,sys,os,time,threading,signal
from pathlib import Path
import streamlit as st

class TerminusLauncher:
   def __init__(self):
       self.base_dir=Path(__file__).parent
       self.processes={}
       self.running=True
       self.restart_counts = {}
       self.max_restarts = 5

   def start_ollama(self):
       print("Starting Ollama server...")
       self.processes['ollama']=subprocess.Popen(['ollama','serve'],stdout=subprocess.DEVNULL,stderr=subprocess.DEVNULL)
       time.sleep(5)

   def start_streamlit(self):
       print("Starting Terminus UI...")
       cmd=['streamlit','run',str(self.base_dir/'terminus_ui.py'),'--server.port','8501','--server.address','0.0.0.0']
       self.processes['streamlit']=subprocess.Popen(cmd)

   def monitor_system(self):
       while self.running:
           for name, proc in list(self.processes.items()): # Ensure list() for safe iteration
               if proc.poll() is not None: # Process has terminated
                   current_restarts = self.restart_counts.get(name, 0)
                   if current_restarts >= self.max_restarts:
                       print(f"ERROR: Process {name} has crashed {current_restarts} times and exceeded max restart limit of {self.max_restarts}. Will not attempt to restart again.")
                       # Optionally remove from self.processes or mark as failed
                       # For example, to stop further checks on this failed process:
                       del self.processes[name]
                       continue

                   self.restart_counts[name] = current_restarts + 1
                   print(f"Process {name} (PID {proc.pid}) terminated unexpectedly. Restarting (attempt {self.restart_counts[name]}/{self.max_restarts})...")
                   if name == 'ollama':
                       self.start_ollama()
                   elif name == 'streamlit':
                       self.start_streamlit()
                   # Add a small delay after a restart attempt to prevent rapid failing loops
                   time.sleep(2)
               else:
                   # Process is running, reset its restart count if it was previously failing
                   if self.restart_counts.get(name, 0) > 0:
                       print(f"Process {name} is running normally. Resetting restart count.")
                       self.restart_counts[name] = 0
           time.sleep(10) # Check every 10 seconds

   def shutdown(self,signum=None,frame=None):
       print("\nShutting down Terminus AI...")
       self.running=False
       for proc in self.processes.values():
           proc.terminate()
       sys.exit(0)

   def launch(self):
       signal.signal(signal.SIGINT,self.shutdown)
       signal.signal(signal.SIGTERM,self.shutdown)

       print("TERMINUS AI - ULTIMATE LOCAL AI ECOSYSTEM")
       print(f"Base Directory: {self.base_dir}")
       print("Models: 25+ AI Models Ready")
       print("Agents: 12 Specialized AI Agents")
       print("Total Size: ~280GB")
       print("Interface: http://localhost:8501")

       self.start_ollama()
       self.start_streamlit()

       monitor_thread=threading.Thread(target=self.monitor_system,daemon=True)
       monitor_thread.start()

       print("Terminus AI is now running!")
       print("Access the interface at: http://localhost:8501")
       print("Press Ctrl+C to shutdown")

       try:
           while self.running:time.sleep(1)
       except KeyboardInterrupt:
           self.shutdown()

if __name__=="__main__":
   launcher=TerminusLauncher()
   launcher.launch()
EOF

chmod +x "$INSTALL_DIR/launch_terminus.py"
}

# Function for creating the final README and any other wrap-up tasks
finalize_installation() {
    progress "FINALIZING INSTALLATION & CREATING README"
    cat>"$INSTALL_DIR/README.md"<<'EOF'
# TERMINUS AI - ULTIMATE LOCAL AI ECOSYSTEM

## OPUS MAGNUM SPECIFICATIONS
- **Total Size**: ~280GB
- **AI Models**: 25+ State-of-the-Art Models
- **Agents**: 12 Specialized AI Agents
- **Capabilities**: Unlimited & Uncensored
- **Architecture**: Quantum-Classical Hybrid
- **Interface**: Advanced Web-Based Control Center

## INSTALLATION COMPLETE
### Quick Start:
```bash
cd ~/.terminus-ai
python3 launch_terminus.py
```
EOF
    echo "TERMINUS AI INSTALLATION COMPLETE. See README.md in $INSTALL_DIR for details." | tee -a "$LOG"
}

# Main execution flow
main() {
    initialize_setup
    install_system_dependencies
    install_python_core_libraries
    install_python_framework_libraries
    install_python_utility_libraries
    install_ollama_and_dependencies
    pull_ollama_models
    create_agent_orchestration_script
    create_terminus_ui_script
    create_auto_dev_script
    create_data_engine_script
    create_quantum_engine_script
    create_nas_engine_script
    create_distributed_engine_script
    create_security_engine_script
    create_launcher_script
    finalize_installation

    echo "ALL STAGES COMPLETED SUCCESSFULLY!" | tee -a "$LOG"
}

# Run the main function
main
