#!/bin/bash
# OPUS MAGNUM: ULTIMATE TERMINALIS AI ECOSYSTEM
# Size: ~280GB | Models: 25+ | Capabilities: UNLIMITED
set -e

# Determine the directory where the script is located
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"

INSTALL_DIR="$HOME/.terminus-ai"
LOG="$INSTALL_DIR/install.log"
TOTAL=17 # Total number of main functions called by main()
STEP=0

# Function to display progress
progress(){
    STEP=$((STEP+1))
    echo "[$STEP/$TOTAL-$((STEP*100/TOTAL))%] $1" | tee -a "$LOG"
}

# Function for initial directory creation and logging setup
initialize_setup() {
    progress "INITIALIZING SETUP"
    mkdir -p "$INSTALL_DIR"/{core,models,agents,tools,data,logs,cache}
    touch "$LOG"
    echo "TERMINUS AI: THE ULTIMATE LOCAL AI ECOSYSTEM" | tee -a "$LOG"
    echo "Total: ~280GB | Models: 25+ | Agents: Unlimited" | tee -a "$LOG"

    # Copy models.conf from script directory to INSTALL_DIR
    if [ -f "$SCRIPT_DIR/models.conf" ]; then
        cp "$SCRIPT_DIR/models.conf" "$INSTALL_DIR/models.conf"
        echo "Copied models.conf to $INSTALL_DIR" | tee -a "$LOG"
    else
        echo "WARNING: models.conf not found in script directory ($SCRIPT_DIR). Model configuration will rely on fallback or potentially fail if critical." | tee -a "$LOG"
        # The pull_ollama_models function has its own check for $INSTALL_DIR/models.conf and fallback.
    fi
}

# Function to install system dependencies
install_system_dependencies() {
    progress "INSTALLING SYSTEM DEPENDENCIES" # Corresponds to old "INITIALIZING QUANTUM CORE SYSTEMS" (second part)
    if command -v apt &>/dev/null; then
        sudo apt update && sudo apt install -y python3 python3-pip docker.io git curl wget build-essential cmake ninja-build nodejs npm golang rust-all-dev
    fi
    if command -v brew &>/dev/null; then
        brew install python docker git curl wget cmake ninja nodejs go rust
    fi
    # Add more error checking here in later plan steps
}

# No longer defining MODELS array globally. It's handled in pull_ollama_models function.

# Function to install base Python packages like torch, transformers, etc.
install_python_core_libraries() {
    progress "INSTALLING PYTHON CORE LIBRARIES"
    python3 -m pip install --upgrade pip setuptools wheel
    if [ $? -ne 0 ]; then
        echo "WARNING: Failed to upgrade pip, setuptools, or wheel. Check $LOG for details." | tee -a "$LOG"
    fi
    echo "Installing Python core libraries from $SCRIPT_DIR/core_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/core_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/core_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
}

# Function for Langchain, Autogen, UI frameworks, etc.
install_python_framework_libraries() {
    progress "INSTALLING PYTHON FRAMEWORK LIBRARIES"
    echo "Installing Python framework libraries from $SCRIPT_DIR/frameworks_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/frameworks_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/frameworks_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
}

# Function for web scraping, data handling, file processing, etc.
install_python_utility_libraries() {
    progress "INSTALLING PYTHON UTILITY LIBRARIES"
    echo "Installing Python utility libraries from $SCRIPT_DIR/utils_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/utils_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/utils_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
}

# Function for installing Ollama
install_ollama_and_dependencies() {
    progress "INSTALLING OLLAMA AND DEPENDENCIES"
    curl -fsSL https://ollama.ai/install.sh | sh
    ollama serve &
    echo "Waiting for Ollama server to start..." | tee -a "$LOG"
    sleep 10 # Increased sleep time for robustness
    if ! ollama list > /dev/null 2>&1 && ! curl -sf --head http://localhost:11434 | grep "HTTP/[12]\.[01] [2].." > /dev/null; then
        echo "ERROR: Ollama server failed to start or is not responding. Aborting. Check $LOG for details." | tee -a "$LOG"
        exit 1
    else
        echo "Ollama server started successfully." | tee -a "$LOG"
    fi
}

# Function for downloading AI models
pull_ollama_models() {
    progress "SELECTING AND PULLING OLLAMA MODELS"

    ALL_AVAILABLE_MODELS=()
    CORE_MODELS=()
    CURRENT_SECTION=""
    CONFIG_FILE="$INSTALL_DIR/models.conf" # Assuming models.conf is in INSTALL_DIR

    if [ ! -f "$CONFIG_FILE" ]; then
        echo "ERROR: Configuration file '$CONFIG_FILE' not found." | tee -a "$LOG"
        echo "Please ensure 'models.conf' exists in $INSTALL_DIR." | tee -a "$LOG"
        echo "Proceeding with no models available for selection. You can only skip model download." | tee -a "$LOG"
        # Fallback: Define minimal core models if config is missing, to prevent errors later if user tries to select core
        CORE_MODELS=("llama3.1:8b" "mistral:7b") # Minimal fallback
    else
        echo "Reading model lists from $CONFIG_FILE..." | tee -a "$LOG"
        while IFS= read -r line || [ -n "$line" ]; do
            # Remove leading/trailing whitespace (optional, but good for robustness)
            line=$(echo "$line" | awk '{$1=$1};1')

            # Skip empty lines and comments
            [[ "$line" =~ ^\s*# ]] && continue
            [[ "$line" =~ ^\s*$ ]] && continue

            if [[ "$line" =~ ^\[(.*)\]$ ]]; then
                CURRENT_SECTION="${BASH_REMATCH[1]}"
            else
                # Remove potential carriage returns for cross-platform compatibility
                line=$(echo "$line" | tr -d '\r')
                if [ -n "$line" ]; then # Ensure line is not empty after stripping CR
                    case "$CURRENT_SECTION" in
                        ALL_AVAILABLE_MODELS)
                            ALL_AVAILABLE_MODELS+=("$line")
                            ;;
                        CORE_MODELS)
                            CORE_MODELS+=("$line")
                            ;;
                    esac
                fi
            fi
        done < "$CONFIG_FILE"
        echo "Finished reading model lists. Found ${#ALL_AVAILABLE_MODELS[@]} available models and ${#CORE_MODELS[@]} core models." | tee -a "$LOG"
    fi

    if [ ${#ALL_AVAILABLE_MODELS[@]} -eq 0 ] && [ -f "$CONFIG_FILE" ]; then
        echo "WARNING: No models were loaded from $CONFIG_FILE. It might be empty or incorrectly formatted." | tee -a "$LOG"
        echo "Model download options will be limited. You may only be able to skip." | tee -a "$LOG"
    elif [ ${#ALL_AVAILABLE_MODELS[@]} -eq 0 ] && [ ! -f "$CONFIG_FILE" ]; then
        # Error already printed, this is just to ensure the flow is logical
        echo "Continuing with no models defined due to missing models.conf." | tee -a "$LOG"
    fi

    # Ensure CORE_MODELS is not empty if user might select it, even if ALL_AVAILABLE_MODELS is empty.
    # This is a safety net, though the user prompt should guide them.
    if [ ${#CORE_MODELS[@]} -eq 0 ] && [ ${#ALL_AVAILABLE_MODELS[@]} -gt 0 ]; then
        echo "WARNING: CORE_MODELS list is empty in models.conf. Selecting 'CORE' will result in no models being downloaded unless ALL models are also empty." | tee -a "$LOG"
    elif [ ${#CORE_MODELS[@]} -eq 0 ] && [ ${#ALL_AVAILABLE_MODELS[@]} -eq 0 ]; then
         # If both are empty (e.g. models.conf missing and no fallback for CORE_MODELS or ALL_AVAILABLE_MODELS)
         CORE_MODELS=("llama3.1:8b" "mistral:7b") # Re-apply minimal fallback for safety if somehow cleared
         echo "Re-applying minimal fallback for CORE_MODELS as both lists were empty." | tee -a "$LOG"
    fi


    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "Ollama Model Installation Options:" | tee -a "$LOG"
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "Available models for installation:" | tee -a "$LOG"
    for i in "${!ALL_AVAILABLE_MODELS[@]}"; do
        printf "  %2d. %s\n" "$((i+1))" "${ALL_AVAILABLE_MODELS[$i]}" | tee -a "$LOG"
    done
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "You can choose to:" | tee -a "$LOG"
    echo "  1. Download ALL available models (${#ALL_AVAILABLE_MODELS[@]} models, ~180GB+)." | tee -a "$LOG"
    echo "  2. Download a CORE set of essential models (${#CORE_MODELS[@]} models, ~20-50GB)." | tee -a "$LOG"
    echo "  3. Select specific models to download." | tee -a "$LOG"
    echo "  4. Skip Ollama model downloads for now." | tee -a "$LOG"
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    read -r -p "Enter your choice (1, 2, 3, or 4): " user_choice

    MODELS_TO_PULL=()
    case "$user_choice" in
        1)
            echo "Preparing to download ALL ${#ALL_AVAILABLE_MODELS[@]} models." | tee -a "$LOG"
            MODELS_TO_PULL=("${ALL_AVAILABLE_MODELS[@]}")
            ;;
        2)
            echo "Preparing to download CORE set of ${#CORE_MODELS[@]} models." | tee -a "$LOG"
            MODELS_TO_PULL=("${CORE_MODELS[@]}")
            ;;
        3)
            echo "Enter the names of the models you wish to download, separated by spaces." | tee -a "$LOG"
            echo "Example: llama3.1:8b mistral:7b deepseek-coder-v2:16b" | tee -a "$LOG"
            echo "Available models listed above. Please type or copy-paste exact names." | tee -a "$LOG"
            read -r -p "Selected models: " selected_models_str
            # Convert the space-separated string to an array
            read -r -a USER_SELECTED_MODELS <<< "$selected_models_str"

            # Validate user selections
            for model_name in "${USER_SELECTED_MODELS[@]}"; do
                is_valid=false
                for available_model in "${ALL_AVAILABLE_MODELS[@]}"; do
                    if [[ "$model_name" == "$available_model" ]]; then
                        MODELS_TO_PULL+=("$model_name")
                        is_valid=true
                        break
                    fi
                done
                if ! $is_valid; then
                    echo "WARNING: Model '$model_name' is not in the list of available models and will be skipped." | tee -a "$LOG"
                fi
            done

            if [ ${#MODELS_TO_PULL[@]} -eq 0 ] && [ ${#USER_SELECTED_MODELS[@]} -ne 0 ]; then
                 echo "No valid models selected from your input. Defaulting to CORE models." | tee -a "$LOG"
                 MODELS_TO_PULL=("${CORE_MODELS[@]}")
            elif [ ${#MODELS_TO_PULL[@]} -eq 0 ]; then
                 echo "No models selected. Defaulting to CORE models." | tee -a "$LOG"
                 MODELS_TO_PULL=("${CORE_MODELS[@]}")
            fi
            ;;
        4)
            echo "Skipping Ollama model downloads as per user choice." | tee -a "$LOG"
            # MODELS_TO_PULL will remain empty
            ;;
        *)
            echo "Invalid choice. Defaulting to CORE set of models." | tee -a "$LOG"
            MODELS_TO_PULL=("${CORE_MODELS[@]}")
            ;;
    esac

    if [ ${#MODELS_TO_PULL[@]} -eq 0 ]; then
        echo "No models selected for download. Skipping Ollama model pulling phase." | tee -a "$LOG"
        return
    fi

    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    echo "The following ${#MODELS_TO_PULL[@]} models will be downloaded:" | tee -a "$LOG"
    for model_to_pull in "${MODELS_TO_PULL[@]}"; do
        echo "- $model_to_pull" | tee -a "$LOG"
    done
    echo "----------------------------------------------------------------------" | tee -a "$LOG"
    sleep 3 # Give user time to read

    FAILED_MODELS=()
    SUCCESSFUL_MODELS=0
    TOTAL_MODELS_TO_PULL=${#MODELS_TO_PULL[@]}

    for model in "${MODELS_TO_PULL[@]}";do # Iterate over MODELS_TO_PULL
        echo "Pulling $model ($((SUCCESSFUL_MODELS + ${#FAILED_MODELS[@]} + 1))/$TOTAL_MODELS_TO_PULL)..." | tee -a "$LOG"
        ollama pull "$model"
        if [ $? -ne 0 ]; then
            echo "WARNING: Failed to pull model $model. It will be skipped. Check $LOG for details." | tee -a "$LOG"
            FAILED_MODELS+=("$model")
        else
            echo "Successfully pulled $model." | tee -a "$LOG"
            SUCCESSFUL_MODELS=$((SUCCESSFUL_MODELS + 1))
        fi
    done

    echo "Ollama model pulling complete. $SUCCESSFUL_MODELS/$TOTAL_MODELS models downloaded successfully." | tee -a "$LOG"

    if [ ${#FAILED_MODELS[@]} -ne 0 ]; then
        echo "Summary of failed model downloads (${#FAILED_MODELS[@]}):" | tee -a "$LOG"
        for failed_model in "${FAILED_MODELS[@]}"; do
            echo "- $failed_model" | tee -a "$LOG"
        done
    fi
}

# Function for generating master_orchestrator.py
create_agent_orchestration_script() {
    progress "CREATING AGENT CONFIGURATION FILE (agents.json)"
    cat>"$INSTALL_DIR/agents.json"<<'AGENTS_EOF'
[
  {
    "name": "DeepThink",
    "model": "deepseek-r1:32b",
    "specialty": "Advanced Reasoning & Logic",
    "active": true
  },
  {
    "name": "CodeMaster",
    "model": "deepseek-coder-v2:16b",
    "specialty": "Programming & Development",
    "active": true
  },
  {
    "name": "DataWizard",
    "model": "qwen2.5:72b",
    "specialty": "Data Analysis & Processing",
    "active": true
  },
  {
    "name": "WebCrawler",
    "model": "dolphin-mixtral:8x7b",
    "specialty": "Web Research & Intelligence",
    "active": true
  },
  {
    "name": "DocProcessor",
    "model": "llama3.1:70b",
    "specialty": "Document Analysis & Generation",
    "active": true
  },
  {
    "name": "VisionAI",
    "model": "llava:34b",
    "specialty": "Image & Visual Processing",
    "active": true
  },
  {
    "name": "MathGenius",
    "model": "deepseek-math:7b",
    "specialty": "Mathematical Computations",
    "active": true
  },
  {
    "name": "CreativeWriter",
    "model": "nous-hermes2:34b",
    "specialty": "Creative Content Generation",
    "active": true
  },
  {
    "name": "SystemAdmin",
    "model": "codellama:34b",
    "specialty": "System Administration",
    "active": true
  },
  {
    "name": "SecurityExpert",
    "model": "mixtral:8x22b",
    "specialty": "Cybersecurity Analysis",
    "active": true
  },
  {
    "name": "ResearchBot",
    "model": "yi:34b",
    "specialty": "Scientific Research",
    "active": true
  },
  {
    "name": "MultiLang",
    "model": "qwen2.5-coder:32b",
    "specialty": "Multilingual Processing",
    "active": true
  },
  {
    "name": "ImageForge",
    "model": "diffusers/stable-diffusion-xl-base-1.0",
    "specialty": "Image Generation",
    "active": true
  }
]
AGENTS_EOF
    echo "Created agents.json in $INSTALL_DIR" | tee -a "$LOG"

    progress "CREATING AGENT ORCHESTRATION SCRIPT (master_orchestrator.py)" # Clarified progress message
    cat>"$INSTALL_DIR/agents/master_orchestrator.py"<<'EOF'
import asyncio, json, requests, subprocess, threading, queue, time, datetime
import torch
import aiohttp
from diffusers import DiffusionPipeline
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
from dataclasses import dataclass
from typing import List,Dict,Any,Optional
from pathlib import Path
from tools.auto_dev import auto_dev # Added for project scaffolding
# streamlit and pandas are not directly used here but in the UI script that imports this.

@dataclass
class Agent:
   name:str;model:str;specialty:str;active:bool=True

class TerminusOrchestrator:
   def __init__(self):
       self.agents = []
       agents_file_path = Path(__file__).parent.parent / "agents.json"
       try:
           with open(agents_file_path, 'r') as f:
               agents_data = json.load(f)
           for agent_config in agents_data:
               self.agents.append(Agent(
                   name=agent_config.get('name'),
                   model=agent_config.get('model'),
                   specialty=agent_config.get('specialty'),
                   active=agent_config.get('active', True)
               ))
       except FileNotFoundError:
           print(f"ERROR: agents.json not found at {agents_file_path}. No agents loaded.")
       except json.JSONDecodeError:
           print(f"ERROR: Could not decode agents.json. Invalid JSON format. No agents loaded.")
       except Exception as e:
           print(f"ERROR: An unexpected error occurred while loading agents from agents.json: {e}. No agents loaded.")

       self.ollama_url="http://localhost:11434/api/generate"
       self.active_tasks={}

       # Image Generation Setup
       self.image_gen_pipeline = None
       self.device = "cuda" if torch.cuda.is_available() else "cpu"
       self.image_gen_model_id = "stabilityai/stable-diffusion-xl-base-1.0" # Default, can be made configurable
       self.generated_images_dir = Path(__file__).parent.parent / "data" / "generated_images"
       self.generated_images_dir.mkdir(parents=True, exist_ok=True)

   async def scaffold_new_project(self, project_name: str, project_type: str) -> Dict:
       if not project_name or not project_type:
           return {"status": "error", "message": "Project name and type are required."}

       # Sanitize project_name to prevent directory traversal or invalid characters
       # A simple alphanumeric check, allowing underscores and hyphens
       safe_project_name = "".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in project_name)
       if not safe_project_name: # Handle case where sanitization results in an empty string
           safe_project_name = "default_project_name"

       print(f"Attempting to scaffold project: Name='{safe_project_name}', Type='{project_type}'")
       try:
           # auto_dev is imported at the top of the file now
           message = auto_dev.create_project(name=safe_project_name, project_type=project_type)

           if "successfully" in message:
               return {"status": "success", "message": message, "project_name": safe_project_name, "project_type": project_type}
           else:
               return {"status": "error", "message": message}
       except Exception as e:
           error_message = f"Failed to scaffold project '{safe_project_name}' of type '{project_type}': {str(e)}"
           print(f"ERROR: {error_message}")
           return {"status": "error", "message": error_message}

   async def generate_image_with_diffusion(self, prompt: str) -> Dict:
       if self.image_gen_pipeline is None:
           print(f"Loading image generation model ({self.image_gen_model_id})... This may take a while.")
           try:
               self.image_gen_pipeline = DiffusionPipeline.from_pretrained(
                   self.image_gen_model_id,
                   torch_dtype=torch.float16, # Use float16 for efficiency
                   use_safetensors=True
               )
               self.image_gen_pipeline.to(self.device)
               # Optional: if low VRAM, enable CPU offloading
               # if self.device == "cuda": # Only for CUDA, check VRAM if possible
               #     try:
               #         if torch.cuda.get_device_properties(0).total_memory < 8 * 1024**3: # Example: Less than 8GB VRAM
               #             print("Low VRAM detected, enabling model CPU offload for image generation.")
               #             self.image_gen_pipeline.enable_model_cpu_offload()
               #     except Exception as e:
               #         print(f"Could not check VRAM properties: {e}. Proceeding without CPU offload check.")
               print("Image generation model loaded.")
           except Exception as e:
               print(f"ERROR: Could not load image generation model: {str(e)}")
               return {"agent": "ImageForge", "response": f"Error loading model: {str(e)}", "status": "error", "image_path": None}

       print(f"Generating image for prompt: '{prompt}' on device: {self.device}")
       try:
           image = self.image_gen_pipeline(prompt).images[0]

           timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
           image_filename = f"image_{timestamp}.png"
           image_path = self.generated_images_dir / image_filename
           image.save(image_path)
           print(f"Image saved to {image_path}")

           return {
               "agent": "ImageForge",
               "response": f"Image generated successfully: {image_filename}",
               "status": "success",
               "image_path": str(image_path)
           }
       except Exception as e:
           print(f"ERROR: Failed to generate image: {str(e)}")
           return {"agent": "ImageForge", "response": f"Error generating image: {str(e)}", "status": "error", "image_path": None}

   async def execute_agent(self, agent: Agent, prompt: str, context: Dict = None) -> Dict:
       if agent.name == "ImageForge": # Or use agent.specialty == "Image Generation"
           return await self.generate_image_with_diffusion(prompt)
       else:
           try:
               payload = {"model": agent.model, "prompt": f"[{agent.specialty}] {prompt}", "stream": False, "options": {"temperature": 0.7}}
               if context:
                   payload["prompt"] += f"\nContext: {json.dumps(context)}"

               async with aiohttp.ClientSession() as session:
                   async with session.post(self.ollama_url, json=payload) as resp:
                       if resp.status != 200:
                           error_text = await resp.text()
                           print(f"Ollama API Error for agent {agent.name} with model {agent.model}: {resp.status} - {error_text}")
                           return {"agent": agent.name, "model": agent.model, "response": f"Error from Ollama: {resp.status} - {error_text}", "status": "error"}
                       result = await resp.json()
                       return {"agent": agent.name, "model": agent.model, "response": result.get("response", "Error: No response field"), "status": "success"}
           except aiohttp.ClientConnectorError as e:
               print(f"Connection Error for agent {agent.name} (model {agent.model}) targeting {self.ollama_url}: {str(e)}")
               return {"agent": agent.name, "model": agent.model, "response": f"Connection Error: Could not connect to Ollama server at {self.ollama_url}. Details: {str(e)}", "status": "error"}
           except Exception as e:
               print(f"Generic Error executing agent {agent.name} (model {agent.model}): {str(e)}")
               return {"agent": agent.name, "model": agent.model, "response": f"Error executing agent: {str(e)}", "status": "error"}

   async def parallel_execution(self, prompt: str, selected_agents: List[str] = None, context: Dict = None) -> List[Dict]:
       prompt_lower = prompt.lower()
       active_agents_list = []
       agent_selection_reason = "User selected"

       if not selected_agents:
           agent_keywords = {
               "ImageForge": ["image of", "picture of", "draw a", "generate art", "create a photo", "generate image"],
               "CodeMaster": ["python code for", "write a script to", "generate a function that", "develop a program to", "code snippet for"],
               "DataWizard": ["analyze data", "statistics for csv", "excel report on", "plot data from", "database insights"],
               "WebCrawler": ["search the web for", "find information online about", "what's the latest on", "look up on internet"],
               "DocProcessor": ["summarize document", "analyze this text", "process pdf content", "read file content"],
               "MathGenius": ["calculate", "solve math", "what is the result of", "compute"],
           }

           determined_agent_objects = []
           # Use a set to keep track of names to avoid duplicate Agent objects
           determined_agent_names = set()

           for agent_name, keywords in agent_keywords.items():
               if any(keyword in prompt_lower for keyword in keywords):
                   agent_obj = next((a for a in self.agents if a.name == agent_name and a.active), None)
                   if agent_obj and agent_obj.name not in determined_agent_names:
                       determined_agent_objects.append(agent_obj)
                       determined_agent_names.add(agent_obj.name)

           if determined_agent_objects:
               active_agents_list = determined_agent_objects
               agent_selection_reason = "Intent recognized"
           else:
               default_general_agents = ["DeepThink", "CreativeWriter"]
               active_agents_list = [a for a in self.agents if a.name in default_general_agents and a.active]
               agent_selection_reason = "Default general agents"

               if not active_agents_list: # If default general agents are not active or found
                   active_agents_list = [a for a in self.agents if a.active] # Fallback to all active agents
                   agent_selection_reason = "Fallback to all active"
       else:
           active_agents_list = [a for a in self.agents if a.name in selected_agents and a.active]

       if not active_agents_list:
           print("No active agents determined for the prompt. Cannot execute.")
           return [{"agent": "System", "model": "N/A", "response": "No suitable active agents found for your request.", "status": "error"}]

       print(f"Agents selected for prompt '{prompt[:50]}...': {[a.name for a in active_agents_list]} (Reason: {agent_selection_reason})")

       tasks = [self.execute_agent(agent, prompt, context) for agent in active_agents_list]
       results = await asyncio.gather(*tasks, return_exceptions=True)

       processed_results = []
       for i, r_or_e in enumerate(results):
           agent_name = active_agents_list[i].name if i < len(active_agents_list) else "UnknownAgent"
           agent_model = active_agents_list[i].model if i < len(active_agents_list) else "N/A"
           if isinstance(r_or_e, Exception):
               print(f"Error during execution for agent {agent_name}: {r_or_e}")
               processed_results.append({"agent": agent_name, "model": agent_model, "response": f"An unexpected error occurred: {str(r_or_e)}", "status": "error"})
           elif isinstance(r_or_e, dict):
               processed_results.append(r_or_e)
           else:
               processed_results.append({"agent": agent_name, "model": agent_model, "response": f"Unexpected result type: {type(r_or_e)}", "status": "error"})

       return processed_results

   def consensus_analysis(self,results:List[Dict])->Dict:
       responses=[r["response"] for r in results if r["status"]=="success"]
       # Basic consensus: return the most common response or the longest one if all unique
       if not responses:
           return {"consensus_score":0, "best_response":"No valid responses from agents.", "summary":"No successful agent responses."}

       # Example: Could count frequencies or find longest/most detailed if diverse.
       # For now, a simplified approach:
       best_response = max(responses, key=len) if responses else "No valid responses"
       consensus_score = len(responses) / len(results) if results else 0 # results could be empty if all selected_agents were inactive

       return {"consensus_score":consensus_score,"best_response":best_response,"summary":f"Processed by {len(responses)} of {len(results)} initially tasked agents."}

class DocumentUniverse:
   def __init__(self):
       self.processors={"pdf":self.pdf_proc,"docx":self.docx_proc,"xlsx":self.xlsx_proc,"html":self.html_proc,"json":self.json_proc,"csv":self.csv_proc,"txt":self.txt_proc}
   def pdf_proc(self,file):import fitz;return fitz.open(file).get_text()
   def docx_proc(self,file):import docx;return '\n'.join([p.text for p in docx.Document(file).paragraphs])
   def xlsx_proc(self,file):import openpyxl;return str(list(openpyxl.load_workbook(file).active.values))
   def html_proc(self,file):from bs4 import BeautifulSoup;return BeautifulSoup(open(file),'html.parser').get_text()
   def json_proc(self,file):return json.load(open(file))
   def csv_proc(self,file):import csv;return list(csv.reader(open(file)))
   def txt_proc(self,file):return open(file).read()
   def process_file(self,file_path):
       # This method expects a file path string or a file-like object for Streamlit's UploadedFile
       # For now, assuming file_path is an UploadedFile object from Streamlit, which has a 'name' attribute
       # and can be passed directly to functions expecting a file-like object.
       # If it's a path string, it needs to be opened.
       # This might need adjustment based on how Streamlit passes the file.
       # Assuming the UploadedFile object itself can be passed to the processors if they handle file-like objects.
       # For paths, they'd need open(file_path, 'rb') or similar.
       # The current implementation seems to mix this. For simplicity, let's assume it receives a path for now.
       # This part needs careful review if Streamlit UploadedFile objects are passed.
       # For now, the logic is kept as is from the original script.
       ext=Path(file_path.name).suffix.lower()[1:] if hasattr(file_path, 'name') else Path(file_path).suffix.lower()[1:]
       processor=self.processors.get(ext)
       return processor(file_path) if processor else "Unsupported format"


class WebIntelligence:
   def __init__(self):
       self.session=requests.Session()
       self.session.headers.update({"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
   def search_web(self,query):
       from duckduckgo_search import DDGS
       return [{"title":r["title"],"url":r["href"],"snippet":r["body"]} for r in DDGS().text(query,max_results=10)]
   def scrape_page(self,url):
       try:
           from bs4 import BeautifulSoup
           resp=self.session.get(url,timeout=10)
           return BeautifulSoup(resp.content,'html.parser').get_text()[:5000]
       except:return "Scraping failed"

orchestrator=TerminusOrchestrator()
doc_processor=DocumentUniverse()
web_intel=WebIntelligence()
EOF
}

# Function for generating terminus_ui.py
create_terminus_ui_script() {
    progress "CREATING TERMINUS UI SCRIPT"
    cat>"$INSTALL_DIR/terminus_ui.py"<<'EOF'
import streamlit as st,asyncio,json,time,os,subprocess
from pathlib import Path
import pandas as pd,plotly.express as px,plotly.graph_objects as go
from agents.master_orchestrator import orchestrator,doc_processor,web_intel

st.set_page_config(page_title="TERMINUS AI",layout="wide",initial_sidebar_state="expanded")

def main():
   st.markdown("""<div style='text-align:center;background:linear-gradient(90deg,#FF6B6B,#4ECDC4,#45B7D1,#96CEB4);padding:20px;border-radius:10px;margin-bottom:20px'>
   <h1 style='color:white;text-shadow:2px 2px 4px rgba(0,0,0,0.5)'>TERMINUS AI NEXUS</h1>
   <p style='color:white;font-size:18px'>ULTIMATE LOCAL AI ECOSYSTEM | 25+ MODELS | UNLIMITED POWER</p></div>""",unsafe_allow_html=True)

   # Sidebar Controls
   with st.sidebar:
       st.header("COMMAND CENTER")
       operation_mode=st.selectbox("Operation Mode",["Multi-Agent Chat","Document Processing","Web Intelligence","Image Generation","Code Generation","Data Analysis","Creative Suite"])

       st.subheader("Agent Selection")
       agent_names=[a.name for a in orchestrator.agents]
       selected_agents=st.multiselect("Active Agents",agent_names,default=agent_names[:6])

       st.subheader("Parameters")
       temperature=st.slider("Temperature",0.0,2.0,0.7)
       max_tokens=st.slider("Max Tokens",500,8000,2000)
       parallel_mode=st.checkbox("Parallel Execution",True)
       consensus_mode=st.checkbox("Consensus Analysis",True)

   # Main Interface
   if operation_mode=="Multi-Agent Chat":
       col1,col2=st.columns([2,1])
       with col1:
           st.subheader("UNIVERSAL AI COMMAND")
           user_prompt=st.text_area("Enter your command:",height=200,placeholder="Ask anything - the entire AI constellation will respond...")

           if st.button("EXECUTE ALL AGENTS",type="primary"):
               if user_prompt and selected_agents:
                   with st.spinner("Processing across AI constellation..."):
                       results=asyncio.run(orchestrator.parallel_execution(user_prompt,selected_agents))

                       if consensus_mode:
                           consensus=orchestrator.consensus_analysis(results)
                           st.success(f"Consensus Score: {consensus['consensus_score']:.2%}")

                       st.subheader("AGENT RESPONSES")
                       for result in results:
                           status_icon="✅" if result["status"]=="success" else "❌"
                           with st.expander(f"{status_icon} {result['agent']} ({result['model']})"):
                               st.write(result["response"])

       with col2:
           st.subheader("SYSTEM STATUS")
           st.metric("Total Agents",len(orchestrator.agents))
           st.metric("Active Agents",len(selected_agents))
           st.metric("Total Models",len(set(a.model for a in orchestrator.agents)))

           # Agent Status
           st.subheader("AGENT STATUS")
           agent_df=pd.DataFrame([{"Agent":a.name,"Model":a.model,"Status":"Active" if a.active else "Inactive"} for a in orchestrator.agents])
           st.dataframe(agent_df,use_container_width=True)

   elif operation_mode=="Document Processing":
       st.subheader("UNIVERSAL DOCUMENT PROCESSOR")
       uploaded_files=st.file_uploader("Upload documents",accept_multiple_files=True,type=['pdf','docx','xlsx','txt','csv','json','html'])

       if uploaded_files:
           for file in uploaded_files:
               with st.expander(f"{file.name}"):
                   content=doc_processor.process_file(file)
                   st.text_area("Content Preview",content[:1000]+"..." if len(content)>1000 else content,height=200)

                   if st.button(f"Analyze with AI",key=f"analyze_{file.name}"):
                       prompt=f"Analyze this document content: {content[:2000]}"
                       results=asyncio.run(orchestrator.parallel_execution(prompt,selected_agents[:3]))
                       for result in results:
                           st.info(f"**{result['agent']}**: {result['response'][:500]}...")

   elif operation_mode=="Web Intelligence":
       st.subheader("WEB INTELLIGENCE NEXUS")
       search_query=st.text_input("Search Query:")

       if st.button("SEARCH & ANALYZE"):
           if search_query:
               with st.spinner("Searching and analyzing..."):
                   search_results=web_intel.search_web(search_query)
                   st.json(search_results[:3])

                   analysis_prompt=f"Analyze these search results: {json.dumps(search_results[:3])}"
                   results=asyncio.run(orchestrator.parallel_execution(analysis_prompt,selected_agents[:3]))

                   for result in results:
                       with st.expander(f"{result['agent']} Analysis"):
                           st.write(result["response"])

   elif operation_mode == "Image Generation":
       st.subheader("IMAGE GENERATION STUDIO")
       image_prompt = st.text_area("Enter your image description:", height=100, placeholder="E.g., 'A photorealistic cat astronaut on the moon'")
       if st.button("Generate Image", type="primary"):
           if image_prompt:
               with st.spinner("Generating image... This may take a while, especially on first run or CPU."):
                   # Ensure 'ImageForge' is available and selected
                   imageforge_agent = next((agent for agent in orchestrator.agents if agent.name == "ImageForge"), None)
                   if imageforge_agent and imageforge_agent.active:
                       results = asyncio.run(orchestrator.parallel_execution(prompt=image_prompt, selected_agents=["ImageForge"]))
                       if results and isinstance(results, list) and len(results) > 0:
                           generation_result = results[0] # Expect one result for ImageForge
                           if generation_result.get("status") == "success" and generation_result.get("image_path"):
                               st.image(generation_result["image_path"], caption=f"Generated image for: {image_prompt}")
                               st.success(f"Image successfully generated and saved to: {generation_result['image_path']}")
                           else:
                               st.error(f"Image generation failed: {generation_result.get('response', 'Unknown error')}")
                       else:
                           st.error("Image generation failed to produce a result.")
                   else:
                       st.error("ImageForge agent not found or is not active. Please check agent configuration.")
           else:
               st.warning("Please enter an image description.")

   elif operation_mode == "Code Generation":
       st.subheader("PROJECT SCAFFOLDING & CODE GENERATION")

       st.markdown("### 🏗️ Scaffold New Project")

       project_name = st.text_input("Enter Project Name:", placeholder="e.g., my_new_cli_app")

       project_type_options = {
           "Python CLI (Typer)": "python_cli",
           "Python FastAPI Backend": "python_fastapi",
           "Python Streamlit Dashboard": "python_streamlit",
           "Basic Python Project": "python_basic"
       }
       display_project_type = st.selectbox(
           "Select Project Type:",
           options=list(project_type_options.keys())
       )
       selected_project_type_value = project_type_options[display_project_type]

       if st.button("Scaffold Project", type="primary"):
           if project_name and selected_project_type_value:
               with st.spinner(f"Scaffolding '{project_name}' as {display_project_type}..."):
                   result = asyncio.run(orchestrator.scaffold_new_project(
                       project_name=project_name,
                       project_type=selected_project_type_value
                   ))

                   if result and result.get("status") == "success":
                       st.success(result.get("message", "Project scaffolded successfully!"))
                   else:
                       st.error(result.get("message", "Failed to scaffold project."))
           else:
               st.warning("Please enter a project name and select a project type.")

       st.markdown("---")
       st.markdown("### 🤖 AI-Assisted Code Generation (Future Feature)")
       st.info("The ability to generate code snippets or refine existing code using AI agents will be developed here.")

if __name__=="__main__":
   main()
EOF
}

# Function for generating auto_dev.py
create_auto_dev_script() {
    progress "CREATING AUTO DEV SCRIPT"
    echo "Installing auto_dev dependencies from $SCRIPT_DIR/dev_requirements.txt" | tee -a "$LOG"
    pip3 install -r "$SCRIPT_DIR/dev_requirements.txt" || { echo "ERROR: Failed to install critical packages from $SCRIPT_DIR/dev_requirements.txt. Aborting. Check $LOG for details." | tee -a "$LOG"; exit 1; }
    cat>"$INSTALL_DIR/tools/auto_dev.py"<<'EOF'
import subprocess,os,ast,json,shlex
from pathlib import Path

class AutoDev:
    def __init__(self):
        self.tools = {"format": "black", "lint": "flake8", "type": "mypy", "test": "pytest", "security": "bandit"}
        self.gitignore_content = """# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# PEP 582; __pypackages__
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
"""

    def create_project(self, name, project_type="python_cli"):
        project_path = Path(name)
        project_path.mkdir(exist_ok=True)

        if project_type == "python_cli":
            main_py_content = """#!/usr/bin/env python3
import typer
from typing_extensions import Annotated

app = typer.Typer()

@app.command()
def hello(name: Annotated[str, typer.Option(prompt="Enter your name")] = "World"):
    print(f"Hello {name}")

@app.command()
def goodbye(name: str = "World", formal: bool = False):
    if formal:
        print(f"Goodbye Ms. {name}. Have a good day.")
    else:
        print(f"Bye {name}!")

if __name__ == "__main__":
    app()
"""
            requirements_txt_content = "typer[all]==0.12.3"
            readme_md_content = f"""# {name}

A Command Line Interface (CLI) application built with Typer.

## Created by Terminus AI

## Installation
```bash
pip install -r requirements.txt
```

## Usage
```bash
python main.py --help
python main.py hello
python main.py goodbye --name Gokay
```
"""
            (project_path / "main.py").write_text(main_py_content)
            (project_path / "requirements.txt").write_text(requirements_txt_content)
            (project_path / "README.md").write_text(readme_md_content)
            (project_path / ".gitignore").write_text(self.gitignore_content)
            return f"Python CLI project '{name}' created successfully at {project_path}"

        elif project_type == "python_fastapi":
            app_dir = project_path / "app"
            app_dir.mkdir(exist_ok=True)

            main_py_content_fastapi = f"""from fastapi import FastAPI
from typing import Dict

app = FastAPI(title="{name} API", version="0.1.0")

@app.get("/")
async def read_root() -> Dict[str, str]:
    return {{"message": "Welcome to {name}!"}}

@app.get("/items/{{item_id}}")
async def read_item(item_id: int, q: str | None = None) -> Dict[str, any]:
    return {{"item_id": item_id, "q": q}}

# To run this application:
# 1. Install uvicorn: pip install "uvicorn[standard]"
# 2. Navigate to the project directory in your terminal.
# 3. Run: uvicorn app.main:app --reload
"""
            (app_dir / "main.py").write_text(main_py_content_fastapi)

            requirements_txt_content_fastapi = """fastapi==0.111.0
uvicorn[standard]==0.29.0
"""
            (project_path / "requirements.txt").write_text(requirements_txt_content_fastapi)

            readme_md_content_fastapi = f"""# {name} - FastAPI Backend

A FastAPI backend application.

## Created by Terminus AI

## Project Structure
```
{name}/
├── app/
│   └── main.py     # Main application logic
├── requirements.txt  # Project dependencies
└── README.md         # This file
```

## Setup and Installation
1.  Create a virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use \`venv\Scripts\\activate\`
    ```
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Running the Application
Navigate to the project's root directory (`{name}/`) in your terminal and run:
```bash
uvicorn app.main:app --reload
```
The application will typically be available at `http://127.0.0.1:8000`.
"""
            (project_path / "README.md").write_text(readme_md_content_fastapi)
            (project_path / ".gitignore").write_text(self.gitignore_content) # Reuse
            return f"Python FastAPI project '{name}' created successfully at {project_path}"

        elif project_type == "python_streamlit":
            app_py_content_streamlit = f"""import streamlit as st
import pandas as pd
import numpy as np

st.set_page_config(layout="wide")

st.title("📊 {name} - Streamlit Dashboard")

st.header("Sample Data Visualization")
st.write("This is a simple example of a Streamlit dashboard.")

# Sample Data
chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c']
)

st.subheader("Line Chart")
st.line_chart(chart_data)

st.subheader("Area Chart")
st.area_chart(chart_data)

st.sidebar.header("Controls")
option = st.sidebar.selectbox(
    "Choose a chart type:",
    ("Line Chart", "Area Chart", "Bar Chart (Random)")
)

if option == "Bar Chart (Random)":
    st.subheader("Bar Chart")
    bar_data = pd.DataFrame(
        np.random.randint(0, 100, 50),
        columns=['Data']
    )
    st.bar_chart(bar_data)

st.write("---")
st.write("Generated by Terminus AI AutoDev.")
"""
            (project_path / "app.py").write_text(app_py_content_streamlit)

            requirements_txt_content_streamlit = """streamlit==1.36.0
pandas
numpy
"""
            (project_path / "requirements.txt").write_text(requirements_txt_content_streamlit)

            readme_md_content_streamlit = f"""# {name} - Streamlit Dashboard

A Streamlit dashboard application.

## Created by Terminus AI

## Project Structure
```
{name}/
├── app.py            # Main Streamlit application
├── requirements.txt  # Project dependencies
└── README.md         # This file
```

## Setup and Installation
1.  Create a virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use \`venv\Scripts\\activate\`
    ```
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Running the Application
Navigate to the project's root directory (`{name}/`) in your terminal and run:
```bash
streamlit run app.py
```
The application will typically open in your web browser automatically.
"""
            (project_path / "README.md").write_text(readme_md_content_streamlit)
            (project_path / ".gitignore").write_text(self.gitignore_content) # Reuse
            return f"Python Streamlit project '{name}' created successfully at {project_path}"
        else:
            # Fallback to basic Python project
            (project_path/"main.py").write_text("""#!/usr/bin/env python3

def main():
    print('Hello World')

if __name__=='__main__':
    main()""")
            (project_path/"requirements.txt").write_text("")
            (project_path/"README.md").write_text(f"# {name}\n\nProject created by Terminus AI")
            return f"Basic Python project '{name}' created successfully at {project_path}"

   def analyze_code(self,file_path):
       try:
           with open(file_path) as f:
               tree=ast.parse(f.read())
           return {"functions":[n.name for n in ast.walk(tree) if isinstance(n,ast.FunctionDef)],"classes":[n.name for n in ast.walk(tree) if isinstance(n,ast.ClassDef)],"lines":len(open(file_path).readlines())}
       except:return {"error":"Analysis failed"}

   def run_command(self,cmd_string): # Renamed cmd to cmd_string for clarity
       try:
           cmd_list = shlex.split(cmd_string)
           # Basic validation: Ensure the command is one of the known tools if possible,
           # or ensure it does not contain metacharacters if not splitting.
           # For now, we rely on shlex.split() to handle arguments safely.
           # A more robust solution might involve checking cmd_list[0] against self.tools.values()
           return subprocess.run(cmd_list, capture_output=True, text=True, check=True).stdout
       except subprocess.CalledProcessError as e:
           # Log error or return more specific error info
           return f"Command failed with error: {e.stderr}"
       except Exception as e:
           return f"Command execution failed: {str(e)}"

auto_dev=AutoDev()
EOF
}

# Function for generating data_engine.py
create_data_engine_script() {
    progress "CREATING DATA ENGINE SCRIPT"
    # Dependencies (dask, distributed, polars, etc.) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/tools/data_engine.py"<<'EOF'
import pandas as pd,numpy as np,json,sqlite3,redis,pymongo
from pathlib import Path
import plotly.express as px,plotly.graph_objects as go

class DataUniverse:
   def __init__(self):
       self.connections={}
       self.cache={}

   def load_data(self,source,format_type="auto"):
       if format_type=="auto":format_type=Path(source).suffix[1:]
       loaders={"csv":pd.read_csv,"json":pd.read_json,"xlsx":pd.read_excel,"parquet":pd.read_parquet,"sql":self.load_sql}
       loader=loaders.get(format_type,pd.read_csv)
       return loader(source) if format_type!="sql" else loader(source)

   def analyze_dataframe(self,df):
       return {"shape":df.shape,"columns":list(df.columns),"dtypes":df.dtypes.to_dict(),"missing":df.isnull().sum().to_dict(),"stats":df.describe().to_dict()}

   def create_visualization(self,df,chart_type="scatter",x=None,y=None):
       if chart_type=="scatter":return px.scatter(df,x=x,y=y)
       elif chart_type=="line":return px.line(df,x=x,y=y)
       elif chart_type=="bar":return px.bar(df,x=x,y=y)
       else:return px.histogram(df,x=x)

   def load_sql(self,query,db_path="data.db"):
       conn=sqlite3.connect(db_path)
       return pd.read_sql_query(query,conn)

data_engine=DataUniverse()
EOF
}

# Function for generating quantum_engine.py
create_quantum_engine_script() {
    progress "CREATING QUANTUM ENGINE SCRIPT"
    # Dependencies (qiskit, pennylane, etc.) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/core/quantum_engine.py"<<'EOF'
import numpy as np
from qiskit import QuantumCircuit,execute,Aer
from qiskit.circuit.library import RealAmplitudes,ZZFeatureMap

class QuantumProcessor:
   def __init__(self):
       self.backend=Aer.get_backend('qasm_simulator')
       self.circuits={}

   def create_circuit(self,qubits=4):
       qc=QuantumCircuit(qubits,qubits)
       return qc

   def quantum_transform(self,data):
       # Quantum data transformation
       qc=self.create_circuit(len(data))
       for i,val in enumerate(data):
           qc.ry(val*np.pi,i)
       qc.measure_all()
       job=execute(qc,self.backend,shots=1024)
       return job.result().get_counts()

   def quantum_optimization(self,objective_function,params):
       # Quantum optimization algorithm
       return {"optimized_params":params,"cost":objective_function(params)}

quantum_proc=QuantumProcessor()
EOF
}

# Function for generating nas_engine.py
create_nas_engine_script() {
    progress "CREATING NAS ENGINE SCRIPT"
    cat>"$INSTALL_DIR/core/nas_engine.py"<<'EOF'
import torch,torch.nn as nn,random

class NASEngine:
   def __init__(self):
       self.architectures=[]
       self.performance_history={}

   def generate_architecture(self,layers=5):
       layer_types=['conv','linear','attention','residual']
       activations=['relu','gelu','swish','leaky_relu']
       arch={'layers':[],'optimizer':'adam','lr':0.001}
       for i in range(layers):
           layer={'type':random.choice(layer_types),'activation':random.choice(activations),'size':random.choice([64,128,256,512])}
           arch['layers'].append(layer)
       return arch

   def evolve_architecture(self,base_arch,mutation_rate=0.1):
       new_arch=base_arch.copy()
       if random.random()<mutation_rate:
           layer_idx=random.randint(0,len(new_arch['layers'])-1)
           new_arch['layers'][layer_idx]['size']*=random.choice([0.5,2])
       return new_arch

   def search_optimal_architecture(self,dataset,epochs=10):
       best_arch=None
       best_performance=0
       for _ in range(epochs):
           arch=self.generate_architecture()
           performance=self.evaluate_architecture(arch,dataset)
           if performance>best_performance:
               best_arch,best_performance=arch,performance
       return best_arch,best_performance

   def evaluate_architecture(self,arch,dataset):
       # Simplified evaluation
       return random.random()

nas_engine=NASEngine()
EOF
}

# Function for generating distributed_engine.py
create_distributed_engine_script() {
    progress "CREATING DISTRIBUTED ENGINE SCRIPT"
    # Dependencies (ray, dask, distributed, celery) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/core/distributed_engine.py"<<'EOF'
import ray,asyncio,threading,multiprocessing
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor

@ray.remote
class DistributedWorker:
   def __init__(self,worker_id):
       self.worker_id=worker_id
       self.tasks_completed=0

   def process_task(self,task_data):
       # Distributed task processing
       self.tasks_completed+=1
       return f"Worker {self.worker_id} processed task: {task_data}"

class DistributedMesh:
   def __init__(self,num_workers=4):
       if not ray.is_initialized():ray.init()
       self.workers=[DistributedWorker.remote(i) for i in range(num_workers)]
       self.task_queue=[]

   def distribute_tasks(self,tasks):
       futures=[]
       for i,task in enumerate(tasks):
           worker=self.workers[i%len(self.workers)]
           future=worker.process_task.remote(task)
           futures.append(future)
       return ray.get(futures)

   def scale_workers(self,new_count):
       current=len(self.workers)
       if new_count>current:
           self.workers.extend([DistributedWorker.remote(i) for i in range(current,new_count)])
       return f"Scaled to {new_count} workers"

distributed_mesh=DistributedMesh()
EOF
}

# Function for generating security_engine.py
create_security_engine_script() {
    progress "CREATING SECURITY ENGINE SCRIPT"
    # Dependencies (cryptography, keyring, etc.) are expected to be installed via main requirements files.
    cat>"$INSTALL_DIR/core/security_engine.py"<<'EOF'
import hashlib,secrets,base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

class SecurityEngine:
   def __init__(self):
       self.master_key=Fernet.generate_key()
       self.cipher=Fernet(self.master_key)
       self.sessions={}

   def encrypt_data(self,data):
       return self.cipher.encrypt(data.encode()).decode()

   def decrypt_data(self,encrypted_data):
       return self.cipher.decrypt(encrypted_data.encode()).decode()

   def generate_session_token(self):
       return secrets.token_urlsafe(32)

   def hash_password(self,password,salt=None):
       if not salt:salt=secrets.token_hex(16)
       return hashlib.pbkdf2_hmac('sha256',password.encode(),salt.encode(),100000).hex(),salt

   def verify_password(self,password,hash_value,salt):
       return self.hash_password(password,salt)[0]==hash_value

   def secure_communication(self,message):
       encrypted=self.encrypt_data(message)
       signature=hashlib.sha256(encrypted.encode()).hexdigest()
       return {"encrypted_message":encrypted,"signature":signature}

security_engine=SecurityEngine()
EOF
}

# Function for generating launch_terminus.py
create_launcher_script() {
    progress "CREATING LAUNCHER SCRIPT"
    cat>"$INSTALL_DIR/launch_terminus.py"<<'EOF'
#!/usr/bin/env python3
import subprocess,sys,os,time,threading,signal
from pathlib import Path
import streamlit as st

class TerminusLauncher:
   def __init__(self):
       self.base_dir=Path(__file__).parent
       self.processes={}
       self.running=True
       self.restart_counts = {}
       self.max_restarts = 5

   def start_ollama(self):
       print("Starting Ollama server...")
       self.processes['ollama']=subprocess.Popen(['ollama','serve'],stdout=subprocess.DEVNULL,stderr=subprocess.DEVNULL)
       time.sleep(5)

   def start_streamlit(self):
       print("Starting Terminus UI...")
       cmd=['streamlit','run',str(self.base_dir/'terminus_ui.py'),'--server.port','8501','--server.address','0.0.0.0']
       self.processes['streamlit']=subprocess.Popen(cmd)

   def monitor_system(self):
       while self.running:
           for name, proc in list(self.processes.items()): # Ensure list() for safe iteration
               if proc.poll() is not None: # Process has terminated
                   current_restarts = self.restart_counts.get(name, 0)
                   if current_restarts >= self.max_restarts:
                       print(f"ERROR: Process {name} has crashed {current_restarts} times and exceeded max restart limit of {self.max_restarts}. Will not attempt to restart again.")
                       # Optionally remove from self.processes or mark as failed
                       # For example, to stop further checks on this failed process:
                       del self.processes[name]
                       continue

                   self.restart_counts[name] = current_restarts + 1
                   print(f"Process {name} (PID {proc.pid}) terminated unexpectedly. Restarting (attempt {self.restart_counts[name]}/{self.max_restarts})...")
                   if name == 'ollama':
                       self.start_ollama()
                   elif name == 'streamlit':
                       self.start_streamlit()
                   # Add a small delay after a restart attempt to prevent rapid failing loops
                   time.sleep(2)
               else:
                   # Process is running, reset its restart count if it was previously failing
                   if self.restart_counts.get(name, 0) > 0:
                       print(f"Process {name} is running normally. Resetting restart count.")
                       self.restart_counts[name] = 0
           time.sleep(10) # Check every 10 seconds

   def shutdown(self,signum=None,frame=None):
       print("\nShutting down Terminus AI...")
       self.running=False
       for proc in self.processes.values():
           proc.terminate()
       sys.exit(0)

   def launch(self):
       signal.signal(signal.SIGINT,self.shutdown)
       signal.signal(signal.SIGTERM,self.shutdown)

       print("TERMINUS AI - ULTIMATE LOCAL AI ECOSYSTEM")
       print(f"Base Directory: {self.base_dir}")
       print("Models: 25+ AI Models Ready")
       print("Agents: 12 Specialized AI Agents")
       print("Total Size: ~280GB")
       print("Interface: http://localhost:8501")

       self.start_ollama()
       self.start_streamlit()

       monitor_thread=threading.Thread(target=self.monitor_system,daemon=True)
       monitor_thread.start()

       print("Terminus AI is now running!")
       print("Access the interface at: http://localhost:8501")
       print("Press Ctrl+C to shutdown")

       try:
           while self.running:time.sleep(1)
       except KeyboardInterrupt:
           self.shutdown()

if __name__=="__main__":
   launcher=TerminusLauncher()
   launcher.launch()
EOF

chmod +x "$INSTALL_DIR/launch_terminus.py"
}

# Function for creating the final README and any other wrap-up tasks
finalize_installation() {
    progress "FINALIZING INSTALLATION & CREATING README"
    cat>"$INSTALL_DIR/README.md"<<'EOF'
# TERMINUS AI - ULTIMATE LOCAL AI ECOSYSTEM

## OPUS MAGNUM SPECIFICATIONS
- **Total Size**: ~280GB
- **AI Models**: 25+ State-of-the-Art Models
- **Agents**: 12 Specialized AI Agents
- **Capabilities**: Unlimited & Uncensored
- **Architecture**: Quantum-Classical Hybrid
- **Interface**: Advanced Web-Based Control Center

## INSTALLATION COMPLETE
### Quick Start:
```bash
cd ~/.terminus-ai
python3 launch_terminus.py
```
EOF
    echo "TERMINUS AI INSTALLATION COMPLETE. See README.md in $INSTALL_DIR for details." | tee -a "$LOG"
}

# Main execution flow
main() {
    initialize_setup
    install_system_dependencies
    install_python_core_libraries
    install_python_framework_libraries
    install_python_utility_libraries
    install_ollama_and_dependencies
    pull_ollama_models
    create_agent_orchestration_script
    create_terminus_ui_script
    create_auto_dev_script
    create_data_engine_script
    create_quantum_engine_script
    create_nas_engine_script
    create_distributed_engine_script
    create_security_engine_script
    create_launcher_script
    finalize_installation

    echo "ALL STAGES COMPLETED SUCCESSFULLY!" | tee -a "$LOG"
}

# Run the main function
main
